<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>ECE 4&#x2F;599</title>
        <link>https%3A//khale.github.io/mem-systems-w25</link>
        <description></description>
        <generator>Zola</generator>
        <language>en</language>
        <atom:link href="https%3A//khale.github.io/mem-systems-w25/rss.xml" rel="self" type="application/rss+xml"/>
        <icon>https%3A//khale.github.io/mem-systems-w25/img/favicon.ico</icon>
        <lastBuildDate>Wed, 12 Mar 2025 00:00:00 +0000</lastBuildDate>
        
            <item>
                <title>The Challenges of BTRFS implementation and getting persistent memory</title>
                <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/btrfs/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/btrfs/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This project focused on exploring the benefits and challenges of the BTRFS file system and how it relates to persistent memory. BTRFS is a Merkle-Tree or B-Tree file system designed with CoW in mind. BTRFS is a type of harddrive type or file structure type characterized by its support of snapshotting and its built-in CoW functions. BTRFS dynamically organizes its data dynamically unlike other file systems that use bitmaps and inode tables. BTRFS also includes failsafes like checksums to recover metadata of files in the case that data gets corrupted.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;fins5.png&quot; alt=&quot;Example of BTRFS Snapshots&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;table-of-contents&quot;&gt;Table of Contents&lt;&#x2F;h2&gt;
&lt;p&gt;BTRFS - binary-tree file system. A file system that has properties including COW (copy on Writing) and snapshot.&lt;br &#x2F;&gt;
Persistent memory - A new type of memory device that has high performance and is byte-addressable.&lt;br &#x2F;&gt;
CoW - Copy on Write&lt;br &#x2F;&gt;
PMEM - Persistent Memory&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implementation&quot;&gt;Implementation:&lt;&#x2F;h2&gt;
&lt;p&gt;To start it will be assumed that you are installing this filesystem on ubuntu 22.04 through WSL2&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;fin1.png&quot; alt=&quot;fig1&quot; &#x2F;&gt;
Figure 1: Setting up a 5GB Disk&lt;&#x2F;p&gt;
&lt;p&gt;You can use a command like &lt;code&gt;sudo mount -o loop,subvol=data btrfs-test.img &#x2F;mnt&#x2F;btrfs_test&lt;&#x2F;code&gt; to mount your test image you created. My test image is &lt;code&gt;btrfs-test.img&lt;&#x2F;code&gt; and im mounting &lt;code&gt;&#x2F;mnt&#x2F;btrfs_test&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;fin2.png&quot; alt=&quot;fig2&quot; &#x2F;&gt;
Figure 2: checking that our drive is type btrfs&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;fin3.png&quot; alt=&quot;fig3&quot; &#x2F;&gt;
Figure 3: creating a sub volume&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;fin4.png&quot; alt=&quot;fig4&quot; &#x2F;&gt;
Figure 4: create a snapshot of a subdirectory&lt;&#x2F;p&gt;
&lt;p&gt;Example initialization of a BTRFS drive:&lt;&#x2F;p&gt;
&lt;p&gt;Create a virtual image&lt;br &#x2F;&gt;
&lt;code&gt;dd if=&#x2F;dev&#x2F;zero of=btrfs-test.img bs=1M count=1024&lt;&#x2F;code&gt;&lt;br &#x2F;&gt;
Format the system&lt;br &#x2F;&gt;
&lt;code&gt;mkfs.btrfs btrfs-test.img&lt;&#x2F;code&gt;
Create a mount&lt;br &#x2F;&gt;
&lt;code&gt;mkdir -p &#x2F;mnt&#x2F;btrfs_test&lt;&#x2F;code&gt;
Set the mount&lt;br &#x2F;&gt;
&lt;code&gt;sudo mount -o loop btrfs-test.img &#x2F;mnt&#x2F;btrfs_test&lt;&#x2F;code&gt;
Confirm with: &lt;code&gt;df -Th | grep btrfs&lt;&#x2F;code&gt;&lt;br &#x2F;&gt;
Create subvolume&lt;br &#x2F;&gt;
&lt;code&gt;sudo btrfs subvolume create &#x2F;mnt&#x2F;btrfs_test&#x2F;data&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;persistent-memory&quot;&gt;Persistent memory&lt;&#x2F;h2&gt;
&lt;p&gt;What is persistent memory?   Persistent memory is a newish type of memory that
serves as a sudo hard disk but in the form of ram. Officially it’s a solid
state high performance byte addressable memory device that resides on the
memory bus of a system, but in layman’s terms it simply stores memory in a non
volatile manner unlike your typical DRAM. This means that it essentially
combines the properties of DRAM and an SSD which gives several advantages like
quicker memory accesses than SSD’s and byte addressable storage.&lt;&#x2F;p&gt;
&lt;p&gt;BTRFS can be used on persistent memory computers, however btrfs is designed
with block devices in mind meaning that for a non block device like PMEM a new
file system would need to be developed for btrfs which would be possible thanks
to PMEMs byte-addressability. BPFS comes to mind as an example of such
a project, although they created a new system rather than adapting btrfs.&lt;&#x2F;p&gt;
&lt;p&gt;##Tests&lt;br &#x2F;&gt;
Our tests focused on the snapshot capabilities of the btrfs file system to get
a better understanding of their capabilities and their applications. Due to
poor planning we were not able to get access to a computer with persistent
memory so we will instead be showing several tests performed on a standard
BTRFS file system using very normal specs (Intel I7-5500U, 8gb DDR4 RAM). The
environment for this experiment was WSL2 running ubuntu 24.04&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;snapshots frequency vs time to take a snapshot&lt;&#x2F;li&gt;
&lt;li&gt;snapshot with vs without noisy programs&lt;&#x2F;li&gt;
&lt;li&gt;snapshot performance over slow change of file&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;fin6.png&quot; alt=&quot;noise&quot; &#x2F;&gt;
BTRFS noise experiment&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;data_change_plot.png&quot; alt=&quot;fig6&quot; &#x2F;&gt;
Figure 6: Random data change plot&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;frequency_plot.png&quot; alt=&quot;fig7&quot; &#x2F;&gt;
Figure 7: Frequency of snapshots vs time&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;btrfs&#x2F;noise_comparison.png&quot; alt=&quot;fig8&quot; &#x2F;&gt;
Figure 8: Snapshotting with and without background processes&lt;&#x2F;p&gt;
&lt;p&gt;Looking at our results we see what we are expecting to see, as we increase our
workload we see a decrease in snapshotting speed. Snapshots of bigger files
take longer, although this is not really visible on the data change graph that
we provided since the range of snapshots did not include large enough sizes.
Snapshots of smaller files (5 GB or lower)&lt;&#x2F;p&gt;
&lt;h2 id=&quot;challenges&quot;&gt;Challenges&lt;&#x2F;h2&gt;
&lt;p&gt;We faced many challenges trying to study this file system and work with it, as
it turns out however our greatest challenge was finding a machine with
persistent memory hardware. We could have used Amazon web services however we
did not register for them quickly enough so it became unrealistic to work with
them. We were also not able to find a computer lab on campus with an available
persistent memory computer, therefore we were left with testing BTRFS on
a standard computer running a VM.   Both of us were not knowledgeable in
regards to BTRFS really so getting familiar with navigating all the commands
needed for this project was a far greater undertaking than we understood at
first. We also needed help with designing the tests for the file system and we
needed to ask for advice from our professor multiple times. The only other
major setbacks that challenged us was that our testing environment would crash
occasionally causing tasks to take longer than needed.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Blog about BTRFS and WSL2(&lt;a href=&quot;https:&#x2F;&#x2F;blog.bryanroessler.com&#x2F;2020-12-14-btrfs-on-wsl2&#x2F;#:~:text=Since%20BTRFS%20is%20baked%20into,is%20still%20a%20preview%20build&quot;&gt;https:&#x2F;&#x2F;blog.bryanroessler.com&#x2F;2020-12-14-btrfs-on-wsl2&#x2F;#:~:text=Since%20BTRFS%20is%20baked%20into,is%20still%20a%20preview%20build&lt;&#x2F;a&gt;. )&lt;&#x2F;li&gt;
&lt;li&gt;Reading on BPFS a file system developed for persistent memory(&lt;a href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;1629575.1629589&quot;&gt;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;abs&#x2F;10.1145&#x2F;1629575.1629589&lt;&#x2F;a&gt; )&lt;&#x2F;li&gt;
&lt;li&gt;BTRFS wikipedia for initial understanding and reference search(&lt;a href=&quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Btrfs&quot;&gt;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Btrfs&lt;&#x2F;a&gt; )&lt;&#x2F;li&gt;
&lt;li&gt;BTRFS commands website(&lt;a href=&quot;https:&#x2F;&#x2F;docs.oracle.com&#x2F;en&#x2F;operating-systems&#x2F;oracle-linux&#x2F;6&#x2F;admin&#x2F;ol_use_case3_btrfs.html&quot;&gt;https:&#x2F;&#x2F;docs.oracle.com&#x2F;en&#x2F;operating-systems&#x2F;oracle-linux&#x2F;6&#x2F;admin&#x2F;ol_use_case3_btrfs.html&lt;&#x2F;a&gt; )&lt;&#x2F;li&gt;
&lt;li&gt;BTRFS Blog with useful pictures(&lt;a href=&quot;https:&#x2F;&#x2F;recoverit.wondershare.com&#x2F;file-system&#x2F;what-is-btrfs-file-system.html&quot;&gt;https:&#x2F;&#x2F;recoverit.wondershare.com&#x2F;file-system&#x2F;what-is-btrfs-file-system.html&lt;&#x2F;a&gt; )&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Closes #25&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>Comprehending CHI Coherency in gem5</title>
                <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/chi-coherency/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/chi-coherency/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;In computer architecture classes we are introduced to cache coherency protocols like MSI, MESI, and MOESI, as well as snooping and directory-based approaches.  However, as we look at modern implementations of systems we see more complexity and different terminology.  Also, it can be difficult to map to commercial implementations as not all details are publicly visible.  This project focuses on understanding CHI, a modern cache coherent interconnect protocol and how it is used on advanced interconnect topologies.  One of the best ways to learn is to run real programs on real systems (or simulations of real systems) so in this project we execute workloads on the gem5 simulator modeling a mesh-topology interconnect that uses the CHI protocol.&lt;&#x2F;p&gt;
&lt;p&gt;The AMBA CHI Architecture Specification defines the protocol for a cache-coherent interconnect used on most ARM and many RISC-V systems. It defines a packet-based network-on-chip protocol that is used for a wide range of topologies from local subsystem interconnects to complex multi-die and multi-socket designs. The CHI interconnect relies on separate channels for requests, responses, snoops, and data transfer enabling a high degree concurrency and asynchronicity in its operations. The CHI protocol enables MESI and MOESI style coherency protocols but does so using more concise (and complex) state definitions.&lt;&#x2F;p&gt;
&lt;p&gt;The gem5 simulator enables architecture exploration by simulating different processor architectures and implementations, different cache and memories hierarchies, and different interconnect and coherency protocols with the goal of analyzing performance and correctness.  The gem5 simulator supports of an entire operating system stack (Full System Emulation - FS) or execution of standalone userspace programs (System Call Emulation - SE).  Included amongst the various topologies and coherency protocols already provided in gem5 is support for the CHI protocol.  Example system configurations for CHI are available including a simple crossbar interconnect with one directory&#x2F;system-level cache as well as a more advanced 2x4 mesh topology with multiple requester, home, and subordinate nodes.&lt;&#x2F;p&gt;
&lt;p&gt;This project examines the CHI specification, following the evolution of AMBA interconnects over time, maps these concepts onto some representative commercial interconnect IPs and then seeks to reinforce these concepts by simulating programs in gem5 on a CHI mesh interconnect.  We employ a workload designed to induce coherency traffic and examine how varying workload parameters impacts system performance.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;amba-evolution-from-apb-to-chi&quot;&gt;AMBA Evolution from APB to CHI&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;specifications&quot;&gt;Specifications&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Title&lt;&#x2F;th&gt;&lt;th&gt;Spec&lt;&#x2F;th&gt;&lt;th&gt;Year&lt;&#x2F;th&gt;&lt;th&gt;Key Features&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;APB&lt;&#x2F;td&gt;&lt;td&gt;Advanced Peripheral Bus&lt;&#x2F;td&gt;&lt;td&gt;1997&lt;&#x2F;td&gt;&lt;td&gt;Simple&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;AHB&lt;&#x2F;td&gt;&lt;td&gt;Advanced High Performance Bus&lt;&#x2F;td&gt;&lt;td&gt;1997&lt;&#x2F;td&gt;&lt;td&gt;Multiple masters, larger bus widths&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;AXI3&lt;&#x2F;td&gt;&lt;td&gt;Advanced eXtensible Interface&lt;&#x2F;td&gt;&lt;td&gt;2003&lt;&#x2F;td&gt;&lt;td&gt;Higher perf, higher clock freq&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ACE&lt;&#x2F;td&gt;&lt;td&gt;AXI Coherency Extensions&lt;&#x2F;td&gt;&lt;td&gt;2010&lt;&#x2F;td&gt;&lt;td&gt;Additional signaling for system-wide coherency&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CHI&lt;&#x2F;td&gt;&lt;td&gt;Coherent Hub Interconnect&lt;&#x2F;td&gt;&lt;td&gt;2014&lt;&#x2F;td&gt;&lt;td&gt;Redesigned transport layer for higher performance&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;apb-advanced-peripheral-bus&quot;&gt;APB - Advanced Peripheral Bus&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;apb.png&quot; alt=&quot;APB Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Advanced Peripheral Bus Subsystem&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The APB bus is a shared bus parallel bus with separate select lines.  It relies on an upstream device (depicted above as an APB bridge) to decode address ranges to assert the proper select line for the peripheral.  There is only one requester from the perspective of the APB bus so there is no concept of multi-initiator operation or arbitration.&lt;&#x2F;p&gt;
&lt;p&gt;APB is a low performance bus so it does not support bursting, exclusive accesses, overlapping transactions or coherency.  The shared bus for address, data, attributes and strobes inherently limits performance and scalability but is optimal for small designs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ahb-advanced-high-performance-bus&quot;&gt;AHB - Advanced High Performance Bus&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;ahb.png&quot; alt=&quot;AHB Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Diagram from AMBA AHB Protocol Specification&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The AHB bus provides somewhat higher performance including support for bursts and exclusives.  It can support multiple initiators with some arbitration logic.  Like APB, the address is decoded and used to select the appropriate target device.  Additional logic is needed to select read data. There is still no support for overlapping transactions or coherency.  The AHB bus is suitable for small microcontroller cores or equivalently simple processors.  AHB could be implemented as a point-to-point or as a simple crossbar.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;axi-advanced-extensible-interface&quot;&gt;AXI - Advanced eXtensible Interface&lt;&#x2F;h2&gt;
&lt;p&gt;The AXI specification defines the first high performance AMBA interconnect with support for multiple outstanding transactions and out-of-order completions to requests.  The address and data phases are decoupled.  AXI supports bursting, exclusives, and QoS identifiers.  No coherency support exists in AXI.&lt;&#x2F;p&gt;
&lt;p&gt;AXI is appropriate for high performance single-core processors that do not require coherency from other system initiators such as DMA agents or other processors in a heterogenous architecture.  AXI may be appropriate for an embedded processor attachment or as an intermediate interconnect for peripherals that initiate their own transactions.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;axi.png&quot; alt=&quot;AXI Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example System using AXI, from: What is AMBA?, https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=CZlDTQzOfq4&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;axi4-transaction-attributes&quot;&gt;AXI4 Transaction Attributes&lt;&#x2F;h3&gt;
&lt;p&gt;It’s worth taking a moment to study the transaction attributes for AXI.  Any initiator of a transaction drives attributes that describe the treatment of the transaction for the purposes of ordering, buffering, caching and protection.  While this is only an interconnect specification, these attributes align with the ARMv7 architecture memory model definitions.&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;strong&gt;AxCACHE&lt;&#x2F;strong&gt; signals determine the ordering, buffering and cacheability treatment for the transaction.  We divide these into cacheable and non-cacheable categories and then subdivide based on the buffering and ordering treatments.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Cacheability&lt;&#x2F;th&gt;&lt;th&gt;Type&lt;&#x2F;th&gt;&lt;th&gt;Early Ack?&lt;&#x2F;th&gt;&lt;th&gt;Mergeable?&lt;&#x2F;th&gt;&lt;th&gt;Prefetchable?&lt;&#x2F;th&gt;&lt;th&gt;Example&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Non-Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Device Non-Bufferable&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;Strongly Ordered MMIO Register&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Non-Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Device Bufferable&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;Weakly Ordered MMIO Register&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Non-Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Normal Non-Bufferable&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;No&lt;&#x2F;td&gt;&lt;td&gt;Strongly Ordered Shared Mem&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Non-Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Normal Bufferable&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Weakly Ordered Shared Mem&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-through + NA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Framebuffer or Other DMA Buf&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-through + RA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Framebuffer or Other DMA Buf&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-through + WA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Framebuffer or Other DMA Buf&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-through + RWA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Framebuffer or Other DMA Buf&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-back + NA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Main memory&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-back + RA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Main memory&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-back + WA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Main memory&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Cacheable&lt;&#x2F;td&gt;&lt;td&gt;Write-back + RWA&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Yes&lt;&#x2F;td&gt;&lt;td&gt;Main memory&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;Allocation Hints:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;NA - no allocate&lt;&#x2F;li&gt;
&lt;li&gt;RA - read allocate&lt;&#x2F;li&gt;
&lt;li&gt;WA - write allocate&lt;&#x2F;li&gt;
&lt;li&gt;RWA read &amp;amp; write allocate&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;As we can see the transaction attributes are expressive as it pertains to the treatment of the transaction.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;ace-axi-coherency-extensions&quot;&gt;ACE - AXI Coherency Extensions&lt;&#x2F;h2&gt;
&lt;p&gt;The first time we see coherency support in the series of AMBA specification is ACE, AXI Coherency Extensions.  Why do we focus on ACE when the goal is to learn CHI?  The concepts introduced in ACE continue on into CHI and as a learning vehicle it’s simpler to learn in the context of ACE than will the full complexity of the latest CHI specification.&lt;&#x2F;p&gt;
&lt;p&gt;It’s interesting that coherency support did not require a replacement of AXI but simply could be layered on top of AXI.  The way this is accomplished is by adding additional signals (DOMAIN, SNOOP, and BAR) alongside the standard AXI ones to apply additional attributes each transaction request.  In addition to added signal for transaction attributes an additional SNOOP channel is defined to allow the interconnect to send snoop and cache management requests into requesters.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;ace.png&quot; alt=&quot;ACE Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example System using ACE, from ARM CoreLink CCI-400 TRM&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;With ACE we can now build a system where caches can be snooped and invalidated enabling multiprocessing with shared memory (full coherency).  A subset of ACE called ACE-Lite is defined as well to enable coherency from IO devices initiating transactions that are expected to snoop processor caches but the IO devices themselves do not have any snoopable caches of their own (IO coherency, or one-way coherency).&lt;&#x2F;p&gt;
&lt;p&gt;In addition to all of the cache coherency goodness we see other concepts necessary for performant multiprocessing emerge.&lt;&#x2F;p&gt;
&lt;p&gt;ACE defines a set of transactions to enable Distributed Virtual Memory, or DVM.  While this sounds fancy DVM messages are required to enable efficient invalidations of virtual memory addresses across multiprocessors, namely TLB invalidates (after pagetable updates) and instruction cache invalidation (code loading or JITters).&lt;&#x2F;p&gt;
&lt;p&gt;ACE also defines a set of barrier messages to enable synchronization between weak&#x2F;relaxed ordered operations.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ace-chi-cache-states&quot;&gt;ACE&#x2F;CHI Cache States&lt;&#x2F;h3&gt;
&lt;p&gt;ACE and CHI employ a cache state model that allows MOESI style coherence but using entirely different terminology.  Fortunately the terminology is more concise than MOESI so it’s easier to reason about the meaning.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;ace-cache-states.png&quot; alt=&quot;ACE Cache States Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;ACE Cache States, from AMBA AXI and ACE Protocol Specification&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The cache states are as follows:&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;Invalid: Cache line not present in cache&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;Unique: Cache line only exists in one cache&lt;&#x2F;li&gt;
&lt;li&gt;Shared: Cache line might exist in more than one cache&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;Clean: This cache not responsible for updating main memory&lt;&#x2F;li&gt;
&lt;li&gt;Dirty: This cache is responsible for updating main memory&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h3 id=&quot;ace-chi-transactions&quot;&gt;ACE&#x2F;CHI Transactions&lt;&#x2F;h3&gt;
&lt;p&gt;Where complexity really starts to emerge are in the various transaction types.  These transactions encompass the desired treatment of the data as far as whether caches should be accessed at all (NoSnoop), what state the initiator’s caches will end up in, and the expected state of the target caches.  There are also transactions that are used for programmatic cache maintenance (e.g. “please flush these addresses to memory”) and even transactions that are intended to provide visibility to upstream snoop filters (Evict).&lt;&#x2F;p&gt;
&lt;p&gt;The &lt;em&gt;intent&lt;&#x2F;em&gt; of these transactions can be difficult to find - for example a ReadShared is used for a Read but a ReadUnique or MakeUnique is used to prepare for a write!&lt;&#x2F;p&gt;
&lt;p&gt;To help manage this complexity it’s useful to map the transaction types to typical use cases as shown below.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;ace-chi-transactions-from-processor-to-interconnect-read-write-requests&quot;&gt;ACE&#x2F;CHI Transactions from Processor to Interconnect (Read&#x2F;Write Requests)&lt;&#x2F;h4&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Transaction&lt;&#x2F;th&gt;&lt;th&gt;Meaning&lt;&#x2F;th&gt;&lt;th&gt;Typical Use&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;ReadNoSnoop&lt;&#x2F;td&gt;&lt;td&gt;Read, with no snooping&#x2F;coherency with other nodes&lt;&#x2F;td&gt;&lt;td&gt;Non-coherent memory or mmio access&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;WriteNoSnoop&lt;&#x2F;td&gt;&lt;td&gt;Write, with no snooping&#x2F;coherency with other nodes&lt;&#x2F;td&gt;&lt;td&gt;Non-coherent memory or mmio access&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadOnce&lt;&#x2F;td&gt;&lt;td&gt;Read, initiator will not cache, ends in invalid state&lt;&#x2F;td&gt;&lt;td&gt;Snapshot of data, non-temporal&#x2F;non-spatial&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadClean&lt;&#x2F;td&gt;&lt;td&gt;Read, initiator ends in clean state&lt;&#x2F;td&gt;&lt;td&gt;Write-through cache&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;WriteClean&lt;&#x2F;td&gt;&lt;td&gt;Write, initiator ends in clean state&lt;&#x2F;td&gt;&lt;td&gt;Speculative writeback&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadNotSharedDirty&lt;&#x2F;td&gt;&lt;td&gt;Read, unique-dirty OK, but shared-dirty not OK&lt;&#x2F;td&gt;&lt;td&gt;Data load or instruction fetch&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadShared&lt;&#x2F;td&gt;&lt;td&gt;Read, all states OK&lt;&#x2F;td&gt;&lt;td&gt;Data load or instruction fetch&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadUnique&lt;&#x2F;td&gt;&lt;td&gt;Read, initiator ends in unique-clean&#x2F;dirty state&lt;&#x2F;td&gt;&lt;td&gt;Prep for partial cache line store, initiator doesn’t have cache line&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;WriteUnique&lt;&#x2F;td&gt;&lt;td&gt;Write, initiator ends in shared-clean state&lt;&#x2F;td&gt;&lt;td&gt;Initiator without cache (e.g. IO) writing&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CleanUnique&lt;&#x2F;td&gt;&lt;td&gt;Clean other caches, initiator ends in unique state&lt;&#x2F;td&gt;&lt;td&gt;Prep for partial cache line store, initiator already has cache line&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CleanShared&lt;&#x2F;td&gt;&lt;td&gt;Clean all caches&lt;&#x2F;td&gt;&lt;td&gt;Explicit cache maintenance instructions in program&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CleanInvalid&lt;&#x2F;td&gt;&lt;td&gt;Clean and invalidate all caches&lt;&#x2F;td&gt;&lt;td&gt;Snoop filter back-invalidation&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;MakeUnique&lt;&#x2F;td&gt;&lt;td&gt;Remove other copies, initiator ends in unique-dirty&lt;&#x2F;td&gt;&lt;td&gt;Data store miss of full cache line&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;MakeInvalid&lt;&#x2F;td&gt;&lt;td&gt;Invalidate all caches&lt;&#x2F;td&gt;&lt;td&gt;Explicit cache maintenance instructions in program&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Evict&lt;&#x2F;td&gt;&lt;td&gt;Indicates cache line has been evicted&lt;&#x2F;td&gt;&lt;td&gt;No data transfer, only provides visibility to a snoop filter&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;WriteEvict&lt;&#x2F;td&gt;&lt;td&gt;Write clean cache line to lower cache level&lt;&#x2F;td&gt;&lt;td&gt;Eviction of clean cache line down the hierarchy&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Similarly we can map transactions arrive at a coherent node on the Snoop channel.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ace-chi-transactions-from-interconnect-to-processor-snoop-requests&quot;&gt;ACE&#x2F;CHI Transactions from Interconnect to Processor (Snoop Requests)&lt;&#x2F;h3&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Transaction&lt;&#x2F;th&gt;&lt;th&gt;Meaning&lt;&#x2F;th&gt;&lt;th&gt;Typical Use&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;ReadOnce&lt;&#x2F;td&gt;&lt;td&gt;Snoop read, initiator will not cache&lt;&#x2F;td&gt;&lt;td&gt;Enable the snooped processor to retain cacheline in unique state&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadClean&lt;&#x2F;td&gt;&lt;td&gt;Snoop read&lt;&#x2F;td&gt;&lt;td&gt;Initiator has a write-through cache&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadShared&lt;&#x2F;td&gt;&lt;td&gt;Snoop read, snooped to shared-dirty or shared-clean&lt;&#x2F;td&gt;&lt;td&gt;Initiator data load or instruction fetch&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadNotSharedDirty&lt;&#x2F;td&gt;&lt;td&gt;Snoop read, snooped to shared-dirty or invalid&lt;&#x2F;td&gt;&lt;td&gt;Initiator data load or instruction fetch&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;ReadUnique&lt;&#x2F;td&gt;&lt;td&gt;Snoop read, snooped must end in invalid state&lt;&#x2F;td&gt;&lt;td&gt;Prep for partial cache line store, initiator doesn’t have cache line&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CleanInvalid&lt;&#x2F;td&gt;&lt;td&gt;Clean and invalidate all caches&lt;&#x2F;td&gt;&lt;td&gt;Explicit cache maintenance instructions in program&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;MakeInvalid&lt;&#x2F;td&gt;&lt;td&gt;Invalidate all caches&lt;&#x2F;td&gt;&lt;td&gt;Explicit cache maintenance instructions in program&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CleanShared&lt;&#x2F;td&gt;&lt;td&gt;Clean all caches&lt;&#x2F;td&gt;&lt;td&gt;Explicit cache maintenance instructions in program&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;ace-coherency-responses&quot;&gt;ACE Coherency Responses&lt;&#x2F;h3&gt;
&lt;p&gt;Although the transaction types indicate the initiator’s desired end state, that’s not the full story.  The responder also has a say on the final state and can choose to pass ownership for writing back data or can indicate if the snooped is in a shared state.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;PassDirty&lt;&#x2F;strong&gt;
The node receiving this response gains responsibility for writing back the modified cacheline
If set, resulting state will be UniqueDirty or SharedDirty&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;IsShared&lt;&#x2F;strong&gt;
Indicates the returned data may be held in another cache
If set, resulting state will be SharedClean or SharedDirty&lt;&#x2F;p&gt;
&lt;p&gt;Some interesting scenarios can arise here - if a requester wants a clean line and a responder passes a dirty response then it’s the interconnect’s job to resolve this situation by writing data back to main memory &lt;em&gt;and&lt;&#x2F;em&gt; providing a clean response back to the requester.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ace-chi-snoop-filter&quot;&gt;ACE&#x2F;CHI Snoop Filter&lt;&#x2F;h3&gt;
&lt;p&gt;If the interconnect were to send snoop requests to every cache in the system we would see performance degradation due to excessive snoop traffic on the interconnect as well as excessive cache lookups as they check for matching addresses.&lt;&#x2F;p&gt;
&lt;p&gt;In computer architecture classes we learn about the directory methodology for coherence, where a directory (or set of directories) track the node IDs on a per-cache block basis.  In essence an ACE&#x2F;CHI Snoop Filter is conceptually the same, except we treat the Snoop Filter more like a cache than a directory.  The snoop filter will allocate an entry for any block address in an upstream cache so it can decide whether to forward coherency requests upstream or to ignore them.  In this case an entry simply consists of an address corresponding to the cache block - so it’s equivalent a cache itself with a tag and a valid bit but no data.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;ace-snoop-filter.png&quot; alt=&quot;ACE Snoop Filter Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;ACE System Using a Snoop Filter, from ARM CoreLink CCI-550 TRM&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Ideally, the Snoop Filter is configured with enough entries to track the state of all “upstream” cache blocks.  This means that snoop filter sizing must take into account all attached caches for optimal performance.&lt;&#x2F;p&gt;
&lt;p&gt;For example the ARM CCI 500 TRM advises:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;Arm recommends that you configure the snoop filter directory to be 0.75-1 times the total size of exclusive caches of &amp;gt; ?&amp;gt; processors that are attached to the CCI-500. The snoop filter is 8-way set associative and, to minimize conflicts, stores &amp;gt; twice as many tags as the configured size.&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;So what happens if a snoop filter runs out of space?  It’s not acceptable for the snoop filter to be out of sync from upstream caches as it would break the coherence protocol so instead the snoop filter itself will initiate an eviction from an upstream cache, called a back-invalidation.  This is something important to consider when sizing resources in a memory hierarchy as any benefit from larger caches on your processor can be negated by insufficient snoop filter sizing!&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Reduce coherency broadcasts&lt;&#x2F;li&gt;
&lt;li&gt;Track tags from upstream caches
&lt;ul&gt;
&lt;li&gt;A cache of tags without data&lt;&#x2F;li&gt;
&lt;li&gt;If capacity exceeded, perform a back-invalidation in upstream caches&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;Only issue snoop transactions in case of a filter hit&lt;&#x2F;li&gt;
&lt;li&gt;Requires additional ACE coherency messaging to maintain filter state
&lt;ul&gt;
&lt;li&gt;like Evict to inform filter when lines are invalidated due to capacity&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;chi&quot;&gt;CHI&lt;&#x2F;h2&gt;
&lt;p&gt;The CHI specification builds on AXI and ACE.  It defines a layered architecture enabling
distinct link layers with packets (Flits) riding on top.  It enables a network of nodes including arbitrarily complex topologies including rings and meshes as well as aligning to physical topologies like die-to-die and socket-to-socket topologies.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;chi-topologies.png&quot; alt=&quot;CHI Topologies Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Example CHI Topologies, from CHI Architecture Specification&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;CHI introduces formal concepts for coherency (Point of Coherency), ordering (Point of Serialization), persistence (Point of Persistence), and encryption (Point of Encryption).&lt;&#x2F;p&gt;
&lt;p&gt;Key in the CHI architecture are the roles that nodes take on.  Instead of previous generations which depicted a monolithic interconnect that could magically host caches or directories&#x2F;snoop filters, CHI formalizes the relationship between the nodes with caches, nodes that manage coherency for a portion of the address map, and nodes that serve as targets for main memory accesses.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;chi-nodes.png&quot; alt=&quot;CHI Node Types&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;CHI Node Examples, from CHI Architecture Specification&lt;&#x2F;em&gt;&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The first category of nodes is “Requester” which may have caches (RN-F, F meaning fully coherent) or may not have caches (RN-I, I meaning IO).&lt;&#x2F;p&gt;
&lt;p&gt;The second category of nodes is “Home” which, for addresses corresponding to cacheable memory (HN-F, meaning fully coherent) typically marks the Point of Coherency for some subset of the address space and can host system-level caches.  There is also a category of Home nodes for IO requests (HN-I) - no coherency to worry about here but the HN-I still services a role in ordering requests.&lt;&#x2F;p&gt;
&lt;p&gt;The last major category of nodes is the “Subordinate” which, for coherent addresses will host a memory controller or a bridge to some other memory target.&lt;&#x2F;p&gt;
&lt;p&gt;How does an RN know which HN node ID to talk to?  Conceptually there is a Requester-Node System Address Map (RN-SAM).  For performance reasons this SAM is usually co-located with the RN.  Similarly, how does an HN know which SN node ID to talk to?  It’s another address map, the Home Node System Address Map (HN-SAM).&lt;&#x2F;p&gt;
&lt;p&gt;Both the RN-SAM and HN-SAM can define interleaving to spread traffic across destinations.  RN-SAMs can interleave addresses across System-Level Caches and, indirectly Memory Controllers, and HN-SAMs can interleave addresses across multiple memory controllers serving that one HN.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;basic-chi-flow&quot;&gt;Basic CHI Flow&lt;&#x2F;h3&gt;
&lt;p&gt;The flow of transactions originates from an RN, targets an HN and then can proceed many places in the system depending on where the data resides.  Consider a case where we are performing a coherent data read from RN-F0.&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;RN-F0 looks in the RN-SAM and gets the node ID for the HN-F, say HN-F0.&lt;&#x2F;li&gt;
&lt;li&gt;HN-F0 gets the request and checks if its local SLC slice has the data.  If not it checks its snoop filter to see if any upstream RN-Fs have the data&lt;&#x2F;li&gt;
&lt;li&gt;HN-F0 determines from its snoop filter than RN-F1 has the data.  HN-F0 issues a read to RN-F1.&lt;&#x2F;li&gt;
&lt;li&gt;RN-F1 performs the read and replies.  CHI includes an optimization where these responses can be sent direct to the requester node and bypass the intermediate home node.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;It’s also possible that the address is not in any cache and the HN-F would:&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Look in the HN-SAM to find the node ID for the SN-F, say SN-F0.&lt;&#x2F;li&gt;
&lt;li&gt;SN-F0 performs the read and replies.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Now that we have a basic understanding of CHI topologies and coherence we can move on to some real (but simulated) systems and workloads.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;analyzing-chi-coherency-performance-with-gem5&quot;&gt;Analyzing CHI Coherency Performance with gem5&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;environment&quot;&gt;Environment&lt;&#x2F;h2&gt;
&lt;p&gt;A build environment was created on an Ubuntu 24.04 host using a docker container for building and executing gem5.  The gem5 git repo (tag v24.1.0.2) was cloned.  Since gem5 already provides a Dockerfile no special detective work was required to find dependencies.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;configuring-gem5&quot;&gt;Configuring gem5&lt;&#x2F;h2&gt;
&lt;p&gt;Two different emulation modes are provided with gem5.  The Full System emulation (FS) will emulate an entire machine from its reset vector enabling execution of boot firmware (like ARM TF-A or RISC-V OpenSBI) and then booting into a full OS.  This capability is important for projects that require OS interaction or modifications like memory management, scheduler work and any specialized driver or HAL&#x2F;BSP type of work.&lt;&#x2F;p&gt;
&lt;p&gt;For the CHI coherency project we started exporting FS mode and it was observed that a single Ubuntu boot takes multiple hours to complete (RV64 with a simple timing cpu model).  A rough comparison of performance is that about 3 seconds of OS boot execution corresponded to over an hour of simulation.  For Linux, it is possible to bypass the initialization of the OS distro and boot from a kernel and then set init to run &#x2F;bin&#x2F;bash but cycle times are still very slow compared to SE mode (below).  Given this performance would hamper any meaningful development we explored other modes of development.&lt;&#x2F;p&gt;
&lt;p&gt;The other mode of operation supported by gem5 is Syscall Emulation (SE).  In this mode user-space applications are executed and any time a call is made to an OS kernel the call is intercepted by gem5 and handled natively on the host.  This has the benefit of a major performance improvement over FS because with SE you can skip the OS boot process and begin executing the first program opcodes within a few seconds.  A downside of SE is that it does not match the OS behaviors for interacting with hardware which for an architectural simulator can be important when it comes to things like utilization of caches, TLBs and memory.  The Syscall Emulation mode also requires that syscalls that appear in programs are well-supported in gem5.  If a new syscall is added to, say, the Linux ABI then gem5 will need to be updated to faithfully handle (or sometimes ignore) the syscall so the program can function.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;syscall-emulation&quot;&gt;Syscall Emulation&lt;&#x2F;h2&gt;
&lt;p&gt;We began exploring SE mode for emulating the Linux kernel ABI and needed to confirm that SE would be capable of handling multiprocessing in one program - sending threads to other cores so we can see the appropriate interconnect and coherency effects.  Fortunately the gem5 source tree includes a basic multiprocessing test called ‘threads’ which distributes a matrix multiplication across all detected CPU cores.  It turns out that running this simple threads test exposed a deficiency in the capability of SE mode on some architectures.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;building-thread-test&quot;&gt;Building Thread Test&lt;&#x2F;h3&gt;
&lt;p&gt;By default this is only built for x86 but the makefile at tests&#x2F;test-progs&#x2F;threads&#x2F;src&#x2F;Makefile can be adjusted to add ARM and RISC-V architectures:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;..&#x2F;bin&#x2F;arm&#x2F;linux&#x2F;threads: threads.cpp
&lt;&#x2F;span&gt;&lt;span&gt;	aarch64-linux-gnu-g++ -static -o ..&#x2F;bin&#x2F;arm&#x2F;linux&#x2F;threads threads.cpp -pthread -std=c++11
&lt;&#x2F;span&gt;&lt;span&gt;
&lt;&#x2F;span&gt;&lt;span&gt;..&#x2F;bin&#x2F;riscv&#x2F;linux&#x2F;threads: threads.cpp
&lt;&#x2F;span&gt;&lt;span&gt;	riscv64-linux-gnu-g++ -static -o ..&#x2F;bin&#x2F;riscv&#x2F;linux&#x2F;threads threads.cpp -pthread -std=c++11
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Note the -static switch - without this a dynamic library will be built and if you are running cross-architecture some difficult dynamic library path fixups will be required.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;se-pitfalls&quot;&gt;SE Pitfalls&lt;&#x2F;h3&gt;
&lt;p&gt;Initially the intent was to use the ARM processor architecture for the CHI coherency work because it seemed like the most natural fit - the AMBA CHI specification originates from ARM, ARM products have the longest history with CHI, and the gem5 build for ARM selects CHI as it’s default coherency protocol.  So with an ARM variant of gem5 built, we tried to execute the ‘threads’ program in SE mode and it immediately failed.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;fatal: Syscall 435 out of range&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We inspected the syscalls supported in the gem5 source.  The syscall support is both OS- and architecture-specific.  For our ARM Linux case, we examined &lt;code&gt;src&#x2F;arch&#x2F;arm&#x2F;linux&#x2F;se_workload.cc&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;So for our failing syscall 435 we can examined the syscall emulation table for 64-bit ARM linux:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;class SyscallTable64 : public SyscallDescTable&amp;lt;EmuLinux::SyscallABI64&amp;gt;
&lt;&#x2F;span&gt;&lt;span&gt;...
&lt;&#x2F;span&gt;&lt;span&gt;        {  base + 292, &amp;quot;io_pgetevents&amp;quot;},
&lt;&#x2F;span&gt;&lt;span&gt;        {  base + 293, &amp;quot;rseq&amp;quot;, ignoreWarnOnceFunc },
&lt;&#x2F;span&gt;&lt;span&gt;        {  base + 294, &amp;quot;kexec_file_load&amp;quot;},
&lt;&#x2F;span&gt;&lt;span&gt;        { base + 1024, &amp;quot;open&amp;quot;, openFunc&amp;lt;ArmLinux64&amp;gt; },
&lt;&#x2F;span&gt;&lt;span&gt;        { base + 1025, &amp;quot;link&amp;quot; },
&lt;&#x2F;span&gt;&lt;span&gt;        { base + 1026, &amp;quot;unlink&amp;quot;, unlinkFunc },
&lt;&#x2F;span&gt;&lt;span&gt;        { base + 1027, &amp;quot;mknod&amp;quot;, mknodFunc },
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;and as we see there’s a gap that skips over 435.&lt;&#x2F;p&gt;
&lt;p&gt;How do we know what 435 means?  We could just look at glibc or linux kernel source code or through searching we can find a &lt;a href=&quot;https:&#x2F;&#x2F;www.chromium.org&#x2F;chromium-os&#x2F;developer-library&#x2F;reference&#x2F;linux-constants&#x2F;syscalls&#x2F;#aarch64_435&quot;&gt;quick reference&lt;&#x2F;a&gt; from the chromium project (chromium runs on the linux kernel, so that’s ok).  From this reference we see this is the clone3 syscall.  This is a series of syscalls that implements fork-like functionality as we can see from the &lt;a href=&quot;https:&#x2F;&#x2F;man.archlinux.org&#x2F;man&#x2F;clone3.2.en&quot;&gt;clone3 man page&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;How is it that a test included in gem5 fails on an architecture supported in gem5 SE?  This is likely due to the fact that the C library (glibc&#x2F;libstdc++) was updated at some point to change the underlying implementation for creating new threads.  The threads program does not call fork or clone directly, it uses the C++ standard threading library (std::thread) which, in turn selects the underlying syscall.  So simply by changing compilers we can select newer version of libraries that can make syscalls in a new way.  This means gem5 maintainers have a difficult job as they need to chase updates in libraries as they are made (or in our case, not).&lt;&#x2F;p&gt;
&lt;p&gt;Interestingly, for the clone3 syscall there is support present in gem5 for &lt;em&gt;other&lt;&#x2F;em&gt; architectures, specifically X86 and RISC-V::&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;src&#x2F;arch&#x2F;x86&#x2F;linux&#x2F;syscall_tbl64.cc:    { 435, &amp;quot;clone3&amp;quot;, clone3Func&amp;lt;X86Linux64&amp;gt; },
&lt;&#x2F;span&gt;&lt;span&gt;src&#x2F;arch&#x2F;riscv&#x2F;linux&#x2F;se_workload.cc:    { 435,  &amp;quot;clone3&amp;quot;, clone3Func&amp;lt;RiscvLinux64&amp;gt; },
&lt;&#x2F;span&gt;&lt;span&gt;src&#x2F;sim&#x2F;syscall_emul.hh:clone3Func(SyscallDesc *desc, ThreadContext *tc,
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;This led to the broader question - what other syscalls might we be missing?  Since we are building a plain linux executable, we can run it on a different machine and use strace under linux (the full OS this time) to trace syscall usage.  On an arm64 host&#x2F;vm&#x2F;qemu we launch the command under strace thusly:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;code&gt;strace -n -o strace.txt .&#x2F;threads&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;p&gt;and thanks to the -n switch we can see the syscalls including syscall number:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;[ 134] rt_sigaction(SIGRT_1, {sa_handler=0xfd15ae0c2840, sa_mask=[], sa_flags=SA_ONSTACK|SA_RESTART|SA_SIGINFO}, NULL, 8) = 0
&lt;&#x2F;span&gt;&lt;span&gt;[ 135] rt_sigprocmask(SIG_UNBLOCK, [RTMIN RT_1], NULL, 8) = 0
&lt;&#x2F;span&gt;&lt;span&gt;[ 222] mmap(NULL, 8454144, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0xfd15ad600000
&lt;&#x2F;span&gt;&lt;span&gt;[ 226] mprotect(0xfd15ad610000, 8388608, PROT_READ|PROT_WRITE) = 0
&lt;&#x2F;span&gt;&lt;span&gt;[ 135] rt_sigprocmask(SIG_BLOCK, ~[], [], 8) = 0
&lt;&#x2F;span&gt;&lt;span&gt;[ 435] clone3({flags=CLONE_VM|CLONE_FS|CLONE_FILES|CLONE_SIGHAND|CLONE_THREAD|CLONE_SYSVSEM|CLONE_SETTLS|CLONE_PARENT_SETTID|CLONE_CHILD_CLEARTID, child_tid=0xfd15ade0f230, parent_tid=0xfd15ade0f230, exit_signal=0, stack=0xfd15ad600000, stack_size=0x80ea20, tls=0xfd15ade0f8a0} =&amp;gt; {parent_tid=[36110]}, 88) = 36110
&lt;&#x2F;span&gt;&lt;span&gt;[ 135] rt_sigprocmask(SIG_SETMASK, [], NULL, 8) = 0
&lt;&#x2F;span&gt;&lt;span&gt;[ 222] mmap(NULL, 8454144, PROT_NONE, MAP_PRIVATE|MAP_ANONYMOUS|MAP_STACK, -1, 0) = 0xfd15acc00000
&lt;&#x2F;span&gt;&lt;span&gt;[ 226] mprotect(0xfd15acc10000, 8388608, PROT_READ|PROT_WRITE) = 0
&lt;&#x2F;span&gt;&lt;span&gt;[ 135] rt_sigprocmask(SIG_BLOCK, ~[], [], 8) = 0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;and cross-reference this to gem5’s syscall emulation table&lt;&#x2F;p&gt;
&lt;h4 id=&quot;switching-from-arm-to-risc-v-se&quot;&gt;Switching from ARM to RISC-V SE&lt;&#x2F;h4&gt;
&lt;p&gt;We ported clone3 syscall support from RISC-V to ARM and were able to successfully execute the threads program, however the number of cores detected was incorrect.  Digging into this deeper there was another deficiency in syscall emulation around handling special files used by glibc to detect the number of processor cores via special filesystem paths like &lt;code&gt;&#x2F;sys&#x2F;devices&#x2F;system&#x2F;cpu&#x2F;online&lt;&#x2F;code&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;This raised a question - is it possible that other architectures have better SE&#x2F;syscall support than ARM?  The same threads program was built and tested under gem5 in RISC-V mode and worked the first time with proper clone3 handling and the correct detection of number of CPU cores.  This led us to conclude that the ARM architecture is not being maintained quite as well in 2025 as X86 and RISC-V.  Since it is possible for CHI to be used with other architectures we switched from ARM to RISC-V at this point.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;build-gem5-for-risc-v-with-chi-coherency&quot;&gt;Build gem5 for RISC-V with CHI Coherency&lt;&#x2F;h4&gt;
&lt;p&gt;The default coherency protocol for gem5 for RISC-V is not CHI so we needed a way to get RISC-V and CHI working together.  Although most aspects of the system construction can be changed when starting gem5 without rebuilding it, switching to a different coherency protocol requires a specialized build configuration for gem5.  We had to create a new gem5 configuration file, placed at build_opts&#x2F;RISCV_CHI which specifies that we want RISCV but also to use RUBY and the CHI coherency protocol:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;RUBY=y
&lt;&#x2F;span&gt;&lt;span&gt;PROTOCOL=&amp;quot;CHI&amp;quot;
&lt;&#x2F;span&gt;&lt;span&gt;RUBY_PROTOCOL_CHI=y
&lt;&#x2F;span&gt;&lt;span&gt;BUILD_ISA=y
&lt;&#x2F;span&gt;&lt;span&gt;USE_RISCV_ISA=y
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We then built this RISCV_CHI variant and used it for the project.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;chi-topology-configuration-in-gem5&quot;&gt;CHI Topology Configuration in gem5&lt;&#x2F;h1&gt;
&lt;p&gt;With the project focused on understanding CHI coherency we explored the interconnect topology options already present in gem5.  For CHI, the following configurations are supported:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Crossbar&lt;&#x2F;li&gt;
&lt;li&gt;Pt2Pt&lt;&#x2F;li&gt;
&lt;li&gt;CustomMesh&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;pt2pt&quot;&gt;Pt2Pt&lt;&#x2F;h3&gt;
&lt;p&gt;The point-to-point is a single interconnect with direct connectivity between all nodes.  The  concept implies that requests can traverse the interconnect without having to wait for transaction buffer resources and without incurring additional hops along the way.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;pt2pt.png&quot; alt=&quot;Point-to-Point Diagram&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;from &lt;a href=&quot;https:&#x2F;&#x2F;www.gem5.org&#x2F;documentation&#x2F;general_docs&#x2F;ruby&#x2F;interconnection-network&#x2F;&quot;&gt;gem5 Interconnection Network&lt;&#x2F;a&gt; documentation&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h3 id=&quot;crossbar&quot;&gt;Crossbar&lt;&#x2F;h3&gt;
&lt;p&gt;Practically speaking an everything-to-everything topology is not realistic.  A common approach for small interconnects is a crossbar where the interconnect contains a crossbar switch allowing simultaneous streams of communication between separate initiators and targets.  The crossbar switch is itself a constrained resource because it is limited by internal connectivity and internal queues&#x2F;FIFOs resources.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;chi-xbar.png&quot; alt=&quot;Point-to Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;For small&#x2F;local interconnects the crossbar is a reasonable solution but for larger systems a is not practical and the crossbar switch cannot scale to larger numbers of initiators and targets in the system, both logically and physically.  If we think of a 128-core server SOC with dozens of PCIe and memory controllers we can see that something different is needed.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;mesh&quot;&gt;Mesh&lt;&#x2F;h3&gt;
&lt;p&gt;CustomMesh means that the topology is loaded from a file describing the interconnect configuration.&lt;&#x2F;p&gt;
&lt;p&gt;gem5 includes an example 2x4 custom mesh topology file (2x4.py) which defines the locations of node types in the mesh pattern, primarily RNFs for processors, HNFs for directories and system-level caches (SLCs) and main memory SNFs for memory controllers.&lt;&#x2F;p&gt;
&lt;p&gt;Mesh dimensions are defined as rows and columns:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;class NoC_Params(CHI_config.NoC_Params):
&lt;&#x2F;span&gt;&lt;span&gt;    num_rows = 2
&lt;&#x2F;span&gt;&lt;span&gt;    num_cols = 4
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;and which then implies mesh position numbers:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt; 0 --- 1 --- 2 --- 3
&lt;&#x2F;span&gt;&lt;span&gt; |     |     |     |
&lt;&#x2F;span&gt;&lt;span&gt; 4 --- 5 --- 6 --- 7
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;from here nodes are mapped onto the mesh, for example RNFs (processors) are mapped to the inside positions:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;class CHI_RNF(CHI_config.CHI_RNF):
&lt;&#x2F;span&gt;&lt;span&gt;    class NoC_Params(CHI_config.CHI_RNF.NoC_Params):
&lt;&#x2F;span&gt;&lt;span&gt;        router_list = [1, 2, 5, 6]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;along with HNFs:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;class CHI_HNF(CHI_config.CHI_HNF):
&lt;&#x2F;span&gt;&lt;span&gt;    class NoC_Params(CHI_config.CHI_HNF.NoC_Params):
&lt;&#x2F;span&gt;&lt;span&gt;        router_list = [1, 2, 5, 6]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;and curiously the default main memory SNFs allocated on the left-hand side:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;class CHI_SNF_MainMem(CHI_config.CHI_SNF_MainMem):
&lt;&#x2F;span&gt;&lt;span&gt;    class NoC_Params(CHI_config.CHI_SNF_MainMem.NoC_Params):
&lt;&#x2F;span&gt;&lt;span&gt;        router_list = [0, 4]
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;so there is some imbalance here as RNFs towards the left (positions 1 and 5) will have lower latency memory access compared to the RNFs towards the right (positions 2 and 6).&lt;&#x2F;p&gt;
&lt;p&gt;So combining all the information from the 2x4 NOC config we can visualize the topology with nodes placed:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;chi-mesh.png&quot; alt=&quot;CHI Mesh Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;mapping-gem5-resources-to-chi-topology&quot;&gt;Mapping gem5 Resources to CHI Topology&lt;&#x2F;h2&gt;
&lt;p&gt;When we invoke gem5 via the Syscall Emulation wrapper script, se.py we pass parameters and these get mapped by the CHI configuration logic into a specific topology.    In some cases there may not always be a perfect match between the resources specified on invocation and the topology described for the interconnect so the mapping process will fill available slots, like mapping gem5 CPU instances to CHI RN-f nodes.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Option&lt;&#x2F;th&gt;&lt;th&gt;Example&lt;&#x2F;th&gt;&lt;th&gt;Meaning&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;–cpu-type&lt;&#x2F;td&gt;&lt;td&gt;RiscvTimingSimpleCPU&lt;&#x2F;td&gt;&lt;td&gt;type of CPU to model - simple, in-order, out-of-order&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;–topology&lt;&#x2F;td&gt;&lt;td&gt;CustomMesh&lt;&#x2F;td&gt;&lt;td&gt;interconnect topology to use&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;–chi-config&lt;&#x2F;td&gt;&lt;td&gt;2x4.py&lt;&#x2F;td&gt;&lt;td&gt;topology configuraiton specifying location of node types&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;–num-cpus&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;Number of CPUs, maps to CHI RN-Fs&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;–num-dirs&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;Number of coherency directories, maps to CHI HN-Fs&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;–num-l3caches&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;Number of system-level caches, maps to CHI HN-Fs&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;One thing that is not possible with the mapping system is being able to specify different latencies for different links.  We hoped that we could model some links as having higher latency to simulate a multi-die or multi-socket NUMA scenario.  This limitation is not inherent in gem5 or even in CHI support, it’s only a limitation of the convenient mapping system provided.   It would be possible to more directly specify the configuration through customizing configuration scripts.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;note-about-deprecation-warnings-for-se-py&quot;&gt;Note About Deprecation Warnings for se.py&lt;&#x2F;h4&gt;
&lt;p&gt;Note: The gem5 scripts will warn that se.py is deprecated.  However for many configurations there are no alternatives in-tree that don’t use se.py, specifically the CHI.py module which requires the command-line options and system construction from se.py.  If you try to avoid this you end up going on a wild goose chase and end up back to realizing that se.py is the only way to go until the in-tree configurations are rewritten.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;thrasher-test-program&quot;&gt;Thrasher Test Program&lt;&#x2F;h1&gt;
&lt;p&gt;In order to ensure that interesting coherency traffic occurs we need a multi-processor environment where cores share data in the same cachelines.  We created a multi-threaded test case that is designed to generate coherency traffic by inducing false-sharing.&lt;&#x2F;p&gt;
&lt;p&gt;Using the C++ std::threading interface we created a test program that allocates a thread for each CPU where each thread repeatedly increments a 32-bit value in memory.  A parameter is defined to control the placement of this data to either induce or avoid false-sharing.&lt;&#x2F;p&gt;
&lt;p&gt;If we pack the values being incremented by CPUs together they will access the same cacheline to perform their read-modify-write to increment the value.  The memory layout and CPU accesses are depicted thusly:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;false-sharing-example.png&quot; alt=&quot;False Sharing Example Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;By increasing the stride of allocations we can ensure that the data is located far enough away to reside on different cachelines:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;img&#x2F;padded-example.png&quot; alt=&quot;Padded Data Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;thrasher-invocation&quot;&gt;Thrasher Invocation&lt;&#x2F;h2&gt;
&lt;p&gt;The program accepts one argument which is the stride in memory for the respective 32-bit values.  A stride of 1 will pack the values together so that values shared by different CPUs will occupy one 64-byte cacheline.  (We can fit up to 16 CPUs worth of uint32&#x2F;4-byte data in one 64-byte cacheline).&lt;&#x2F;p&gt;
&lt;p&gt;Alternatively, we can increase this stride to ensure that each CPU will access its own dedicated cacheline.  A stride value of 16 will pad the values to ensure that each 4B value will be spaced out 64B apart eliminating false sharing on the 64B-sized-cacheline architecture.  To verify proper placement, we print the address of each data element at the start of the test.&lt;&#x2F;p&gt;
&lt;p&gt;Here’s a 4-core packed configuration with a stride of 1 to induce false sharing:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;Running on 4 cores. with 10000 values and data stride of 1 uint32_t values
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c220
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c224
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c228
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c22c
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;And here’s the padded config with a stride of 16 to avoid false sharing:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;Running on 4 cores. with 10000 values and data stride of 16 uint32_t values
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c220
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c260
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c2a0
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c2e0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;h2 id=&quot;test-cases&quot;&gt;Test Cases&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;configuration&quot;&gt;Configuration&lt;&#x2F;h3&gt;
&lt;p&gt;We executed using gem5 SE with the CHI 2x4 mesh using 2 HN-Fs with caches.  Each CPU is a RiscvTimingSimpleCPU and each have their own L1 Data, L1 Instruction and Unified L2 caches.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Config&lt;&#x2F;th&gt;&lt;th&gt;Value&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;cache_line_size&lt;&#x2F;td&gt;&lt;td&gt;64 B&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.cpuN.l1d.cache.size&lt;&#x2F;td&gt;&lt;td&gt;64 KB,&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.cpuN.l1d.cache.assoc&lt;&#x2F;td&gt;&lt;td&gt;2-way&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.cpuN.l1dicache.size&lt;&#x2F;td&gt;&lt;td&gt;32 KB,&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.cpuN.l1dicache.assoc&lt;&#x2F;td&gt;&lt;td&gt;2-way&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.cpuN.l2.cache.size&lt;&#x2F;td&gt;&lt;td&gt;2MB&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.cpuN.l2.cache.assoc&lt;&#x2F;td&gt;&lt;td&gt;8-way&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.ruby.hnfN.cntrl.cache.size&lt;&#x2F;td&gt;&lt;td&gt;16MB&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;system.ruby.hnfN.cntrl.cache.assoc&lt;&#x2F;td&gt;&lt;td&gt;16-way&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Since our test will be focused on cache coherency thrashing, the sizes of caches are immaterial.  The cacheline size however is vital to understand the role of data placement and how it induces false sharing.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;memory-interleaving&quot;&gt;Memory Interleaving&lt;&#x2F;h4&gt;
&lt;p&gt;The CHI configuration subsystem automatically configures address interleaving across HNFs as a function of cache line size (64B in our system) and the # of HNFs (2 in our system):&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;        # Create the HNFs interleaved addr ranges
&lt;&#x2F;span&gt;&lt;span&gt;        block_size_bits = int(math.log(cache_line_size, 2))
&lt;&#x2F;span&gt;&lt;span&gt;        llc_bits = int(math.log(len(hnfs), 2))
&lt;&#x2F;span&gt;&lt;span&gt;        numa_bit = block_size_bits + llc_bits - 1
&lt;&#x2F;span&gt;&lt;span&gt;        for i, hnf in enumerate(hnfs):
&lt;&#x2F;span&gt;&lt;span&gt;            ranges = []
&lt;&#x2F;span&gt;&lt;span&gt;            for r in sys_mem_ranges:
&lt;&#x2F;span&gt;&lt;span&gt;                addr_range = AddrRange(
&lt;&#x2F;span&gt;&lt;span&gt;                    r.start,
&lt;&#x2F;span&gt;&lt;span&gt;                    size=r.size(),
&lt;&#x2F;span&gt;&lt;span&gt;                    intlvHighBit=numa_bit,
&lt;&#x2F;span&gt;&lt;span&gt;                    intlvBits=llc_bits,
&lt;&#x2F;span&gt;&lt;span&gt;                    intlvMatch=i,
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;So we get:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;block_size_bits = log2(64) = 6&lt;&#x2F;p&gt;
&lt;p&gt;llc_bits = log2(2) = 1&lt;&#x2F;p&gt;
&lt;p&gt;numa_bit = 6 + 1 - 1 = 6&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The interleaving is based on numa_bit (6) and routes to different HNFs based on this value - HNF0 gets transactions addresses where bit 6 is zero and HNF1 gets transactions whose address bit 6 is one.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;permutations&quot;&gt;Permutations&lt;&#x2F;h3&gt;
&lt;p&gt;We define 8 test permutations, varying the number of processors, and across either packed (stride 1) or padded (stride 16) configurations.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Case&lt;&#x2F;th&gt;&lt;th&gt;Cores&lt;&#x2F;th&gt;&lt;th&gt;Stride&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;1c-stride1&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2c-stride1&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;4c-stride1&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;8c-stride1&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;1c-stride16&lt;&#x2F;td&gt;&lt;td&gt;1&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;2c-stride16&lt;&#x2F;td&gt;&lt;td&gt;2&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;4c-stride16&lt;&#x2F;td&gt;&lt;td&gt;4&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;8c-stride16&lt;&#x2F;td&gt;&lt;td&gt;8&lt;&#x2F;td&gt;&lt;td&gt;16&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;data-collected&quot;&gt;Data Collected&lt;&#x2F;h2&gt;
&lt;p&gt;To analyze the effect of coherency on CPU performance we collect CPU cycles and cycles per instruction (CPI) as we expect the CPU performance to reduce (more cycles, higher CPI) as coherency traffic increases.&lt;&#x2F;p&gt;
&lt;p&gt;We also capture counters for CHI coherency traffic at the HN-Fs, snoops to L2 cache and snoops to L1 d-cache.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Stat&lt;&#x2F;th&gt;&lt;th&gt;Reason&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Cycles&lt;&#x2F;td&gt;&lt;td&gt;Compare processor performance reduction due to coherency traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;CPI&lt;&#x2F;td&gt;&lt;td&gt;Compare processor performance reduction due to coherency traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;L1&#x2F;L2 Snoops&lt;&#x2F;td&gt;&lt;td&gt;Compare amount of coherency traffic across different strides&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;HNF Snoops&lt;&#x2F;td&gt;&lt;td&gt;Examine coherency traffic as well as HN-F SAM effects&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h1 id=&quot;test-results&quot;&gt;Test Results&lt;&#x2F;h1&gt;
&lt;p&gt;We examine the CPU performance as measured across eight workloads representing the permutations of CPU core count and data stride inducing false-sharing.  We use total CPU cycles as a measure of performance as the same work is being done in all cases, so additional CPU cycles reveal time the CPU spends waiting for data.&lt;&#x2F;p&gt;
&lt;p&gt;Figure 1 depicts how CPU cycles increase in proportion to the number of CPU cores when false-sharing is induced.  When false-sharing is avoided we see the CPU cycle counts remain constant across CPU core counts.  This data demonstrates the significance of coherency on CPU performance.  It doesn’t matter how large or how plentiful the caches are, if shared data is placed poorly performance will suffer dramatically.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;png&#x2F;cycles.png&quot; alt=&quot;CPU Cycles&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;Figure 1: CPU Cycles&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;We examine traffic to the CPU L1 cache.  We focused on CPU0 as we expect similar traffic patterns across all CPUs given the symmetric nature of the workload.  This traffic reveals a stark contrast between the false-sharing and the no-false-sharing cases.&lt;&#x2F;p&gt;
&lt;p&gt;In Figure 2 the left three charts reflect outbound traffic from the L1 d-cache, from the CPU towards the interconnect.  The right two charts reflect inbound traffic to the L1 d-cache from the interconnect towards the CPU.&lt;&#x2F;p&gt;
&lt;p&gt;The SendReadShared is the response to a ReadShared transaction from the interconnect indicating another core is performing the read portion of the read-modify-write.&lt;&#x2F;p&gt;
&lt;p&gt;The SendReadUnique is the response to a ReadUnique transaction from the interconnect indicating preparation for a write where the data did not already exist in the cache of the requesting CPU.  Why would this be the case during a read-modify-write?  Another CPU can take ownership of the cacheline to do a write between this CPU’s read and write such that the write causes a ReadUnique.&lt;&#x2F;p&gt;
&lt;p&gt;The SendCleanUnique is the response to a CleanUnique from the interconnect indicating preparation for a write where the data already existed in the requesting CPU cache in a Shared state.  This reflects the upgrade in the read-modify-write sequence where the requesting CPU managed to retain the cacheline across the read and write.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;png&#x2F;l1-traffic.png&quot; alt=&quot;L1 Snoop Traffic&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;Figure 2: L1 D-Cache Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;In essence we are seeing two modes due to the timing of concurrent execution, one where the read-modify-write occurs with the cacheline remains resident in the originating CPU’s L1 data cache for the duration of the RmW operation and another mode where the cacheline is migrated in the middle of the RmW operation.&lt;&#x2F;p&gt;
&lt;p&gt;Note that we do see some surprising variations per-core which we suspect is due to other CPU caches holding data given the timing of cacheline movement.  We suspect that other CPUs will account for the missing data but this could not be confirmed at the time of writing.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;png&#x2F;l2-traffic.png&quot; alt=&quot;L2 Snoop Traffic&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;Figure 3: L2 Snoop Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;The L2 snoop traffic depicted in Figure 3 matches the snoop traffic we see to L1.  This makes  sense as the working set of this simply coherency test does not make use of L2 cache for capacity purposes.  Instead the L2 only serves as a hop along the way for coherency traffic.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;png&#x2F;hnf-readshared.png&quot; alt=&quot;HNF ReadShared Count&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;Figure 4: HNF ReadShared Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;Although the Home Nodes contain system-level (L3) caches, again as our working set is so small these are not used for storing data.  But the Home Nodes do serve as directories for main memory and we see that effect as the majority of traffic targets HNF0.  (Be sure to check the y-axis on the HNF0 and HNF1 charts as they vary by two orders of magnitude!)&lt;&#x2F;p&gt;
&lt;p&gt;Why HNF0?  Recall that memory is interleaved across HNFs at a 64B granularity.  With our data placement at:&lt;&#x2F;p&gt;
&lt;p&gt;stride-1:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;a[i] at address 0x 23c220
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c224
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c228
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c22c
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;stride-16:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;a[i] at address 0x 23c220
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c260
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c2a0
&lt;&#x2F;span&gt;&lt;span&gt;a[i] at address 0x 23c2e0
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;We can see in all cases that bit 6 is zero (0x20 -&amp;gt; 0b0&lt;strong&gt;0&lt;&#x2F;strong&gt;10_0000) so all accesses, for both packed and padded cases.  For this workload HNF interleave has no direct bearing on the result but it allows us to collect the coherency traffic counts under one node.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;png&#x2F;hnf-readunique.png&quot; alt=&quot;HNF ReadUnique Count&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;Figure 5: HNF ReadUnique Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;p&gt;As discussed when we analyzing the L1 data cache snoop traffic, we see both ReadUnique and CleanUnique reflecting the two modes of operation, intact RmW and split RmW sequences.&lt;&#x2F;p&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;chi-coherency&#x2F;png&#x2F;hnf-cleanunique.png&quot; alt=&quot;HNF CleanUnique Count&quot; &#x2F;&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td style=&quot;text-align: center&quot;&gt;Figure 5: HNF CleanUnique Traffic&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;We were able to comprehend the Coherent Hub Interconnect protocol and interconnect topologies that arise from it.  We followed the evolution of CHI from basic peripheral interconnects to multi-requester interconnects, support for complex topologies and more performance traffic flows and eventually full system coherency.  Although the protocol is quite complex we can simplify our understanding by mapping to the five basic CHI coherency states and reasoning about how and why we transition between those states.&lt;&#x2F;p&gt;
&lt;p&gt;We found that with some effort gem5 can be configured for either Full System or Syscall Emulation modes, learned how to select the coherency protocol at build time, and learned about system call compatibility in SE mode.  The gem5 CHI building blocks allow us to model complex topologies and simulate multiprocessor workloads accordingly.  We did discover limitations of gem5 CHI topology mapping and a more flexible topology specification system is necessary for modeling more realistic topologies.&lt;&#x2F;p&gt;
&lt;p&gt;We developed a test program that generates coherency traffic and was able to illustrate through test workloads of varying core counts and shared data placement that we could capture coherency traffic and reason about differences in the data.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;repo&quot;&gt;Repo&lt;&#x2F;h1&gt;
&lt;p&gt;The work for this project can be found at https:&#x2F;&#x2F;github.com&#x2F;eugenecohen-osu&#x2F;chi-gem5 which is a fork of gem5 with additional work on the chi branch.  The key steps to reproduce this is:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;.&#x2F;docker-run build-riscv-chi.sh
&lt;&#x2F;span&gt;&lt;span&gt;.&#x2F;docker-run run-all.sh
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Results for runs will appear in directory starting with the &lt;code&gt;out_&lt;&#x2F;code&gt; prefix with graphs generated in the &lt;code&gt;png&lt;&#x2F;code&gt; subdirectory.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;AMBA APB Protocol Specification: https:&#x2F;&#x2F;developer.arm.com&#x2F;documentation&#x2F;ihi0024&#x2F;latest&#x2F;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;AMBA AHB Protocol Specification: https:&#x2F;&#x2F;developer.arm.com&#x2F;documentation&#x2F;ihi0033&#x2F;latest&#x2F;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;AMBA AXI and ACE Protocol Specification: https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;
5f915b62f86e16515cdc3b1c&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;AMBA CHI Architecture Specification: https:&#x2F;&#x2F;developer.arm.com&#x2F;documentation&#x2F;ihi0050&#x2F;latest&#x2F;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;What is AMBA?: https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=CZlDTQzOfq4&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;ARM® CoreLink CCI-400 Cache Coherent Interconnect: https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;5e8f15d57100066a414f73ce&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Arm CoreLink CCI-550 Cache Coherent Interconnect: https:&#x2F;&#x2F;documentation-service.arm.com&#x2F;static&#x2F;5e7dd450cbfe76649ba52b0c&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;gem5 git: https:&#x2F;&#x2F;github.com&#x2F;gem5&#x2F;gem5&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;gem5 documentation: https:&#x2F;&#x2F;www.gem5.org&#x2F;documentation&#x2F;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>Exploring Disaggregated Memory Performance with Firecracker VMM</title>
                <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/explore-using-disaggregated-memory-with-the-firecracker-vmm/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/explore-using-disaggregated-memory-with-the-firecracker-vmm/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This project aims to test and compare aggregated and disaggregated memory performance when using Firecracker VMM to run virtual machines. This topic was of interest to the group because as programs get larger and memory becomes the bottleneck, disaggregated memory will become more common. Firecracker is a popular tool at the moment because of its ability to rapidly deploy Micro Virtual Machines due to it beinng Virutal Machine Monitor (VMM). Our research sought to combine the fields of disaggregated memory and Virtual Machines (VM). However, due to time and skill constraints, the project became more limited in scope. The new aim of the project was to compare the memory performance of benchmarks in a local machine versus in a Firecracker VMM to facilitate a discussion between the benefits and drawbacks of virtual machines. We expected that the local machine would have better memory performance than Firecracker because there are more overheads. We will also provide a proposal for how to attempt to implement disaggregated memory in Firecracker for exploring this topic more.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;&#x2F;h2&gt;
&lt;p&gt;The implementation for this project started with the standard version of Firecracker, which will be the base case for this project. Firecracker was built onto a local machine using the file provided on GitHub &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;firecracker-microvm&#x2F;firecracker&quot;&gt;here&lt;&#x2F;a&gt;. The local machine used to run the Firecracker VMM is a &lt;strong&gt;Lenovo LOQ 15APH8 laptop&lt;&#x2F;strong&gt; that contains:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;16 GiB of memory&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;1.5 TB disk capacity&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;AMD Ryzen 7 7840HS with Radeon 780M graphics&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Nvidia GeForce RTX 4050 Laptop GPU&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Ubuntu 24.04.02 LTS (64-bit)&lt;&#x2F;strong&gt;, which is compatible with Firecracker’s requirements.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The common setup of Firecracker requires Docker and Bash. We started with a basic &lt;code&gt;config.json&lt;&#x2F;code&gt; provided on the Firecracker GitHub. Due to technical issues with KVM access, we used a configured file alongside a given bin (&lt;code&gt;hello-vmlinux.bin&lt;&#x2F;code&gt;) and rootfs (&lt;code&gt;hello-rootfs.ext4&lt;&#x2F;code&gt;). These two files represent the kernel image and the root filesystem image, both essential for launching Firecracker.&lt;&#x2F;p&gt;
&lt;p&gt;In the &lt;code&gt;my_vm_config.json&lt;&#x2F;code&gt; file, these files were configured. Other configurations specified:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;vCPU count:&lt;&#x2F;strong&gt; 2&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Memory size:&lt;&#x2F;strong&gt; 512 MiB&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;To launch Firecracker, we used the following command:&lt;&#x2F;p&gt;
&lt;pre data-lang=&quot;bash&quot; style=&quot;background-color:#ffffff;color:#303030;&quot; class=&quot;language-bash &quot;&gt;&lt;code class=&quot;language-bash&quot; data-lang=&quot;bash&quot;&gt;&lt;span style=&quot;color:#f0523f;&quot;&gt;sudo&lt;&#x2F;span&gt;&lt;span&gt; firecracker&lt;&#x2F;span&gt;&lt;span style=&quot;color:#f0523f;&quot;&gt; --no-seccomp --config-file&lt;&#x2F;span&gt;&lt;span&gt; my_vm_config.json  
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Once running, we gathered system specifications:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;CPU:&lt;&#x2F;strong&gt; Two AMD Epyc processors with two cores each.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;OS:&lt;&#x2F;strong&gt; Alpine Linux 3.8, running on Kernel 4.14.55-84.37.amzn2.x86_64.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Memory:&lt;&#x2F;strong&gt; 501.8 MB (501852 KB).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The next step was to mount the STREAM benchmark executable. The VM retrieves data from a virtual disk, which serves as an init file containing necessary information to start a VM.
We decided to use the benchmark STREAM because it is a simple benchmark that measures sustainable main memory bandwidth in MB&#x2F;s and the corresponding computation rate for simple vector kernels. Once the VM config file structure contained the compiled file, the benchmark was able to be run within the virtual machine and test the memory bandwidth. It’s important to note that for the benchmark to work on the firecracker VMM, it needed to be statically compiled. For better results, we also ran the benchmark on the local machine that ran Firecracker. The benchmark program was obtained &lt;a href=&quot;https:&#x2F;&#x2F;www.cs.virginia.edu&#x2F;stream&#x2F;&quot;&gt;here&lt;&#x2F;a&gt;.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
&lt;p&gt;Using the STREAM benchmark, we ran 5 trials on each system to understand performance trends. The benchmark evaluates memory bandwidth and computation across four functions:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Copy&lt;&#x2F;strong&gt;: Measures memory read and write.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Scale&lt;&#x2F;strong&gt;: Measures how memory bandwidth handles arithmetic operations.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Add&lt;&#x2F;strong&gt;: Measures multi-source memory bandwidth.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Triad&lt;&#x2F;strong&gt;: Measures a mixture of memory patterns and computation.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;These assess data transfer performance, computational efficiency, parallel memory workloads, and overall memory performance. Figure 1 goes over “Best rate (MB&#x2F;s)”, Figure 2 goes over “Average time (ms)”, Figure 3 goes over “minimum time (ms)”, and Figure 4 goes over “maximum time (ms)”. In the trials, we found that the local machine performed better than the firecracker VMM. In the figures below, we see that the local machine takes less time and has a higher rate of MB&#x2F;s.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;figures&quot;&gt;Figures:&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;explore-using-disaggregated-memory-with-the-firecracker-vmm&#x2F;fig1.png&quot; alt=&quot;Figure1&quot; &#x2F;&gt;&lt;br &#x2F;&gt;
Figure 1: STREAM’s evaluation of Best Rate (MB&#x2F;s) on both Firecracker and Local Machine.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;explore-using-disaggregated-memory-with-the-firecracker-vmm&#x2F;fig2.png&quot; alt=&quot;Figure2&quot; &#x2F;&gt;&lt;br &#x2F;&gt;
Figure 2: STREAM’s evaluation of Average Time (ms) on both Firecracker and Local Machine.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;explore-using-disaggregated-memory-with-the-firecracker-vmm&#x2F;fig3.png&quot; alt=&quot;Figure3&quot; &#x2F;&gt;
Figure 3: STREAM’s evaluation of Minimum Time (ms) on both Firecracker and Local Machine.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;explore-using-disaggregated-memory-with-the-firecracker-vmm&#x2F;fig4.png&quot; alt=&quot;Figure4&quot; &#x2F;&gt;
Figure 4: STREAM’s evaluation of Maximum Time (ms) on both Firecracker and Local Machine.&lt;&#x2F;p&gt;
&lt;p&gt;Taking the average of all four functions of STREAM, we found:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Best Rate (MB&#x2F;s): STREAM has 7.2% less bandwidth on Firecracker compared to the local machine.&lt;&#x2F;li&gt;
&lt;li&gt;Average Time (s): STREAM is 6.9% slower on Firecracker than on the local machine on average.&lt;&#x2F;li&gt;
&lt;li&gt;Minimum Time (s): STREAM’s minimum time benchmark is 7.8% slower on Firecracker than the local machine.&lt;&#x2F;li&gt;
&lt;li&gt;Maximum Time (s): STREAM’s maximum time benchmark is 9.7% slower on Firecracker than the local machine.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;issues&quot;&gt;Issues&lt;&#x2F;h2&gt;
&lt;p&gt;We had many challenges over the course of this project which started with finding a machine to run linux. Originally our team wanted to use an Oregon State University (OSU) owned machine but there were many regulations with this machine. All programs or downloads had to be added through OSU’s Information Technology (IT) department so this had week turn around times between a program request and the implementation. For security reasons, OSU IT would not download Docker which made Firecracker unusable. At this point, our team had to pivot from using an OSU owned machine to a personal machine that has a Linux Operating System (OS) because this is the only allowed OS for Firecracker.&lt;&#x2F;p&gt;
&lt;p&gt;Our next challenge was to get a custom file structure for the VM setup with the compiled benchmark files using Docker. We did not need to use Docker even though the documentation says that Docker is required. This was an issue we were able to overcome.&lt;&#x2F;p&gt;
&lt;p&gt;Implementing disaggregated memory was difficult as it required creating virtual nodes that would simulate the pooled memory. With the switch to a personal computer, the process became difficult to set up, and unfortunately left out of the testing. Another issue was that Firecracker is written in Rust but none of the project members have any prior experience with Rust. An incorrect assumption our team made was that Rust would be an approachable language that we would have the time and resources to learn. This was not the case and posed a more significant challenge than we had anticipated.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;next-steps&quot;&gt;Next Steps&lt;&#x2F;h2&gt;
&lt;p&gt;Our proposal for implementing disaggregated memory support in Firecracker involves modifying several memory-related files in the Firecracker VMM kernel. These files are shown in the following directory map of Firecracker, where three asterisked-files are those that should be modified to support disaggregated memory:&lt;&#x2F;p&gt;
&lt;pre style=&quot;background-color:#ffffff;color:#303030;&quot;&gt;&lt;code&gt;&lt;span&gt;firecracker
&lt;&#x2F;span&gt;&lt;span&gt;├── CHANGELOG.md
&lt;&#x2F;span&gt;&lt;span&gt;├── CHARTER.md
&lt;&#x2F;span&gt;&lt;span&gt;├── CODE_OF_CONDUCT.md
&lt;&#x2F;span&gt;&lt;span&gt;├── CONTRIBUTING.md
&lt;&#x2F;span&gt;&lt;span&gt;├── CREDITS.md
&lt;&#x2F;span&gt;&lt;span&gt;├── Cargo.lock
&lt;&#x2F;span&gt;&lt;span&gt;├── Cargo.toml
&lt;&#x2F;span&gt;&lt;span&gt;├── DEPRECATED.md
&lt;&#x2F;span&gt;&lt;span&gt;├── FAQ.md
&lt;&#x2F;span&gt;&lt;span&gt;├── LICENSE
&lt;&#x2F;span&gt;&lt;span&gt;├── MAINTAINERS.md
&lt;&#x2F;span&gt;&lt;span&gt;├── NOTICE
&lt;&#x2F;span&gt;&lt;span&gt;├── PGP-KEY.asc
&lt;&#x2F;span&gt;&lt;span&gt;├── README.md
&lt;&#x2F;span&gt;&lt;span&gt;├── SECURITY.md
&lt;&#x2F;span&gt;&lt;span&gt;├── SPECIFICATION.md
&lt;&#x2F;span&gt;&lt;span&gt;├── THIRD-PARTY
&lt;&#x2F;span&gt;&lt;span&gt;├── deny.toml
&lt;&#x2F;span&gt;&lt;span&gt;├── docs
&lt;&#x2F;span&gt;&lt;span&gt;├── pre-commit
&lt;&#x2F;span&gt;&lt;span&gt;├── resources
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── chroot.sh
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── guest_configs
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── overlay
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── rebuild.sh
&lt;&#x2F;span&gt;&lt;span&gt;│   └── seccomp
&lt;&#x2F;span&gt;&lt;span&gt;│       ├── ***aarch64-unknown-linux-musl.json***
&lt;&#x2F;span&gt;&lt;span&gt;│       ├── unimplemented.json
&lt;&#x2F;span&gt;&lt;span&gt;│       └── ***x86_64-unknown-linux-musl.json***
&lt;&#x2F;span&gt;&lt;span&gt;├── rusty-hook.toml
&lt;&#x2F;span&gt;&lt;span&gt;├── src
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── acpi-tables
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── clippy-tracing
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── cpu-template-helper
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── firecracker
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── jailer
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── log-instrument
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── log-instrument-macros
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── rebase-snap
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── seccompiler
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── snapshot-editor
&lt;&#x2F;span&gt;&lt;span&gt;│   ├── utils
&lt;&#x2F;span&gt;&lt;span&gt;│   └── vmm
&lt;&#x2F;span&gt;&lt;span&gt;│       ├── benches
&lt;&#x2F;span&gt;&lt;span&gt;│       ├── Cargo.toml
&lt;&#x2F;span&gt;&lt;span&gt;│       ├── src
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── acpi
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── arch
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   ├── aarch64
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── cache_info.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── fdt.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── gic
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── ***layout.rs***
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── mod.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── output_GICv2.dtb
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── output_GICv3.dtb
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── output_initrd_GICv2.dtb
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── output_initrd_GICv3.dtb
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   ├── regs.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   │   └── vcpu.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   ├── mod.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │   └── x86_64
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── cpu_model.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── gdt.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── gen
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── interrupts.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── ***layout.rs***
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── mod.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── mptable.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       ├── msr.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   │       └── regs.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── builder.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── cpu_config
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── device_manager
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── devices
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── dumbo
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── gdb
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── io_uring
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── lib.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── logger
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── mmds
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── persist.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── rate_limiter
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── resources.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── rpc_interface.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── seccomp.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── signal_handler.rs
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── snapshot
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── test_utils
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── utils
&lt;&#x2F;span&gt;&lt;span&gt;│       │   ├── vmm_config
&lt;&#x2F;span&gt;&lt;span&gt;│       │   └── vstate
&lt;&#x2F;span&gt;&lt;span&gt;│       └── tests
&lt;&#x2F;span&gt;&lt;span&gt;├── tests
&lt;&#x2F;span&gt;&lt;span&gt;└── tools
&lt;&#x2F;span&gt;&lt;&#x2F;code&gt;&lt;&#x2F;pre&gt;
&lt;p&gt;Purposes of each file and why they would need to be modified to support disaggregated memory:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;firecracker&#x2F;resources&#x2F;seccomp&#x2F;x86_64-unknown-linux-musl.json&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;firecracker&#x2F;resources&#x2F;seccomp&#x2F;aarch64-unknown-linux-musl.json&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;These files contain the syscall handlers used for x86_64 and aarch64 architectures. Disaggregated-related syscalls would be different from regular local memory syscalls (when using RDMA), so that is why we believe these files would need to be modified to support those different syscalls (depending on architecture of course). This of course depends on the disaggregated memory implementation in the kernel - but if new syscalls were created to support disaggregated memory in the kernel, then these files would need to be modified to support those added syscalls. Note that with CXL disaggregated memory these files would remain the same, as CXL uses the same syscalls as regular local memory.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;firecracker&#x2F;src&#x2F;vmm&#x2F;src&#x2F;arch&#x2F;x86_64&#x2F;layout.rs&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;firecracker&#x2F;src&#x2F;vmm&#x2F;src&#x2F;arch&#x2F;aarch64&#x2F;layout.rs&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;These files contain the address maps of x86_64 and aarch64 architectures. They do not support disaggregated memory in these address maps. For CXL disaggregated memory support, we propose adding CXL memory mappings (start and size) into these files. For RDMA disaggregated memory support, we propose adding locally pinned RDMA memory into these files.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;The above files are what we would change to support disaggregated memory in Firecracker. For RDMA, the number of files needed to be modified would likely be larger than the subset that we highlight due to the fact that memory accesses via RDMA would need completely different interfaces than the regular POSIX-compliant syscalls used for local memory accesses. For CXL, the number of files needed to be modified would also likely be larger than the subset we highlight due to needing to use different names for CXL memory rather than local memory to prevent performance hits. It would be beneficial in that case to have distinct labels for local memory and CXL memory, where CXL memory would be used when local memory is low. This would require changes in files that use memory as well as the &lt;code&gt;layout.rs&lt;&#x2F;code&gt; files that we highlighted. The layout and syscall json files that we mention are likely just a starting point to get disaggregated memory supported in Firecracker.&lt;&#x2F;p&gt;
&lt;p&gt;Once the above files, and possibly others, are modified we would use firecracker to launch a virtual machine that supported disaggregated memory. Since we did not have an available system that supports disaggregated memory, we had planned to utilize server space through CloudLab. One server would support the Firecracker kernel with some memory to approximate a computer and another server would contain entirely memory to approximate a remote memory bank. The test would involve running memory benchmarks with the programs entirely contained in the first server versus with programs having spread their memory between the two servers. Ultimately, tests where entire programs won’t fit in the memory available on the first server would give a better idea of the performance of disaggregated memory because the program has to use this structure rather than arbitrarily using this structure. The benchmarks that we had picked were Parsec, NAS Parallel Benchmark, Graph500, Intel Memory Latency Checker, and STREAM.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h2&gt;
&lt;p&gt;This project went through several iterations before completion. The goal of the first version of this project was to modify the Firecracker VMM source code to provide disaggregated memory support and use performance benchmarks to test this version on CloudLab machines - comparing local aggregated memory performance to disaggregated memory performance. This version was far too ambitious for a single term, so the project’s scope was decreased. This second version of the project aimed to test performance benchmarks running in Firecracker VMM and natively and to provide a short proposal on how to support disaggregated memory in Firecracker VMM. We used the STREAM benchmark to test performance differences between these two options because of the relative ease of compilation of STREAM, and we successfully showed that Firecracker has a small hit (around 10%) to performance when compared to native performance. Our proposal states that for disaggregated memory support, the layout.rs files would need to be modified for CXL (and locally-pinned RDMA memory) disaggregated memory support. It also requires that the syscall json files would need to be modified to support RDMA syscalls. Lastly, our proposal states that more files would need to be changed to support switching between disaggregated memory and local memory for optimal performance.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;key-findings&quot;&gt;Key Findings:&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Firecracker incurs a small (~10%) performance hit compared to native execution.&lt;&#x2F;li&gt;
&lt;li&gt;Modifications required for disaggregated memory:
&lt;ul&gt;
&lt;li&gt;Modify &lt;code&gt;layout.rs&lt;&#x2F;code&gt; files for CXL&#x2F;RDMA memory support.&lt;&#x2F;li&gt;
&lt;li&gt;Modify syscall JSON files for RDMA syscalls.&lt;&#x2F;li&gt;
&lt;li&gt;Optimize memory allocation to distinguish between local and disaggregated memory.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.cs.virginia.edu&#x2F;stream&#x2F;FTP&#x2F;Code&#x2F;&quot;&gt;STREAM Benchmark&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.amd.com&#x2F;en&#x2F;developer&#x2F;zen-software-studio&#x2F;applications&#x2F;spack&#x2F;stream-benchmark.html&quot;&gt;AMD STREAM Benchmark&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;firecracker-microvm.github.io&#x2F;&quot;&gt;Firecracker Documentation&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;firecracker-microvm&#x2F;firecracker&quot;&gt;Firecracker GitHub&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;firecracker-microvm&#x2F;firecracker&#x2F;blob&#x2F;main&#x2F;docs&#x2F;rootfs-and-kernel-setup.md&quot;&gt;Firecracker Kernel Setup&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>Exploring Kernel Samepage Merging (KSM) with Modern Workloads: PyTorch, Redis, Memcached, and Stress-ng</title>
                <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/exploring-ksm-with-modern-workloads/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/exploring-ksm-with-modern-workloads/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;Kernel Samepage Merging (KSM) is a Linux feature that helps optimize memory usage by identifying and merging identical memory pages across processes. This can be really useful in environments and scenarios where memory efficiency is crucial. While KSM has been widely discussed in theory, its practical effectiveness often depends on workload characteristics and system configurations.&lt;&#x2F;p&gt;
&lt;p&gt;Testing KSM with more modern workloads such as PyTorch Redis, Memcached, and the stress-ng tool seemed straightforward enough, but various challenges got in the way of KSM detecting and merging the identical pages in each of the workloads. With each workload, KSM either failed to detect or successfully merge identical pages. This led to me going through the troubleshooting process as thorough as I could. Unfortunately, I wasn’t able to successfully troubleshoot these workloads completely, but I may have figured out a few reasons as to why KSM wasn’t merging any identical pages.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;&#x2F;h1&gt;
&lt;p&gt;KSM scans memory for identical pages, and if found, they’re merged into a single read-only page. This heavily reduces memory consumption depending on the workload. Conceptually, it sounds pretty simple, but there is a lot more to making sure the workloads are taking advantage of KSM. The modern workloads used to test KSM were PyTorch, Redis, Memcached, and stress-ng. They were ran inside a VirtualBox Ubuntu VM that didn’t support KVM. With each workload, the &lt;strong&gt;madvise(MADV_MERGEABLE)&lt;&#x2F;strong&gt; function  was used to designate what regions of memory were mergeable for KSM.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;pytorch&quot;&gt;PyTorch&lt;&#x2F;h1&gt;
&lt;p&gt;The PyTorch workload would allocate large tensors, essentially an array of numerical values, and mark them as mergeable with the madvise function. This was done through modifying the source code to ensure that the tensors being allocated would be seen and scanned by KSM, but unfortunately, several issues appeared. At first, there were dependency issues from Python’s externally managed environment restrictions that required setting up another virtual environment in the VM. Once getting past that obstacle, running the workload would fail, indicating that madvise calls were failing, returning -1. Getting past this obstacle wasn’t too difficult either, as the workload ended up running. Checking the KSM statistics after the initial attempt showed that KSM was not in fact scanning any pages, let alone merging any pages.&lt;&#x2F;p&gt;
&lt;p&gt;Thus began the first troubleshooting session. It included checking each KSM statistic and configurations, ensuring that everything was set up correctly. Ranging from increasing &lt;strong&gt;pages_to_scan&lt;&#x2F;strong&gt;, to decreasing the &lt;strong&gt;sleep_millisec&lt;&#x2F;strong&gt;, to doublechecking the source code, making sure the madvise calls were being called correctly, none of these attempts made any dent in this process.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;redis&quot;&gt;Redis&lt;&#x2F;h1&gt;
&lt;p&gt;With Redis, it was a bit simpler as there weren’t as many challenges. The original plan for this workload was to write a script that would apply the madvise function to the processes the benchmark was using while it was running. This ended up being ineffective because KSM was likely already looking at the memory used by the processes and ignored the madvise application after the benchmark began. The next step was to then modify the Redis benchmark to include the madvise function, directly applying it to its memory allocation functions, &lt;strong&gt;zmalloc&lt;&#x2F;strong&gt; and &lt;strong&gt;zcalloc&lt;&#x2F;strong&gt;. This was where the first complication happened. These functions use jemalloc, which may have affected KSM’s ability to effectively merge pages. This interference shows how important it is to consider the interaction between memory allocators and KSM to ensure optimal performance.&lt;&#x2F;p&gt;
&lt;p&gt;The troubleshooting process for this started off by comparing the original source code and the modified code for the benchmark. After a few attempts at isolating the effects of jemalloc, there was no concrete evidence indicating that jemalloc was the sole reason for KSM’s refusal to scan memory pages. Other potential factors were then explored, including the virtualization environment and  security mechanisms, but none of which were successfully identified as the cause.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;stress-ng&quot;&gt;Stress-ng&lt;&#x2F;h1&gt;
&lt;p&gt;The simplest approach in this experiment was to use stress-ng, a Linux tool designed to stress various system resources, including memory. When stressing memory specifically, the tool will continuously allocate memory and write to it. The plan for this workload was to do exactly that. The tool started 8 workers with 512 MB per worker. Letting it run should allow KSM to identify those pages, scan them, and merge any identical ones. Unfortunately, stress-ng doesn’t seem to have its “workers” fill the pages with identical data.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;&lt;em&gt;full_scans: 3978&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;general_profit: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;ksm_zero_pages: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;pages_scanned: 12360116&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;pages_shared: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;pages_sharing: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;pages_skipped: 332609&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;pages_unshared: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;pages_volatile: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;stable_node_chains: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;stable_node_dups: 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;
&lt;strong&gt;&lt;em&gt;Actual pages saved by KSM (pages_sharing + ksm_zero_pages): 0&lt;&#x2F;em&gt;&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As you can see in the KSM statistics above, roughly 12 million pages were scanned but none of them were merged. There were 300,000 pages skipped, which meant that they have previously not been merged. The missing 12 million pages are likely to be empty, which is why they were scanned, but nothing was done. They also weren’t included in the &lt;strong&gt;pages_skipped&lt;&#x2F;strong&gt; statistic because they likely weren’t considered as candidates for merging.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;memcached&quot;&gt;Memcached&lt;&#x2F;h1&gt;
&lt;p&gt;Memcached was a more controlled environment for testing KSM compared to the other workloads. The plan for this workload was to take a large amount of identical key-value store pairs and send them to a Memcached server after marking them as mergeable with the madvise function. This was automated using a script that would apply the madvise function to large amounts of identical key-value pairs, then sending them off to a Memcached server. Unfortunately, KSM didn’t seem to like this that much either, not scanning or merging any of the pages, so I tried something else. I took YCSB (Yahoo! Cloud Serving Benchmark) and tried to modify it to include madvise. This also didn’t work as there were various issues with the dependency modules. After some testing that included modifying the source code for the benchmark and exploring the built-in security mechanisms for Ubuntu, I realized one of the modules wasn’t even included when I downloaded YCSB to the VM, and when I went to the Github to look for it, it wasn’t there. That was when I realized that the code for YCSB and Memcached was likely to be outdated as the most recent commit for it was 6 years ago.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;troubleshooting&quot;&gt;Troubleshooting&lt;&#x2F;h1&gt;
&lt;p&gt;The main problem for each workload, except for stress-ng was that KSM wasn’t scanning or merging pages at all. The troubleshooting steps taken not specific to any workload was to check KSM’s configuration. To ensure KSM was even running in the first place, the command &lt;em&gt;cat &#x2F;sys&#x2F;kernel&#x2F;mm&#x2F;ksm&#x2F;run&lt;&#x2F;em&gt; would be ran. If it was running, the command would return a 1 if it was running, and a 0 if it wasn’t. KSM doesn’t scan that many memory pages by default, about 100 pages, but you can increase the scan amount by increasing the number from 100, to however much you want it to scan.&lt;&#x2F;p&gt;
&lt;p&gt;There weren’t many specific troubleshooting steps when it comes to each workload, as it was mainly check any modifications I made to the source code, and ensure I made the madvise function calls correctly. For Memcached, it seemed to just be outdated and I wasn’t able to effectively update it to run. For stress-ng, there wasn’t many issues because it just doesn’t seem like it’s a good workload for KSM. For Redis and PyTorch, it may have been a memory allocation conflict or an incorrect modification to include madvise.&lt;&#x2F;p&gt;
&lt;p&gt;To address these challenges, including workloads that have static memory pages, or pages that aren’t constantly changing, would be a much better fit for KSM, though it may not be as applicable to real-world applications. Additionally, performing these experiments on either a host machine or a VM that includes KVM support would likely produce better results as well.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;After an extensive testing and troubleshooting process, KSM did not effectively merge pages on any of the workloads tested. Something I noticed from this experiment is that KSM isn’t something that can be applied to every workload, at least not easily. Workloads that frequently modify memory aren’t necessarily the “right” workloads for KSM either as there will be little to no benefits because the merging process is much less effective and beneficial when there aren’t many identical pages, especially if they’re constantly changing. The type of memory allocation would also have to be taken into account as jemalloc would interfere with KSM’s merging, limiting its potential. Using VirtualBox for virtualization may have also plated a role in reducing KSM’s efficiency as without the KVM support, the performance may not have been its peak performance.&lt;&#x2F;p&gt;
&lt;p&gt;Among the four workloads, it seemed like Memcached was the best candidate to test KSM. However, even in the Memcached workload, the expected page merging was not achieved. Even if Memcached is one of the more favorable workloads, it doesn’t mean it’s a great one as there are likely other factors that are preventing the merging process from being successful and efficient.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;p&gt;YCSB&#x2F;Memcached Github: https:&#x2F;&#x2F;github.com&#x2F;brianfrankcooper&#x2F;YCSB&#x2F;tree&#x2F;master&#x2F;memcached
KSM documentation: https:&#x2F;&#x2F;docs.kernel.org&#x2F;admin-guide&#x2F;mm&#x2F;ksm.html
Pytorch: https:&#x2F;&#x2F;pytorch.org&#x2F;
Redis: https:&#x2F;&#x2F;redis.io&#x2F;
Memcached: https:&#x2F;&#x2F;memcached.org&#x2F;
Stress-ng: https:&#x2F;&#x2F;github.com&#x2F;ColinIanKing&#x2F;stress-ng and https:&#x2F;&#x2F;wiki.ubuntu.com&#x2F;Kernel&#x2F;Reference&#x2F;stress-ng&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>Automatic offloading of memory-intensive compute to nearby PIM modules</title>
                <pubDate>Wed, 12 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/pim-offload/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/pim-offload/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;PIM (Processing-In-Memory) is a computing paradigm where memory intensive operations are executed in DPUs (DRAM&#x2F;Data Processing Units) that are situated near if not within the DRAM (main memory) chip. PIM enables near data processing leading to higher memory bandwidth utilization and higher overall performance in workloads that consist of memory-intensive operations. This is true because PIM computation bypasses multi-level cache accesses in CPU avoiding all the costs associated with such accesses. The result is improved scalability and performance in many parallel workloads. This is extensively studied in research works such as PRIM benchmarks and SimplePIM framework.&lt;&#x2F;p&gt;
&lt;p&gt;However, these frameworks require programmers to be aware of applications’ dynamic memory characteristics to properly modify the program and re-compile it to reap the benefits of PIM offloading. But, in many cases, this is not necessarily true. To solve this, we propose automatically detecting memory-intensive parts of an application and offload that piece of computation to a nearby PIM device. We use memory profiling and analysis tool, MemGaze, to collect memory traces and metrics. We then use this data to extract memory-intensive regions (“hot sequences”) of the program.&lt;&#x2F;p&gt;
&lt;p&gt;A major challenge however is, how do we decide which functions are worth offloading to nearby PIM processors? To solve this problem we construct a cost model that takes in memory related metrics and clusters as input and output an associated cost for code regions. We can then use the cost to make the offload decision. This is the core contribution of this project.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;background&quot;&gt;Background&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;memgaze&quot;&gt;MemGaze&lt;&#x2F;h2&gt;
&lt;p&gt;MemGaze is a low overhead memory analysis tool that provides dynamic memory traces and memory access metrics such as footprint, re-use distance. MemGaze is also capable of analyzing the memory access patterns from the dynamic trace and classifies them into sequential, strided, indirect accesses. MemGaze injects &lt;code&gt;PTWRITE&lt;&#x2F;code&gt; instruction for each LD&#x2F;ST instruction in the target application binary. MemGaze then runs this instrumented binary to collect memory access samples and generates a whole program dynamic trace. These traces are further analyzed to generate aforementioned memory metrics and access classifications.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;upmem-pim-chips&quot;&gt;UPMEM PIM Chips&lt;&#x2F;h2&gt;
&lt;p&gt;UPMEM’s special DRAM chip come with general purpose DPUs (DRAM&#x2F;Data Processing Units) which enables PIM programming. UPMEM provides Linux kernel drivers, an SDK and a custom compiler to develop PIM programs that can run in these DPUs. Programmers can use UPMEM’s host API to initialize, launch and orchestrate DPU programs from the host (CPU) program.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;design-and-implementation-the-goal&quot;&gt;Design and Implementation (The Goal)&lt;&#x2F;h1&gt;
&lt;p&gt;Since this project is part of much bigger on-going project we outline the scope of this project in the below high level diagram i.e., the green highlighted area is the existing work and red high lighted area is the scope of this project.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;design.png&quot; alt=&quot;Design&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As the figure above illustrates, we designed a novel cost model to capture the cost of offloading compute (further discussed in “Cost Model” section). Before we talk about cost model, we first need to explain how we detect the memory intensive regions of a program. For this we use, MemGaze to collect traces and characterize the memory access intensity by using graph clustering to extract “hot sequences” as explained below.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;memory-characterization&quot;&gt;Memory Characterization&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;trace-analysis.png&quot; alt=&quot;Trace Analysis&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Steps to extract memory-intensive regions:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Collect the dynamic memory trace using MemGaze&lt;&#x2F;li&gt;
&lt;li&gt;Construct the IP (instruction pointer) transition graph where, &lt;code&gt;IP0&lt;&#x2F;code&gt; is followed by &lt;code&gt;IP1&lt;&#x2F;code&gt; &lt;code&gt;count&lt;&#x2F;code&gt; number of times&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;IP-transition.png&quot; alt=&quot;IP Transition&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Use Louvain clustering algorithm to cluster the transition graph&lt;&#x2F;li&gt;
&lt;li&gt;The clustered sub-graphs represent the “hot sequences” (memory intensive) code regions.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;modifying-memgaze&quot;&gt;Modifying MemGaze&lt;&#x2F;h2&gt;
&lt;p&gt;MemGaze outputs FP and RUD metrics at function level as an inclusive metric, i.e., if &lt;code&gt;main()&lt;&#x2F;code&gt; calls &lt;code&gt;foo()&lt;&#x2F;code&gt; (memory intensive function), &lt;code&gt;main()&lt;&#x2F;code&gt;’s footprint includes the footprint of&lt;code&gt;foo()&lt;&#x2F;code&gt;. This is not desirable because &lt;code&gt;main()&lt;&#x2F;code&gt; would be characterized as memory-intensive which is not true. To fix this, we edit MemGaze internals to spit out exclusive FP and RUD metrics for functions.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;cost-model-what-was-done&quot;&gt;Cost Model (What Was Done)&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;cost-model.png&quot; alt=&quot;Cost Model&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;A cost function model was meant to be a comprehensive pipeline that could detect bottlenecks in the code, decide if those code regions were pim friendly, and then decide if the code regions were worth offloading to UPMEM PIM hardware.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;C = W1 * F_PIM-norm + W2 * R_amortize + W3 * L_modularity&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;F_PIM-norm is a term that measures how well a workload matches the UPMEM PIM architecture’s strengths.  Specifically, it aggregates and scales factors such as Compute to Memory Ratio (CMR), Footprint (FP), Memory Bandwidth Demand (MBD), Access Intensity (AI), Parallelism Potential (PP), and Reuse Distance (RD). This linear model is highly interpretable, reduces correlation potential, and balances theoretical pim friendliness, with the UPMEM PIM specific idiosyncracies.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;F_PIM-norm = w1 · RD + w2 · AI + w3 · PP − w4 · CMR + w5 · MBD − w6 · FP&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;The R_amortize term measures whether the execution time of a workload on UPMEM PIM justifies the overhead of offloading it from the host CPU.&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;code&gt;R_amortize = Ttransfer + Tpim + Toverhead&lt;&#x2F;code&gt;
&lt;code&gt;Ttransfer = Data size &#x2F; BW&lt;&#x2F;code&gt;
&lt;code&gt;Tpim = Workload Ops &#x2F; PIM Effective Throughput&lt;&#x2F;code&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;p&gt;Toverhead is the time for data setup and formatting.&lt;&#x2F;p&gt;
&lt;p&gt;The L_modularity term identifies where the memory hotspots in the code are located.
To get this term, the workload graph will be reconstructed from the memgaze memory traces. Then, Leiden community detection will find clusters. These clusters will be refined through temporal and hierarchical analysis. Finally, the results will be mapped back to the source code to validate the findings.&lt;&#x2F;p&gt;
&lt;p&gt;This pipeline balances executing speed, feasibility of data collection, interpretability to non-experts, and robustness against misclassification by utilizing a layered approach.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;evaluation-were-we-successful&quot;&gt;Evaluation (Were We Successful)&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;test-setup&quot;&gt;Test setup:&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;INTEL(R) XEON(R) SILVER 4509Y&lt;&#x2F;li&gt;
&lt;li&gt;32 Cores, 500G RAM&lt;&#x2F;li&gt;
&lt;li&gt;UPMEM Functional Simulator&lt;&#x2F;li&gt;
&lt;li&gt;Benchmarks are run with single thread implementation&lt;&#x2F;li&gt;
&lt;li&gt;Offload if cost &amp;gt; threshold&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;benchmarks&quot;&gt;Benchmarks&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Vector Addition&lt;&#x2F;li&gt;
&lt;li&gt;Reduction&lt;&#x2F;li&gt;
&lt;li&gt;Histogram&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;cost-breakdown-for-vector-addition&quot;&gt;Cost breakdown for Vector Addition&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Function&lt;&#x2F;th&gt;&lt;th&gt;Offload Cost (0 to 1)&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;code&gt;main&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;0.8&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;code&gt;init_arrays&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;0.6&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;code&gt;vector_add&lt;&#x2F;code&gt;&lt;&#x2F;td&gt;&lt;td&gt;0.4&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;h2 id=&quot;speed-up-due-to-offloading&quot;&gt;Speed up due to offloading&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;eval-result.png&quot; alt=&quot;Reults&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h1 id=&quot;challenges-the-hardest-parts-to-get-right&quot;&gt;Challenges (The Hardest Parts To Get Right)&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Which clustering algorithm is more suitable for PIM based workloads?&lt;&#x2F;li&gt;
&lt;li&gt;Which metrics are useful for the cost function?&lt;&#x2F;li&gt;
&lt;li&gt;How to choose coefficients for cost function?&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;testing-related-challenges&quot;&gt;Testing related challenges:&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Upmem sdk + simulator learning curve&lt;&#x2F;li&gt;
&lt;li&gt;NixOS learning curve&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;exploring-investigating-pim-suitable-metrics-and-clustering-algorithms&quot;&gt;Exploring&#x2F;Investigating PIM suitable Metrics and Clustering Algorithms&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;louvain-vs-other-clustering-algorithms&quot;&gt;Louvain vs Other Clustering Algorithms&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;clustering1.png&quot; alt=&quot;Clustering 1&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;clustering2.png&quot; alt=&quot;Clustering 2&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Louvain was previously used for clustering MemGaze IP transition graphs, but after investigation, it was discovered that Leiden delivered better accuracy (0.145 score vs. Louvain 0.142), 3.5x faster inference speed (14.39s vs. 49.76s), and better scalability (16362.5 vs. 85218.7 ratio). Leiden should be used in the final cost function, but Louvain is fine for the MVP.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;linear-regression-vs-other-ml-models&quot;&gt;Linear Regression vs other ML Models&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;LR-vs-others.png&quot; alt=&quot;LR vs other models&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The cost model relies on a linear regression-based approach to compute &lt;code&gt;F_PIM-norm&lt;&#x2F;code&gt;, assessing PIM-friendliness for UPMEM offloading. On a synthetic dataset of 31 workloads, it achieved 87.1% accuracy and 100% recall, correctly identifying all PIM-friendly cases, but its precision (81.82%) reflects some false positives, like misclassifying compute-heavy workloads due to a low threshold. Compared to XGBoost, which scored 90.32% accuracy and 92.74% ROC-AUC, linear regression trades precision (81.82% vs. 89.47%) for perfect recall, missing no offloading opportunities. XGBoost, with a 94.44% recall, missed a few PIM-friendly cases but balanced this with fewer errors (F1: 91.89% vs. 90.00%), leveraging non-linear modeling for better calibration (MAE 0.1698 vs. 0.4917, R² 0.6224 vs. -0.1578).
For the MVP, linear regression’s simplicity (50 lines, no training) and interpretability outweigh XGBoost’s complexity (100 lines, training required), despite the latter’s edge in accuracy and robustness. XGBoost’s explainability concerns can be mitigated with tools like LIME and SHAP, making it a strong candidate for the final cost function model.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;uniform-vs-other-sampling-methods&quot;&gt;Uniform vs Other Sampling Methods&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;uniform-vs-bursty.png&quot; alt=&quot;Uniform vs other&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;MemGaze uses uniform sampling to get memory traces. Uniform sampling is fine given its alignment with Intel PT’s high-speed, low-overhead requirements, especially for mixed or unknown hardware trace patterns. Its performance at low proportions (2%-5% coverage at 1%-2%) is acceptable as a general-purpose method, and its simplicity ensures it will not bottleneck tracing. However, for workloads with predictable patterns such as periodic loops or hotspots, a pattern-aware switch to Cluster or Adaptive sampling could improve coverage without sacrificing speed. If profiling reveals pattern-specific deficiencies, Adaptive sampling should be implemented.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;bandwidth-and-footprint-footprint-growth-correlation&quot;&gt;Bandwidth and Footprint&#x2F;Footprint Growth Correlation&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;pim-offload&#x2F;bandwidth-vs-footprint.png&quot; alt=&quot;Bandwidth vs Footprint&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Memory bandwidth and footprint (F) or footprint growth (ΔF) show little correlation across general workloads, as F reflects capacity. In UPMEM HPC tests (Vector Addition), a weak positive link (0.24) emerged between bandwidth and ΔF when F exceeded cache or used sequential access, hitting UPMEM’s 50 GB&#x2F;s cap. The takeaway is that PIM friendliness metrics had to be checked for correlation to improve the linear model results.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;what-was-surprising&quot;&gt;What Was Surprising&lt;&#x2F;h1&gt;
&lt;p&gt;Comment from Noah:&lt;&#x2F;p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Reality is more robust than academia led me to believe. I thought that picking specific methods like Louvain or Leiden, linear regression or XGBoost, uniform or adaptive sampling would make or break the results, but the less complicated methods did just fine.”&lt;&#x2F;p&gt;
&lt;&#x2F;blockquote&gt;
&lt;h1 id=&quot;future-extensions&quot;&gt;Future Extensions&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;model&quot;&gt;Model&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Improve model iteratively&lt;&#x2F;li&gt;
&lt;li&gt;Measure the accuracy of model&lt;&#x2F;li&gt;
&lt;li&gt;Implement as a runtime library&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;implement-remaining-steps&quot;&gt;Implement remaining steps&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Automatically compile DPU binary&lt;&#x2F;li&gt;
&lt;li&gt;Create combined host+DPU fat binary&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;benchmarks-1&quot;&gt;Benchmarks&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Evaluate remaining workloads from SimplePIM paper&lt;&#x2F;li&gt;
&lt;li&gt;Repeat this on real-world applications&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;In conclusion, we have presented the idea of automatically detecting memory intensive parts of a program and proposed a cost model that can be used to select which of those memory intensive parts need to be offloaded to a PIM device. In addition, we have explored various memory metrics and graph clustering algorithms to find a suitable candidate that can capture PIM memory characteristics. Finally, we have evaluated our approach on three different representative benchmarks, showing speedup of up to 1.3x.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2012.03112&quot;&gt;A Modern Primer on Processing-In-Memory&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2105.03814&quot;&gt;Benchmarking a New Paradigm: An Experimental Analysis of a Real Processing-in-Memory Architecture&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1109&#x2F;PACT58117.2023.00017&quot;&gt;SimplePIM: A Software Framework for Productive and Efficient Processing-in-Memory&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;9912656&quot;&gt;MemGaze: Rapid and Effective Load-Level Memory Trace Analysis&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.upmem.com&#x2F;developer&#x2F;&quot;&gt;UPMEM SDK&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h1 id=&quot;division-of-responsibilities&quot;&gt;Division of Responsibilities&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Nanda: Initial project setup, design, implementation and evaluation&lt;&#x2F;li&gt;
&lt;li&gt;Noah: Constructing&#x2F;investigating cost model, PIM-friendly metrics and evaluation&lt;&#x2F;li&gt;
&lt;li&gt;Both of us equally contributed to slides and blog post write up.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>SimplePIM: A Software Framework for Productive and Efficient Processing-in-Memory</title>
                <pubDate>Mon, 10 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/simplepim/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/simplepim/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;PIM stands for Processing-In-Memory and as the name suggests the idea here is that, instead of doing compute in CPU, it can be done in the DRAM by embedding a processing unit within it, this is usually referred to as DPU (DRAM Processing Unit) [1]. The obvious benefit of having such a mechanism is that, memory bound workloads avoid the DRAM-to-CPU (and vice versa) communication overhead. This is shown in many common workloads where offloading such memory bound compute to DPUs results in better throughput and overall performance [2].&lt;&#x2F;p&gt;
&lt;h1 id=&quot;design-and-implementation&quot;&gt;Design and Implementation&lt;&#x2F;h1&gt;
&lt;p&gt;SimplePIM is a software framework that provides a convenient userspace API that allows programmers to easily implement PIM features into their applications. SimplePIM does this by providing a set of PIM programming primitives for managing, communicating and processing data structures between host (CPU) and DPU.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;simplepim-api&quot;&gt;SimplePIM API&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Management Interface: &lt;code&gt;simple_pim_array_lookup()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_register()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_free()&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Communication Interface: &lt;code&gt;simple_pim_array_broadcast()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_scatter()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_gather()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_allreduce()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_allgather()&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Processing Interface: &lt;code&gt;simple_pim_create_handle()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_map()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_red()&lt;&#x2F;code&gt;, &lt;code&gt;simple_pim_array_zip()&lt;&#x2F;code&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;optimizations&quot;&gt;Optimizations&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Strength reduction&lt;&#x2F;li&gt;
&lt;li&gt;Loop unrolling&lt;&#x2F;li&gt;
&lt;li&gt;Avoiding boundary checks&lt;&#x2F;li&gt;
&lt;li&gt;Function inlining&lt;&#x2F;li&gt;
&lt;li&gt;Adjustment of data transfer sizes&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;results&quot;&gt;Results&lt;&#x2F;h1&gt;
&lt;p&gt;SimplePIM results in, up to 83% reduction in lines of code and improves productivity up to 5.93x. Along with these improvements, SimplePIM speeds up workloads by 1.10x - 1.43x when compared to their hand-optimized implementations.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why is PIM being explored more now?&lt;&#x2F;strong&gt; Things like graph analytics and computations that do not have a lot of cache access can benefit from this type of hardware&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Do you know of any other companies that are making PIM other than UPMEM?&lt;&#x2F;strong&gt; Sk hinex and samsung, samsung is not generally purpose however.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Have you taken any functional programming courses?&lt;&#x2F;strong&gt; They might help with this paper due to the idea of arrays and maps&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Do we know what a histogram calculation is?&lt;&#x2F;strong&gt; Counting the number of occurrences in a data set, if there are repeating entries it will tell you how many times those entries have repeated&lt;&#x2F;li&gt;
&lt;li&gt;Strong scaling vs weak scaling: when you increase the number of cores or nodes that is strong scaling the program stays fixed, you end up dividing the problem over a number of ever increasing cores. weak scaling is meant to change the program, the nodes always have a fixed amount of work to do, the problem is growing as you scale up since each node always has the same amount of work to do.&lt;&#x2F;li&gt;
&lt;li&gt;SimplePIM, is it compatible with the current versions of the Samsung PIM, not currently you would probably have to work to implement them&lt;&#x2F;li&gt;
&lt;li&gt;Compare the performance to hand optimized code seems like a bit of a weakness but I’m not sure what else they would do. They point out it’s from a library, from nanda’s experience Yeah, they have to compare it to that code since they have nothing else to compare it to. &lt;strong&gt;What if they rigged the hand optimized code?&lt;&#x2F;strong&gt; hand optimized is quite a subjective term and is rather arbitrary.&lt;&#x2F;li&gt;
&lt;li&gt;Lots of compiler papers from the 80’s was like this hand optimized code section since compilers were a lot worse back then&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;You are compensating for the fact that you have a strong processor(code optimizations slide), did they put the correct type of processor in the memory? Was this the correct compromise? Did they have enough space on the memory physically?&lt;&#x2F;strong&gt; We could implement caches into these to get rid of some of the issues. Not sure if they picked the best CPU&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Why is this on the DRAM bus, would PCI&#x2F;mem better for this type of work?&lt;&#x2F;strong&gt; That is a fair point, we can consider expanding the hardware to have interconnection and native floating point.&lt;&#x2F;li&gt;
&lt;li&gt;If you talk to a computer architect, they would say this is programming near memory not in&lt;&#x2F;li&gt;
&lt;li&gt;X86 architecture, DPU RISC-V so there is a 5-10% overhead just to ship the data between the two&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Are they adjusting the runtime manually?&lt;&#x2F;strong&gt; We think they adjust them via the compiler to use large DMA transfer sizes when all the accessed data is going to be used. Otherwise manually&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Do they talk about the hardware in this paper?&lt;&#x2F;strong&gt; There might be UPMEM in the paper,&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;What are the internals of the UPMEM engine?&lt;&#x2F;strong&gt; They don’t publish it&lt;&#x2F;li&gt;
&lt;li&gt;I think there might be a point where GPU and PIM might merge, dont you eventually just approximate PIM? GPU and memory get so close to where we just use one to do the other. Would we ever want to do this or is there some law of physics where it makes it impossible?&lt;&#x2F;li&gt;
&lt;li&gt;The new tesla supercomputer (dojo) might be interesting to look at&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;pdf&#x2F;2012.03112&quot;&gt;A Modern Primer on Processing-In-Memory&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;abs&#x2F;2105.03814&quot;&gt;Benchmarking a New Paradigm: An Experimental Analysis of a Real Processing-in-Memory Architecture&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Paper: &lt;a href=&quot;https:&#x2F;&#x2F;people.inf.ethz.ch&#x2F;omutlu&#x2F;pub&#x2F;SimplePIM_pact23.pdf&quot;&gt;https:&#x2F;&#x2F;people.inf.ethz.ch&#x2F;omutlu&#x2F;pub&#x2F;SimplePIM_pact23.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Slides: &lt;a href=&quot;https:&#x2F;&#x2F;events.safari.ethz.ch&#x2F;micro-pim-tutorial&#x2F;lib&#x2F;exe&#x2F;fetch.php?media=realpimtutorial-micro23-simplepim-juan-slides.pdf&quot;&gt;https:&#x2F;&#x2F;events.safari.ethz.ch&#x2F;micro-pim-tutorial&#x2F;lib&#x2F;exe&#x2F;fetch.php?media=realpimtutorial-micro23-simplepim-juan-slides.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</description>
            </item>
        
            <item>
                <title>Managing Memory Tiers with CXL in Virtualized Environments</title>
                <pubDate>Wed, 05 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/memtiers/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/memtiers/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;The topic of this blog is to discuss the paper “Managing Memory Tiers with CXL in Virtualized Environments”. This paper discussed CXL, a high-speed interconnect standard designed to expand memory capacity and enable efficient memory sharing across devices. The key challenge lies in managing latency-sensitive workloads in virtualized environments where traditional software-managed tiering incurs prohibitive overheads.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;executive-summary&quot;&gt;Executive Summary&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;background&quot;&gt;Background:&lt;&#x2F;h3&gt;
&lt;p&gt;CPU core counts scaling faster than memory capacity
CXL enables second-tier memory to facilitate core scaling
But CXL adds latency that hurts performance if not mitigated
Software tiering helps some but is not well suited for public clouds
Contributions:
Intel Flat Memory Mode: First hardware-managed memory tiering for CXL
But still has limitations that degrade workloads
Memstrata: Memory allocator for hardware tiering to mitigate outliers
Slowdown reduces to ~5% vs. unattainable one-tier memory&lt;&#x2F;p&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion:&lt;&#x2F;h2&gt;
&lt;p&gt;So I think that as of this month you can buy Xeon 6 flat memory chips. It also has pcores - performance cores.
hybrid cpu arches - mobile chips, also released for desktop chips, and now recently for
diff arches for cores: small and weak (ecores) cores vs strong  (pcores) cores&lt;&#x2F;p&gt;
&lt;p&gt;Is this related to dark silicon?
Not quite as different as proposals as other dark silicon ideas
starting 2006: concern that you could not power all parts of a chip. -&amp;gt; led to proposals to turn off unused portions of chip to save power.
The current philosophy is to use parts that are low power and use other parts that are higher power.
The trend of increasing power usage is plateauing.&lt;&#x2F;p&gt;
&lt;p&gt;Why is there a hump on the graph comparing core count to memory capacity per core?
Memory capacity got more efficient in 2016.
Counts could also be averaged, and on AMD and Intel processors.&lt;&#x2F;p&gt;
&lt;p&gt;Intel flat memory mode - direct mapped exclusive cache. How much of the slowdown is caused by conflict misses? You could still get significant cache misses even with page coloring due to the direct mapped nature of the cache.
Cutting address space in half. Because cache is exclusive, two memory locations map to a given cache line. To have a system with no conflict misses, you would need a 2-way set associative cache. No clue why they did not try 2-way associative cache.
Tag comparisons are done in parallel in the DRAM with n-way associative caches
This would be intrusive and this paper is supposed to integrate into a product, so I guess they were conservative.&lt;&#x2F;p&gt;
&lt;p&gt;Did Intel model associativity with these workloads? How much is Memstrata compensating for limited associativity in the cache?&lt;&#x2F;p&gt;
&lt;p&gt;They do not talk about how tagging is done in the paper. Could they have presented differences in associativity to performance?&lt;&#x2F;p&gt;
&lt;p&gt;Noisy neighbor problem (other VM is using lots of memory bandwidth on the same chip). There exist hardware solutions to this (ARM and x86). Use a hardware identifier to determine different VMs and even cache partitioning. Why is the noisy neighbor problem such a large problem if solutions already exist? Does Intel flat memory not implement these existing hardware solutions?
ARM: MPAM, x86: RDT (Resource Director Technology)
Gives x number of bandwidth per VM
Maybe it does not translate to CXL very well?
These existing technologies exist on the core - the paper’s hardware exists on the memory controller, so these existing technologies would have to be copied onto the memory controller as well
Maybe since this is first Gen it is not as ambitious?&lt;&#x2F;p&gt;
&lt;p&gt;Which VM is using more local memory vs. CXL memory - use this information to swap pages between VMs. Could this cause ping ponging, where swapping pages between VMs could cause an issue, where one VM might want to use a swapped page after it has been swapped?
This ping ponging would cause a slowdown.
This could occur in a memory-hungry application.
Would want to use a control algorithm to have the correct ratios of local&#x2F;CXL pages for each VM.
Paper uses exponential moving algorithm to do this, but it can still cause oscillations (ping ponging)&lt;&#x2F;p&gt;
&lt;p&gt;Interesting thing compared to other far memory systems: we are not yet directly memory mapping CXL memory into the physical address space. This means that you can’t map CXL memory directly to cache lines. Why is this the case?
TPP does this (transparent page placement) in software.
Could they mark some pages as don’t pull into DRAM? Yes, and you might in some cases want this.
They already have non-temporal loads&#x2F;stores - does this effect DRAM cache? i.e., bringing CXL memory to local DRAM?&lt;&#x2F;p&gt;
&lt;p&gt;CXL memory looks desirable because normal DIMM connections do not scale.
Much cheaper
Also allows for recycling old DIMM memory chips as CXL memory
Backwards compatibility: DDRx tech would need to be supported by the hardware that is used by CXL for far memory
Decommissioning datacenter - all old stuff could be used in these CXL memory pools.
Common proposal: use persistent memory in these CXL memory pools&lt;&#x2F;p&gt;
&lt;p&gt;When do you think we will have thunderbolt memory extenders?
So that you can run 10 more cron tabs on a PCIe card connected via thunderbolt.
External GPUs can be plugged into thunderbolt.
GPU prices are expensive right now
What if your friend is not using their GPU right now - you could plug into their GPU.
Cloud GPUs also support multi-tenants.
LAN parties with multiple people plugging into the same GPU.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi24-zhong-yuhong.pdf&quot;&gt;Paper&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi24_slides-zhong-yuhong.pdf&quot;&gt;Slides&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>TrackFM: Far-out Compiler Support for a Far Memory World</title>
                <pubDate>Mon, 03 Mar 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/trackfm/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/trackfm/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;TrackFM is a compiler-based far memory system which enables programmers to transparently upgrade their applications to use remote memory, extending the capacity of available main memory for those programs. TrackFM does this by using modern compiler analysis and transformation techniques to support far&#x2F;remote memory. To optimize the data movement costs between server and client, TrackFM introduces novel compiler analysis and transformation passes (in the form of LLVM passes).&lt;&#x2F;p&gt;
&lt;p&gt;Along with the compiler techniques, TrackFM extends the library-based far memory solution, AIFM, to reuse it’s runtime and local&#x2F;remote object management mechanims.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;trackfm-design-and-implementation&quot;&gt;TrackFM Design and Implementation&lt;&#x2F;h1&gt;
&lt;p&gt;TrackFM has 2 components:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Compiler:&lt;&#x2F;strong&gt; Takes the unmodified application source code and uses the modern&#x2F;novel compiler analysis and transformation passes to generate the transformed. It also injects required TrackFM runtime dependencies. TrackFM compiler employs two main analysis&#x2F;transformation optimization for minimal remote access overheads:&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Guard Checking:&lt;&#x2F;strong&gt; TracFM uses &lt;em&gt;guards&lt;&#x2F;em&gt; to ensure that an object is present in local memory before accessing it.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Loop Chunking:&lt;&#x2F;strong&gt; Loop chunking is used to reduce the overhead introduced into the program where &lt;em&gt;guards&lt;&#x2F;em&gt; are present in a loop body.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;ol start=&quot;2&quot;&gt;
&lt;li&gt;&lt;strong&gt;Runtime:&lt;&#x2F;strong&gt; TrackFM reuses AIFM’s runtime to create and manage remote objects, i.e., how to fetch&#x2F;evict objects from remote server into the local memory.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h1 id=&quot;results&quot;&gt;Results&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;TrackFM performs around 2.7x better than FastSwap but slightly worse than AIFM around 0.9x. However, TrackFM enables far memory transparently unlike AIFM.&lt;&#x2F;li&gt;
&lt;li&gt;Compilatoin cost for TrackFM is 6x compared to standard LLVM and the generated code size is increased by about 2.4x.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Important to note that the “Normalized performance” on the graph between FastSwap and AIFM is in comparison to a machine with local memory&lt;&#x2F;li&gt;
&lt;li&gt;It is not optimal to check every time, essentially turning one instruction into 14. THe challenge is knowing when NOT to inject it into the code&lt;&#x2F;li&gt;
&lt;li&gt;AIFM gives you prefetching (you get it for free since TrackFM extends AIFM)&lt;&#x2F;li&gt;
&lt;li&gt;Remote cost is where the thing is actually remote and you need to go fetch it. Overhead is drowned out over the network because the network is so expensive&lt;&#x2F;li&gt;
&lt;li&gt;When comparing to overhead, you’re only talking about the remote case, which is what you’re optimizing for&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q:&lt;&#x2F;strong&gt; Is this a synthetic case? &lt;strong&gt;A:&lt;&#x2F;strong&gt; This is like a minor fault, where you’re faulting in a page, not because it’s over the network, but because it’s not mapped in. It’s in memory, just not a page table for it&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q:&lt;&#x2F;strong&gt; In the graph of TrackFM, AIFM, and Fastswap. X-axis was [% of 31GB] for the local memory, is that the percentage free or used? &lt;strong&gt;A:&lt;&#x2F;strong&gt; It can be interpreted as the percentage constrained as it’s only able to use 16GB (50%). Essentially artificially constraining the application to only use this amount of memory&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q:&lt;&#x2F;strong&gt; So that’s what the tuning would be good for? &lt;strong&gt;A:&lt;&#x2F;strong&gt; Yes, the assumption is the person modifying the code understands the infrastructure. So someone using a remote array would use a different object size from someone using the map, but if the compiler is doing it for you, it doesn’t know&lt;&#x2F;li&gt;
&lt;li&gt;Memory constrained to 1Gb out of 12Gb&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q:&lt;&#x2F;strong&gt; What’s zipfy&#x2F;zipf&#x2F;zipfian? &lt;strong&gt;A:&lt;&#x2F;strong&gt; Basically a power law distribution, There are very few hot keys, and very many keys that aren’t accessed very often, essentially a long tail.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q:&lt;&#x2F;strong&gt; The higher the skew, the longer the tail? &lt;strong&gt;A:&lt;&#x2F;strong&gt; Dr.Hale: If you have higher skewness, (accessing one object over and over again, perfect for Fastswap), but for TackFM, it’s not very great, high spatial locality, high guards&#x2F;faults, essentially lookups in a hash table (random access)&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q&lt;&#x2F;strong&gt; Opportunities for hardware to help questions were asked from reviewers a lot (conclusions and thoughts slide)? &lt;strong&gt;A:&lt;&#x2F;strong&gt; Yes, they’re building hardware for architected pointer features to be combined with compiler-driven techniques. Thought about building on top of the cherry(?) extensions
The compiler approach gives more flexibility.&lt;&#x2F;li&gt;
&lt;li&gt;TrackFM only works for sequential data structures currently, anything that’s a linked data structure. It’ll work but it’ll pay the cost of the guards.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q:&lt;&#x2F;strong&gt; Was there data that showed the performance if you didn’t do any loop chunking? &lt;strong&gt;A:&lt;&#x2F;strong&gt; Yes, it was pretty bad&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;In conclusion, TrackFM is a compiler&#x2F;runtime-based solution that enables programmers to write application that can use far memory (with large memory capacity) transparently. Using TrackFM, programmers can expect performance on-par with the existing state-of-the-art library-based solutions (within 10%) along with 2x the programmer transparency. A lot of this is possible because of advancements made in the far memory hardware (RDMA), which provides low latency remote memory accesses, enabling the solutions like TrackFM.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3617232.3624856&quot;&gt;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3617232.3624856&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Lightning Talk Video: &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=e6c0MhP2CJQ&quot;&gt;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=e6c0MhP2CJQ&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Repo: &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;compiler-disagg&#x2F;TrackFM&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;compiler-disagg&#x2F;TrackFM&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Fastswap (2020) paper: &lt;a href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3342195.3387522&quot;&gt;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;pdf&#x2F;10.1145&#x2F;3342195.3387522&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Fastswap slides: &lt;a href=&quot;https:&#x2F;&#x2F;2020.eurosys.org&#x2F;wp-content&#x2F;uploads&#x2F;2020&#x2F;04&#x2F;slides&#x2F;133_amaro_slides.pdf&quot;&gt;https:&#x2F;&#x2F;2020.eurosys.org&#x2F;wp-content&#x2F;uploads&#x2F;2020&#x2F;04&#x2F;slides&#x2F;133_amaro_slides.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;AIFM (2020) paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi20-ruan.pdf&quot;&gt;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi20-ruan.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;AIFM slides: &lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;sites&#x2F;default&#x2F;files&#x2F;conference&#x2F;protected-files&#x2F;osdi20_slides_ruan.pdf&quot;&gt;https:&#x2F;&#x2F;www.usenix.org&#x2F;sites&#x2F;default&#x2F;files&#x2F;conference&#x2F;protected-files&#x2F;osdi20_slides_ruan.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;AIFM RDMA Project: &lt;a href=&quot;https:&#x2F;&#x2F;github.com&#x2F;fengqingthu&#x2F;MIT_6.5810_Adding_RDMA_To_AIFM&#x2F;blob&#x2F;master&#x2F;65810_Final_Project.pdf&quot;&gt;https:&#x2F;&#x2F;github.com&#x2F;fengqingthu&#x2F;MIT_6.5810_Adding_RDMA_To_AIFM&#x2F;blob&#x2F;master&#x2F;65810_Final_Project.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>Disaggregated Memory for Expansion and Sharing in Blade Servers</title>
                <pubDate>Wed, 26 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/disaggregated-memory-for-expansion-and-sharing-in-blade-servers/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/disaggregated-memory-for-expansion-and-sharing-in-blade-servers/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This paper remarks on the current trend in server architecture, there’s an imbalance in the memory-to-capacity ratio, which is accompanied by an increase in power consumption and data center costs. The paper re-evaluates this relationship by looking at the traditional compute-memory co-location and implementing something new called a “memory blade”, a module for blade servers that enables disaggregated memory across a system. This inclusion is used for memory-capacity expansion, resulting in an increase in performance and reducing power costs. The memory blades in particular are building blocks that allow for two system proposals. The two system proposals are the page-swapping remote memory (PS) and fine-grained remote memory access (FGRA). PS is implemented in the virtual layer while FGRA is implemented in the coherence hardware to handle transparent memory expansion and sharing across systems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;&#x2F;h2&gt;
&lt;p&gt;The implementation starts with the memory blade, which is composed of DRAM modules that connect to the server via a high-speed interface such as PCIe. Furthermore, the design of the memory blade includes custom hardware to allow for both PS and FGRA systems to be implemented in testing. For PS, it doesn’t use the custom hardware but leverages virtual memory to handle page swaps in both local and remote memory. The hypervisor in the software stack manages the page access, ensuring that the system works with the OS and applications. Unlike PS, FGRA doesn’t rely on just software and the VMM. FGRA takes full advantage of the custom hardware the memory blade utilizes to enable use of coherence protocols and a custom memory controller. These implementations result in a reduced overhead of page-swapping. Both these systems are put through trace-based simulations and evaluated on benchmarks to compare the performance of a memory-blade system to a traditional system.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
&lt;p&gt;The results contain simulations using benchmarks such as zeusmp, perl, gcc, bwaves, spec4p, nutch4p, tpchmix, mcf, pgbench, indexer, specjbb, and Hmean. Figure 5 details the capacity expansion results, and we find that PS has better performance over FGRA in speedups over M-app-75% provisioning and M-median provisioning. These graphs highlight both FGRA and PS relative to the baseline, with a 4X to 320X increase in performance. In figure 7a, PS handles the imbalance between VM memory demands and local capacity, resulting in a possible 68% reduction of processor count. In terms of cost, PS is slightly better as it’s not reliant on custom hardware, but implementing disaggregated memory improves performance-per-dollar by 87% in Figure 7(b). FGRA has some drawbacks as it doesn’t handle swapping like PS, which is addressed inn figure 8 for potential hardware implementations such as page migration.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;disaggregated-memory-for-expansion-and-sharing-in-blade-servers&#x2F;MMS_Image_1.png&quot; alt=&quot;results&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;weakness&quot;&gt;Weakness&lt;&#x2F;h2&gt;
&lt;p&gt;One of the main weaknesses discussed in the paper is that the memory blade introduces several modes of failure, which could be solved by adding redundancy to the memory controller. Another noted weakness is that PS works without any additional hardware use, but FRGA would require some hardware modifications to be feasible, and this would include addressing page migration. Adding on, both PS and FRGA struggle with heavier loads, which could be addressed by modifying the high-speed interface. This is also a concern when considering local and remote latency tradeoffs, an upgrade from PCIe to CXL could be worth visiting.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Hypervisor Type:&lt;&#x2F;strong&gt; The paper likely assumes a Type 1 hypervisor (runs directly on hardware), but a Type 2 hypervisor (runs on an OS) could work with adjustments. The type matters for implementation details but not the core concept.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;4KB Page Transfers:&lt;&#x2F;strong&gt; Waiting for a 4KB transfer may seem slow, but locality and caching reduce the impact. Prefetching could further help, though its effectiveness depends on the workload.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Workload Locality:&lt;&#x2F;strong&gt; The paper’s workloads (from 2009) may not reflect modern graph workloads with random access patterns. However, improved CPU prefetchers since then could lessen performance penalties.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;CXL (Compute Express Link):&lt;&#x2F;strong&gt; A modern evolution of PCIe, CXL supports coherence natively, potentially simplifying FGRA-like approaches without custom coherence filters.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Modern Trends:&lt;&#x2F;strong&gt; Today, page-based remote memory access often uses InfiniBand or Ethernet (not just PCIe), enabling memory sharing across datacenters.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;practical-considerations&quot;&gt;Practical Considerations&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Increasing Local DRAM:&lt;&#x2F;strong&gt; Adding more DRAM to each server is wasteful, as it’s underutilized most of the time.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Interleaving Memory Blades:&lt;&#x2F;strong&gt; Compute blades can pull pages from memory blades as needed. This is slower than local memory but faster than accessing a hard disk drive (HDD).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dynamic Allocation:&lt;&#x2F;strong&gt; No software changes are required unless dynamically reallocating memory within the blade.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Performance Differences:&lt;&#x2F;strong&gt; PS vs. FGRA differences arise from transfer granularity (4KB pages vs. 64-byte cache blocks) and interconnect speed (PCIe vs. HyperTransport).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;source&quot;&gt;Source:&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;1555754.1555789&quot;&gt;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;1555754.1555789&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;&lt;a href=&quot;https:&#x2F;&#x2F;chatgpt.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;chatgpt.com&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Note: ChatGPT was used to improve clarity on the class discussion section.&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>WSP: Whole System Persistence</title>
                <pubDate>Mon, 24 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/wsp/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/wsp/</guid>
                <description>&lt;h1 id=&quot;background&quot;&gt;Background&lt;&#x2F;h1&gt;
&lt;p&gt;We’ve historically viewed the memory hierarchy as volatile main memory and secondary persistent storage.  With the OS and programs executing directly form main memory, the secondary storage is viewed as a final resting place for persistency of data (and sometimes for additional memory through swapping).  With this primary-volatile, secondary-non-volatile model in mind, advancements have been made over decades in how operating systems and applications use storage, from filesystems to databases supporting transactional storage.&lt;&#x2F;p&gt;
&lt;p&gt;But if all of main memory becomes inherently non-volatile then many of these past conventions get disrupted - having the program itself and the data it is operating on directly persist across power loss and multiple boots presents its own set of opportunities and numerous new challenges as well.  The Whole System Persistence paper examines a solution built on non-volatile main memory.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;problem-statement&quot;&gt;Problem Statement&lt;&#x2F;h2&gt;
&lt;p&gt;Many modern Internet services are built around memory-based key-value stores for speed.  When a server hosting one of these in-memory databases must be rebooted, it can take a significant amount of time and&#x2F;or a significant load on secondary storage devices to reload data from secondary storage to main memory.  An example cited in the paper was an outage from Facebook in 2010 which suffered 2.5 hours of downtime due while in-memory cache servers reloaded data.  If this secondary storage bottleneck could be eliminated it could substantially improve the time to recovery.&lt;&#x2F;p&gt;
&lt;p&gt;The paper states that previous solutions like persistent buffer caches, which use block-based&#x2F;filesystem storage as well as persistent heap-based solutions require state to be duplicated between memory and storage and also argues that it effectively “doubles the memory footprint” of these applications.  (But read below, for a critical analysis of this statement.)&lt;&#x2F;p&gt;
&lt;h2 id=&quot;whole-system-persistence-wsp&quot;&gt;Whole-System Persistence (WSP)&lt;&#x2F;h2&gt;
&lt;p&gt;The Whole-System Persistence design uses non-volatile NVDIMM memory to retain &lt;em&gt;all&lt;&#x2F;em&gt; DRAM state across failures, effectively converting a failure into a suspend&#x2F;resume event.  No changes are required to applications, although performance optimizations may be desirable.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;wsp&#x2F;WSP_Diagram.png&quot; alt=&quot;WSP System Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;In this diagram we see the NVDIMM, which contains DDR DRAM and NAND flash and can autonomously transfer data from DRAM to NAND on command.  For the purposes of saving state at power loss, saving DRAM to NAND Flash alone is not sufficient.  The processor (and in some architectures the chipset) contains caches which must be flushed.  And to preserve program state so we can resume at the next Program Counter value without restarting the processor register context must be saved as well, in much the same manner as a thread context switch.  So at power loss there must be saving of processor state (of all cores), flushing of caches and then finally instructing the NVDIMM to transfer DRAM to Flash.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;save-and-restore-flow&quot;&gt;Save and Restore Flow&lt;&#x2F;h3&gt;
&lt;p&gt;To ensure a complete system state save there must be sufficient energy available to support continuing to execute for the processor state-save handling as well as from the DRAM-&amp;gt;NAND Flash transfer.  This energy would be supplied through capacitors, either auxiliary capacitors to provide non-volatility or by leveraging the bulk capacitance in the system power supply.  (See criticism below on the power supply capacitance characterization work however.)&lt;&#x2F;p&gt;
&lt;p&gt;What is the best way to preserve all of main memory on power loss?  The paper characterized the device state transition time to go to D3, the device state used for suspend-to-ram (ACPI S3) and suspend-to-disk (ACPI S4).  They show that on their two testbed systems that these transitions do not meet the time requirement for sufficient “residual energy”:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;wsp&#x2F;WSP_device_state_save_graph.png&quot; alt=&quot;WSP Device State Save Chart&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Instead the solution was to rely on an optimized suspend path that saved processor state but did not involve device drivers.  (The paper did not address how device drivers were expected to recover from these conditions as their hardware state is reset.  This could be a serious limitation for applying WSP to existing operating systems.)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;wsp&#x2F;WSP_save_restore.png&quot; alt=&quot;WSP Save-Restore Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;nvram-programming-models-block-based-persistent-heap-versus-whole-system&quot;&gt;NVRAM Programming Models: Block-Based, Persistent Heap versus Whole-System&lt;&#x2F;h2&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;Persistence Model&lt;&#x2F;th&gt;&lt;th&gt;Pros&lt;&#x2F;th&gt;&lt;th&gt;Cons&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;Block-Based&lt;&#x2F;td&gt;&lt;td&gt;Works with existing storage models&lt;&#x2F;td&gt;&lt;td&gt;Log&#x2F;journal for reliability, slow recovery&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Persistent Heaps&lt;&#x2F;td&gt;&lt;td&gt;In-memory updates, faster than block-based&lt;&#x2F;td&gt;&lt;td&gt;Non-transparent for app, requires transactional updates&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;Whole System Persistence&lt;&#x2F;td&gt;&lt;td&gt;Instant recovery, transparent to apps*&lt;&#x2F;td&gt;&lt;td&gt;Requires system-wide NVRAM more expensive (today)&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;code&gt;*&lt;&#x2F;code&gt; but if apps already use other techniques then they still need to be modified!&lt;&#x2F;p&gt;
&lt;h3 id=&quot;block-based-serialize-data-and-write-to-a-file-or-database&quot;&gt;Block-based: serialize data and write to a file or database&lt;&#x2F;h3&gt;
&lt;p&gt;Application must convert data to the storage format on each update and back again on recovery.  Additional application overhead is implied by block storage.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;persistent-heaps-use-a-transactional-api-to-update-persistent-objects&quot;&gt;Persistent heaps: use a transactional API to update persistent objects&lt;&#x2F;h3&gt;
&lt;p&gt;Some objects are placed in a persistent heap and use specialized APIs to ensure consistency.  These APIs implement a transaction model ensuring an all-or-nothing update model implying a log-structured implementation.  The paper states that the flush-on-commit methodology required by persistent heap implementations incur significant runtime overheads.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;whole-system-all-objects-are-in-memory-all-state-restored-transparently&quot;&gt;Whole system: all objects are in-memory, all state restored transparently&lt;&#x2F;h3&gt;
&lt;p&gt;All application memory is non-volatile.  Only transient state of the execution must be flushed to memory.  The paper says this is better for legacy applications which are not written for separation of persistent and volatile state.&lt;&#x2F;p&gt;
&lt;p&gt;The paper states that persistent heaps with transactional characteristics are inherently worse than whole-system persistency and if necessary using transactional mechanisms where needed to achieve recovery goals.  The difference is that the transactional API is not imposed everywhere with whole-system persistence.&lt;&#x2F;p&gt;
&lt;p&gt;The paper proposes adopting a flush-on-fail to capture transient processor and cache state only during failures intead of a flush-on-commit method used to write state to persistent storage during operation.  The idea is that the overhead of flushing data to secondary storage can be eliminated.  (But see criticism below about the detectability of all failures.)&lt;&#x2F;p&gt;
&lt;h1 id=&quot;wsp-performance-results&quot;&gt;WSP Performance Results&lt;&#x2F;h1&gt;
&lt;p&gt;The WSP paper evaluated the performance changes resulting from adopting flush-on-fail instead of flush-on-commit in two workloads.&lt;&#x2F;p&gt;
&lt;p&gt;One workload is based on OpenLDAP which inserts 100,000 random entries into a directory, comparing the Mnemosyne NV-heap solution which employs flush-on-commit versus WSP which employs flush-on-fail.  For the purposes of WSP in this context, the flushing – transational instrumentation and logging – is simply removed.  As expected there is a substantial uplift in performance for WSP.  (Is this a realistic modification?  Would in-memory database be able to recover under all interruption scenarios after the outright removal of these layers?  It’s unclear.)&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;wsp&#x2F;WSP_update_throughput.png&quot; alt=&quot;WSP Update Throughput Table&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The second workload is a hash table benchmark using a pre-populated table of 100,000 entries with 1,000,000 random operations applied to it, either a key lookup or update (insert&#x2F;delete).  Multiple configurations were tested to separate out the effects of Software Transactional Memory (STM - the default Mnemosyne config), Undo-Logging (UL, an alternative approach in lieu of STM) and the flush-on-fail which elides both methods:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;wsp&#x2F;WSP_hash_perf.png&quot; alt=&quot;WSP Hash Table Performance Figure&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Overall we see the WSP FoF is 6-13x faster than the default FoC+STM configuration.  In addition to the speedups we expect by removing STM and UL, we also observe that WSP even with STM or UL is substantially faster for writing data so even if transactional methods are retained the persistency of main memory still confers a great benefit.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;analysis&quot;&gt;Analysis&lt;&#x2F;h1&gt;
&lt;h2 id=&quot;thoughts-on-transient-power-loss&quot;&gt;Thoughts on Transient Power Loss&lt;&#x2F;h2&gt;
&lt;p&gt;Intro says saving transient state takes less than 5ms across a range of platforms and says power supplies provide 10-300ms of uptime.  A production system will need a highly deterministic power delivery system so the power usage of the system and capacities of power supply must be carefully scrutinized and characterized.  This become more difficult with expansion - for example a high power consumption coherent CXL device may consume enough power that the transient uptime requirement is violated.&lt;&#x2F;p&gt;
&lt;p&gt;Need to be careful that all components retain their ability to operate during the transient condition.  Imagine a multi-rack interconnect that participates in a hardware coherency protocol and one rack loses power before the other.  What is necessary to be able to complete the necessary flushes to persistent memory under these complex topologies?&lt;&#x2F;p&gt;
&lt;h3 id=&quot;synchronizing-device-state-on-recovery&quot;&gt;Synchronizing Device State on Recovery&lt;&#x2F;h3&gt;
&lt;p&gt;The authors considered using the ACPI-defined suspend-to-RAM model for saving and restoring device state during these transactions but the save significantly exceeded the residual energy window.  (Aside: The S3 model is intended for client systems where latencies of multiple seconds to transition may be considered acceptable so it’s not surprising that this is ill-suited.). They suggest an approach where there is no save phase of for the devices and instead deal with the mess on the way back.&lt;&#x2F;p&gt;
&lt;p&gt;The authors state that a virtualization environment could resolve this as the VM paradigm already supports the suspension and resumption of VM guests.  The hypervisor &#x2F; host OS could be booted cold and then VMs restored from their persistent state.  This approach leverages the device driver model and recovery of the hypervisor drivers is nothing more than cold boot.  This still amounts to an effective surprise-suspension of a VM guest but the limited device drivers in that model make the recovery problem more manageable.  This approach was not yet implemented at the time of writing.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;strengths-and-weaknesses&quot;&gt;Strengths and Weaknesses&lt;&#x2F;h1&gt;
&lt;p&gt;Paper says that traditional block storage techniques store data in two locations (RAM and disk) and hence “doubles the application’s memory footprint”.  This is a bit unfair as the NVDIMM also doubles the memory footprint across a DRAM and Flash, the difference being that both memories reside on the same device instead of on different ones.  And from a price perspective the more specialized NVDIMM may be cost prohibitive compared to commodity RAM and storage devices.  Will the cost increase of NVDIMMs justify the benefits of persistence versus storage-backed schemes?&lt;&#x2F;p&gt;
&lt;p&gt;The paper spent a lot of effort characterizing power supplies examining residual energy on power loss.  But the paper fails to realize that residual energy is not a design goal of power supplies on commodity devices, in fact in some cases minimizing residual energy is a goal to ensure safe component replacement.  (Imagine the electrical issues that may arise if your motherboard power supplies remained energized for a couple minutes after being switched off while you were swapping DIMMs or PCIe cards!)&lt;&#x2F;p&gt;
&lt;p&gt;How is recovery from arbitrary interruption handled?  Is the application expected to resume at the next Program Counter value as if nothing had happened?  As we know from advanced power-management schemes this is quite difficult in practice as the state of external hardware and systems must be re-synchronized.  How realistic is this for applications that may have shared state with HW devices (accelerators, storage and networking)?  The paper implies that applications are inherently prepared to handle this kind of transition but there is not data to support this.&lt;&#x2F;p&gt;
&lt;p&gt;An over-arching assumption is that the platform is able to detect these interruptions and trigger the flushing and persistent storage write.  How realistic is this?  We know that systems can hang, operating systems and even firmware can crash&#x2F;panic.  Reliable server systems have had FMEA studies done already - how well does the Whole System Persistence stand up to the variety of failure modes we can imagine for a server?&lt;&#x2F;p&gt;
&lt;p&gt;What happens if the application crashes and the crash state is persistent?  How does one recover from this?  The paper states “In our model, NVRAM is the first but not the last resort for recovery after a crash failure. Recovery from a back-end storage layer such as a file system or database will always be necessary in some cases, e.g., if an entire server fails.” but this implies that the back-end storage will still need to be written.  Please enlighten us with how we can accomplish this with whole-system persistence in place!&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Step 11 checks image validity, how do they actually do this? They use a commit record, when flushing a state you stamp that record with a completion stamp when it’s done so you know what has been completed. You boot the computer if there is a commit record if not then you did not make it in time and then you crash.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;other-questions-and-comments-from-class-discussion&quot;&gt;Other Questions and Comments from Class Discussion&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This paper used faults of power supplies as an expectation (residual power supply energy), if the capacitor fails then there won’t be residual power to use.  This points out the broader issue that the paper is focused on a single failure mode only.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;They say that the recovery time is instant but we know NAND flash takes time to write and to read back.  The paper does not characterize these times and these will create a bound on recovery time.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;virtualization-s-role-in-wsp&quot;&gt;Virtualization’s Role in WSP&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Virtualization may help solve the device state problem.  For guests using virtualized devices (e.g. virtio) the state of these IO interfaces are held in memory itself (e.g. virtio ring buffers) and will be saved and restored with the rest of memory. But modern techniques that expose host devices directly the guest, like PCIe pass-through of a GPU, will still have the device save-restore problem.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Would checkpoint restart work here? You periodically dump a checkpoint and then you have to start from the latest checkpoint. This system is for a processors most up to date state being saved rather than a close-by restart&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;practical-considerations&quot;&gt;Practical Considerations&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;If my application is based on time, it’s about to do something based on time and it fails and a restore happens what happens if the shutdown is for a long time. What happens to the state of a program that cares about time? How does the program that gets rebooted know how much time has passed?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;How do you determine what memory is important and what memory is not important, how do you sort volatile memory and nonvolatile memory in this system?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;How would things like OS kernel or application updates work?  What does it mean to reboot a system for updating or crash recovery if main memory is the primary point of persistence?  There is a lot more operating systems work to do to address these issues.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;What state is your memory in if it’s failed and you reboot?  What do you recover&#x2F;reload from if the whole point is to sever ties to secondary storage?&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;There are security concerns since someone could just take your memory and plug it into a different computer and read it.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;&#x2F;h2&gt;
&lt;p&gt;The WSP paper poses an important question about the implications of main-memory non-volatility.  It shows distinct benefits from eliminating writes to secondary storage and the overhead associated with transactional updates but it leaves unanswered a bunch of new questions around device driver impacts, reliability across failure modes, and implications for OS and application updates, providing ample opportunities for Systems researchers.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.microsoft.com&#x2F;en-us&#x2F;research&#x2F;wp-content&#x2F;uploads&#x2F;2016&#x2F;02&#x2F;paper-updated.pdf&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>An Empirical Guide to the Behavior and Use of Scalable Persistent Memory</title>
                <pubDate>Wed, 19 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/an-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/an-empirical-guide-to-the-behavior-and-use-of-scalable-persistent-memory/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This blog discusses “An Empirical Guide to the Behavior and Use of Scalable Persistent Memory.” This work stands out as the first real-world performance benchmark of Intel’s Optane DIMMs, a groundbreaking technology that brings scalable, byte-addressable persistent memory to the computing landscape. Designed to bridge the gap between the fast but volatile DRAM and the slower, non-volatile SSDs, Optane DIMMs promised a new era of memory systems after nearly a decade of anticipation. The paper dives into how these devices actually perform, revealing surprises that upend assumptions from years of research based on emulated setups like DRAM stand-ins. Far from being just “slower DRAM,” Optane exhibits unique quirks that demand fresh thinking. Its authors set out not only to measure Optane’s behavior but also to deliver practical guidelines for software developers to optimize their code for this novel hardware. This blog unpacks their findings, shedding light on a technology that’s as complex as it is promising.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;background-and-context&quot;&gt;Background and Context&lt;&#x2F;h2&gt;
&lt;p&gt;Persistent memory represents a transformative leap in the memory hierarchy, blending traits that set it apart from traditional technologies. Unlike DRAM, which loses its contents when power is cut, persistent memory is non-volatile, retaining data even after shutdowns. It’s denser too, packing more storage capacity into less space at a lower cost per gigabyte than DRAM. This makes it a versatile player, sitting between DRAM’s speed and SSDs’ persistence. Its byte-addressable nature—allowing direct access to individual memory locations—unlocks a range of use cases: speeding up database transaction logging and in-memory databases, providing fast persistent caching for high-performance computing (HPC), boosting memory-intensive virtualization in cloud setups, and accelerating storage as a cache or tiered layer.&lt;&#x2F;p&gt;
&lt;p&gt;For over a decade, researchers have been gearing up for persistent memory’s arrival, but without real hardware like Intel’s Optane DIMMs, they relied on emulations—often using DRAM tweaked to mimic non-volatile memory (NVM). These setups, sometimes paired with artificial delays or NUMA effects, fueled speculation about how NVM would behave. However, powered by 3D XPoint technology (a joint Intel-Micron innovation), Optane does not behave as expected. Unlike DRAM’s straightforward capacitor-based design, 3D XPoint introduces “second-order changes”—subtle, cascading effects in latency, bandwidth, and concurrency that add a new layer of complexity. Where DRAM emulation suggested predictable performance, Optane’s real-world behavior defies those models, revealing quirks that prior speculations couldn’t anticipate. This shift challenges the research community to rethink how software interacts with memory, setting the stage for the detailed benchmarks and insights this paper delivers.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;keywords&quot;&gt;Keywords&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Persistent Memory&lt;&#x2F;strong&gt;: A type of memory that retains data even after power is lost, combining non-volatility with high density and often byte-addressable access, bridging the gap between DRAM and SSDs.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Non-Volatile Memory (NVM)&lt;&#x2F;strong&gt;: Memory that preserves data without power, unlike volatile DRAM; includes technologies like flash and persistent memory like Optane.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Optane DIMM&lt;&#x2F;strong&gt;: Intel’s scalable persistent memory module, built on 3D XPoint tech, offering byte-addressable, non-volatile storage in DIMM form for consumer and data center use.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;3D XPoint&lt;&#x2F;strong&gt;: A non-volatile memory technology developed by Intel and Micron, using a crosspoint structure with resistive switching (not transistors) for high speed and endurance.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;App Direct Mode&lt;&#x2F;strong&gt;: An Optane operating mode exposing it as a separate persistent memory device, managed by software for direct access, without DRAM caching.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Memory Mode&lt;&#x2F;strong&gt;: An Optane mode using it as volatile main memory with DRAM as a transparent cache, expanding capacity without persistence.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Asynchronous DRAM Refresh (ADR)&lt;&#x2F;strong&gt;: Intel’s feature ensuring data in the memory controller’s write queues (WPQs) is flushed to persistent media during power loss, within a short hold-up time.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Write Amplification&lt;&#x2F;strong&gt;: The increase in data written to memory beyond what the application requests, often due to block-sized updates (e.g., Optane’s 256B granularity).&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Non-Temporal Store (ntstore)&lt;&#x2F;strong&gt;: A CPU instruction bypassing the cache to write directly to memory, useful for large sequential writes to avoid cache pollution.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;iMC (Integrated Memory Controller)&lt;&#x2F;strong&gt;: The CPU component managing memory access, including read&#x2F;write queues (RPQs&#x2F;WPQs), critical for Optane’s ADR domain.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Wear Leveling&lt;&#x2F;strong&gt;: A technique to distribute writes evenly across memory cells, preventing overuse and extending lifespan, potentially causing Optane’s tail latency spikes.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;CXL (Compute Express Link)&lt;&#x2F;strong&gt;: An emerging interconnect standard for coherent memory access, supporting persistent and disaggregated memory, seen as an Optane alternative.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;1T1C (One Transistor, One Capacitor)&lt;&#x2F;strong&gt;: The basic cell design of DRAM, using a transistor-capacitor pair for each bit, contrasting with 3D XPoint’s resistive approach.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;summary-of-the-paper&quot;&gt;Summary of the Paper&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;&#x2F;h3&gt;
&lt;p&gt;The paper discusses the decade-long wait for scalable non-volatile memory (NVM), a technology researchers have speculated about since the early 2000s. Intel’s Optane DIMMs finally arrived in 2019, offering a real-world testbed to challenge those expectations. The authors set out to measure how this persistent memory performs, aiming to replace guesswork from emulated setups with data.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;2-background-and-methodology&quot;&gt;2. Background and Methodology&lt;&#x2F;h3&gt;
&lt;p&gt;This section discusses how optane works. It plugs into Intel’s Cascade Lake processors (24 cores, dual-socket, 3 TB total Optane across 2 sockets with 6 channels each). Unlike DRAM, Optane uses 256B blocks for internal writes, managed by an on-DIMM controller (XPController) and the CPU’s integrated memory controller (iMC), which handles write pending queues (WPQs). The Asynchronous DRAM Refresh (ADR) feature ensures data in WPQs persists during power loss. The study focuses on App Direct mode—where Optane acts as a standalone persistent device—running on Fedora 27 with a custom kernel, testing 384 GB DRAM alongside 256 GB Optane DIMMs per channel.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-performance-characterization&quot;&gt;3. Performance Characterization&lt;&#x2F;h3&gt;
&lt;p&gt;Read latency clocks in at 2x-3x higher than DRAM, with random reads hit harder than sequential ones due to the 256B XPBuffer’s limits. Tail latency spikes—up to 50 µs, 100x the norm—pop up unpredictably, likely from wear leveling. Bandwidth tells a split story: reads scale well with threads (up to 16), but writes plateau early (around 4-12 threads) and dip with more concurrency, far less scalable than DRAM’s steady climb. Small random writes tank performance, while sequential ones hold up better.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-comparison-to-emulation&quot;&gt;4. Comparison to Emulation&lt;&#x2F;h3&gt;
&lt;p&gt;Past NVM studies used DRAM with delays or NUMA tricks, predicting Optane as “slower DRAM.” Real tests flip that script. In RocksDB, emulation favored fine-grained persistence (moving the memtable to NVM), but on Optane, the FLEX approach (sequential write-ahead logging) wins by 10%, thanks to its dislike for small random writes. Emulated guesses missed Optane’s nuanced behavior.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-best-practices-for-optane-dimms&quot;&gt;5. Best Practices for Optane DIMMs&lt;&#x2F;h3&gt;
&lt;p&gt;The paper distills four guidelines from microbenchmarks: avoid small random writes under 256B (they amplify due to block size), use non-temporal stores (ntstores) for large sequential writes (bypassing cache boosts bandwidth), limit threads hitting one DIMM (more than 4-12 thrash the XPBuffer and iMC), and steer clear of NUMA accesses (remote hits crater performance, up to 30x worse than DRAM’s 3.3x).&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-discussion&quot;&gt;6. Discussion&lt;&#x2F;h3&gt;
&lt;p&gt;Looking ahead, the authors muse on how these tips might evolve. Extended ADR (eADR), including caches in the persistent domain, could nix ntstore needs. CXL might shift persistent memory paradigms, while bigger buffers or smaller blocks (unlike 256B) could tweak trade-offs—though power limits complicate that. The guidelines offer a roadmap for future NVM, even if Optane’s specifics change.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-related-work&quot;&gt;7. Related Work&lt;&#x2F;h3&gt;
&lt;p&gt;Before Optane, NVM research relied on emulation. These shaped file systems, transactional models, and data structures, but lacked real hardware validation. This paper marks a pivot to tangible benchmarks.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;8-conclusion&quot;&gt;8. Conclusion&lt;&#x2F;h3&gt;
&lt;p&gt;Optane DIMMs emerge slower (2x-3x DRAM latency) and more complex than expected, with thread scaling and write granularity tripping up performance. Coders bear the burden of optimization. As the first real benchmark of persistent memory, this study swaps speculation for data, spotlighting Optane’s promise and pitfalls.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;important-results-and-what-they-mean&quot;&gt;Important Results and What They Mean&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Emulation Errors&lt;&#x2F;strong&gt;: For years, researchers emulated non-volatile memory (NVM) with DRAM, tweaking it to guess how Optane might behave. Real tests cleared up this assumption.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Latency&#x2F;Bandwidth&lt;&#x2F;strong&gt;: Read latency hits 2x-3x higher than DRAM, with random reads suffering more than sequential ones due to internal buffering limits. Write bandwidth lags too, far below DRAM’s, and scalability takes a hit: while DRAM’s bandwidth climbs steadily with threads, Optane peaks at 4-12 threads (reads at 16, writes at 4) then drops off—a stark contrast to DRAM’s monotonic rise.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;256B Granularity&lt;&#x2F;strong&gt;: Small random writes (under 256B) amplify—updating one byte means rewriting the whole block, slashing efficiency and bandwidth. Sequential writes, though, shine, aligning with the block size to minimize overhead. This split explains why random access patterns tank performance while sequential ones improve performance.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Tail Latency&lt;&#x2F;strong&gt;: Reliability is reduced by rare tail latency spikes—up to 50 µs, 100x the typical latency. These outliers, likely from wear leveling or internal remapping, are sparse (0.006% of accesses) but cause serious problems. For systems needing predictability, like databases or real-time apps, this is a red flag.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;strengths-and-weaknesses-of-the-paper&quot;&gt;Strengths and Weaknesses of the Paper&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;strengths&quot;&gt;Strengths&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;First Real Optane Benchmark&lt;&#x2F;strong&gt;: This paper stands out as the first benchmark of Intel’s Optane DIMMs with actual hardware.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Practical Guidelines with Case Studies&lt;&#x2F;strong&gt;: Beyond measurements, it offers actionable advice—avoid small writes, limit threads, use ntstores—backed by real-world tweaks.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;weaknesses&quot;&gt;Weaknesses&lt;&#x2F;h3&gt;
&lt;p&gt;&lt;strong&gt;Limited Testing Scope&lt;&#x2F;strong&gt;: The paper’s focus narrows to RocksDB and NOVA, with testing confined to App Direct mode. This leaves Memory Mode and broader applications unexplored, raising questions about how well the findings generalize across diverse workloads or setups.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;No Cost-Benefit Analysis or NVM Comparisons&lt;&#x2F;strong&gt;: It skips the economics—Optane’s cost “dramatically increases” compared to DRAM or SSDs, but there’s no breakdown of trade-offs or benefits to justify it. Nor does it stack Optane against other NVM technologies, leaving a gap in context.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion&lt;&#x2F;h2&gt;
&lt;p&gt;&lt;strong&gt;Capacitors in 3D XPoint&lt;&#x2F;strong&gt;: The 3D XPoint explainer video sparked curiosity. Unlike DRAM’s 1T1C (one transistor, one capacitor) simplicity, 3D XPoint’s crosspoint structure ditches transistors for resistive switching, with capacitors likely aiding dense packing and fast bit access.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;256B Block Size&lt;&#x2F;strong&gt;: Why 256B? Is it baked into Optane’s DNA or a deliberate pick? Eugene wondered if smaller blocks (say, 64B) could ease small-write woes—random writes under 256B tank performance due to whole-block updates. Kyle Hale guessed 256B strikes a “good medium ground,” balancing efficiency and complexity. Smaller blocks might cut amplification but could spike power demands or shrink density, a trade-off Optane’s designers might have dodged.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;NUMA Puzzle&lt;&#x2F;strong&gt;: Optane’s NUMA penalty—up to 30x bandwidth drop vs. DRAM’s 3.3x. Class eyed iMC contention: mixed read-write loads (like 3:1 ratios) clog short queues, worse with remote nodes. Eugene suggested it’s less iMC and more NVDIMM thrashing, but DRAM handles NUMA better.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Thread Scaling&lt;&#x2F;strong&gt;: Bandwidth peaking at 4-12 threads then dropped. Eugene blamed XPBuffer thrashing—multiple threads trash its 16KB write-merging space. Class added iMC queues: slow Optane drainage blocks them, slashing throughput. Is it buffer size, queue depth, or both? How do we keep threads from tripping over each other?&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Interconnect Efficiency&lt;&#x2F;strong&gt;: Optane’s interconnect got scrutiny. How does it handle mixed read-write access? Class asked about gigabytes-per-second graphs (assuming bandwidth), wondering if it copes under chaos.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Commercial Fate&lt;&#x2F;strong&gt;: Intel’s 2022 Optane exit, after a $7B loss, hit hard. This was refered to as “swept under the rug.” Supply chain difficulty and high costs tanked it. Micron got out of persistent memory, but tinkering with persistent memory persists. CXL’s rising for persistent and disaggregated setups.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;eADR Shift&lt;&#x2F;strong&gt;: eADR was brought up — cache-inclusive ADR from Intel’s 3rd-gen Xeons. It simplifies coding (no ntstores or flushes) and boosts bandwidth by keeping caches persistent. Class saw it easing Optane’s quirks.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Security Risks&lt;&#x2F;strong&gt;: DIMM theft was mentioned. If someone takes an Optane DIMM, data’s readable. How big of a problem is this? For databases or HPC, it’s a dealbreaker; consumer use, maybe less so. Why no mitigation discussion in the paper?&lt;&#x2F;p&gt;
&lt;p&gt;&lt;strong&gt;Hybrid Potential&lt;&#x2F;strong&gt;: The class pitched DRAM (1T1C speed) plus Optane (persistence). Class liked the idea: DRAM for fast random access, Optane for big persistent stores.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources:&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;An Empirical Guide to the Behavior and Use of Scalable Persistent Memory: &lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;conference&#x2F;fast20&#x2F;presentation&#x2F;yang&quot;&gt;http:&#x2F;&#x2F;web.cs.ucla.edu&#x2F;~todd&#x2F;research&#x2F;isca12.pdf&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;OSTEP Textbook Remzi Arpaci-Dusseau&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Computer Organization and Design RISC-V Edition The Hardware Software Interface 2nd Edition - December 11, 2020 Authors: David A. Patterson, John L. Hennessy&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Computer Architecture, Sixth Edition: A Quantitative Approach December 2017 Authors: John L. Hennessy, David A. Patterson&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;generative-ai&quot;&gt;Generative AI&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;Link to specific Tools: &lt;a href=&quot;https:&#x2F;&#x2F;chatgpt.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;chatgpt.com&#x2F;&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;www.deepseek.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.deepseek.com&#x2F;&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;claude.ai&quot;&gt;https:&#x2F;&#x2F;claude.ai&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;gemini.google.com&#x2F;app&quot;&gt;https:&#x2F;&#x2F;gemini.google.com&#x2F;app&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;table&gt;&lt;thead&gt;&lt;tr&gt;&lt;th&gt;&lt;strong&gt;Metric&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th&gt;&lt;strong&gt;ChatGPT 4o (GPT-4o)&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th&gt;&lt;strong&gt;DeepSeek R1&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th&gt;&lt;strong&gt;Gemini 2.0 Flash&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;th&gt;&lt;strong&gt;Claude 3.7 Sonnet&lt;&#x2F;strong&gt;&lt;&#x2F;th&gt;&lt;&#x2F;tr&gt;&lt;&#x2F;thead&gt;&lt;tbody&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;MMLU Accuracy&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;88.7%&lt;&#x2F;td&gt;&lt;td&gt;85%&lt;&#x2F;td&gt;&lt;td&gt;80%&lt;&#x2F;td&gt;&lt;td&gt;88%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;TruthfulQA Accuracy&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;65%&lt;&#x2F;td&gt;&lt;td&gt;60%&lt;&#x2F;td&gt;&lt;td&gt;55%&lt;&#x2F;td&gt;&lt;td&gt;62%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;HellaSwag Accuracy&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;96%&lt;&#x2F;td&gt;&lt;td&gt;94%&lt;&#x2F;td&gt;&lt;td&gt;90%&lt;&#x2F;td&gt;&lt;td&gt;95%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;GSM8K Accuracy&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;95%&lt;&#x2F;td&gt;&lt;td&gt;92%&lt;&#x2F;td&gt;&lt;td&gt;85%&lt;&#x2F;td&gt;&lt;td&gt;94%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;BBH Accuracy&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;85%&lt;&#x2F;td&gt;&lt;td&gt;75%&lt;&#x2F;td&gt;&lt;td&gt;70%&lt;&#x2F;td&gt;&lt;td&gt;85%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;tr&gt;&lt;td&gt;&lt;strong&gt;HumanEval Pass Rate&lt;&#x2F;strong&gt;&lt;&#x2F;td&gt;&lt;td&gt;85%&lt;&#x2F;td&gt;&lt;td&gt;80%&lt;&#x2F;td&gt;&lt;td&gt;75%&lt;&#x2F;td&gt;&lt;td&gt;84%&lt;&#x2F;td&gt;&lt;&#x2F;tr&gt;
&lt;&#x2F;tbody&gt;&lt;&#x2F;table&gt;
&lt;ul&gt;
&lt;li&gt;&lt;h3 id=&quot;key-observations&quot;&gt;Key Observations&lt;&#x2F;h3&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;ChatGPT 4o&lt;&#x2F;strong&gt; consistently performs at or near the top across all metrics, achieving the highest average score of 85.78%. It stands out in knowledge (MMLU), truthfulness (TruthfulQA), commonsense reasoning (HellaSwag), mathematical reasoning (GSM8K), and code generation (HumanEval), while tying for the lead in advanced reasoning (BBH). This broad excellence makes it the most well-rounded model among those analyzed.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Claude 3.7 Sonnet&lt;&#x2F;strong&gt; is a very close second with an average score of 84.67%. It matches ChatGPT 4o in advanced reasoning (BBH) and performs strongly in most areas, though it falls slightly behind in truthfulness (TruthfulQA) and code generation (HumanEval). Its near parity with ChatGPT 4o highlights its competitive edge.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DeepSeek R1&lt;&#x2F;strong&gt; secures a solid third place with an average score of 81.00%. It delivers competitive results across most metrics but lags in advanced reasoning (BBH) and truthfulness (TruthfulQA).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Gemini 2.0 Flash&lt;&#x2F;strong&gt; trails with an average score of 75.83%, likely due to its design as a faster, smaller model optimized for efficiency.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;This tool was used to help generate ideas for the outline, provide explanations of keywords, and offer feedback on specific prose.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Generative AI tools are excellent tools for brainstorming, offering feedback, and providing explanations. These tools should not be trusted for accuracy. Any specific detail mentioned must be externally validated.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Concrete example: changed “A memory type that holds data after power loss. Has high density and sits between DRAM and SSD.” to “A type of memory that retains data even after power is lost, combining non-volatility with high density and often byte-addressable access, bridging the gap between DRAM and SSDs.”&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Information given could be misleading. For example, the LLM claimed that a weakness of the paper was that “Intel axed Optane in 2022 after a $7B loss, signaling a “lack of profit” that the paper doesn’t address. This omission sidesteps a critical real-world angle—why a promising tech flopped commercially—limiting its forward-looking relevance.” However, this paper came out before Intel “axed” optane in 2022 so this weakness should not be fairly considered.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>DRust: Language-Guided Distributed Shared Memory with Fine Granularity, Full Transparency, and Ultra Efficiency</title>
                <pubDate>Mon, 17 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/drust/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/drust/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;The paper, &lt;em&gt;DRust: Language-Guided Distributed Shared Memory with Fine Granularity, Full Transparency, and Ultra Efficiency&lt;&#x2F;em&gt;, explores a novel approach that takes Rust’s ownership model and uses it to optimize the performance of a distributed shared memory (DSM) system. Integrating the ownership semantics into the coherence protocol, DRust eliminates unnecessary overhead from synchronization, allowing it to outperform current existing DSM systems.&lt;&#x2F;p&gt;
&lt;p&gt;Traditional DSM systems face challenges such as synchronization overhead and the complexity of maintaining memory coherency and consistency across distributed nodes. DRust addresses these challenges with Rust’s memory safety features, and incorporates them into the DSM runtime. It ensures fine-grained memory coherence without needing to manually synchronize everything. This minimizes network traffic and bottlenecks stemming from synchronization, leading to a more efficient and scalable solution.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;overview&quot;&gt;Overview&lt;&#x2F;h1&gt;
&lt;p&gt;DRust enhances distributed shared memory (DSM) systems by incorporating Rust’s ownership model, which guarantees memory safety and consistency. Rust’s core features, such as single-writer, multiple-reader (SWMR) semantics, allow DRust to track ownership, enabling automatic, fine-grained memory coherence without requiring explicit synchronization, reducing network overhead and bottlenecks commonly seen in traditional DSM systems.&lt;&#x2F;p&gt;
&lt;p&gt;This system replaces the standard Rust pointers with distributed-aware structures which can track ownership and memory access across nodes. This approach allows objects to migrate based on ownership transfers, minimizing cache invalidations and reducing unnecessary network fetches. By integrating these features, DRust can manage memory coherence across distributed systems without additional synchronization costs, reducing communication traffic and accelerating memory access.&lt;&#x2F;p&gt;
&lt;p&gt;DRust further optimizes performance by leveraging RDMA (Remote Direct Memory Access) for high-speed memory access between nodes. It also utilizes affinity-based scheduling to place threads close to the data they are working with, ensuring optimal data locality. This reduces the need for cross-node memory access, enhancing both scalability and performance. DRust integrates seamlessly with existing Rust applications, requiring only minimal modifications to the codebase, which allows developers to efficiently scale their applications to distributed environments with minimal overhead.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;results&quot;&gt;Results&lt;&#x2F;h1&gt;
&lt;p&gt;In performance evaluations, DRust outperforms existing DSM systems, including GAM and Grappa, with significant speedups. Notably:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;DRust achieves a &lt;strong&gt;1.33x to 2.64x&lt;&#x2F;strong&gt; speedup over GAM.&lt;&#x2F;li&gt;
&lt;li&gt;It shows &lt;strong&gt;2.53x to 29.16x&lt;&#x2F;strong&gt; speedup compared to Grappa.&lt;&#x2F;li&gt;
&lt;li&gt;The system incurs only a &lt;strong&gt;2.42%&lt;&#x2F;strong&gt; slowdown compared to single-node execution, indicating minimal overhead.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In real-world workload tests, DRust excels in areas such as DataFrame processing (e.g., Polars), where it achieves up to &lt;strong&gt;5.57x&lt;&#x2F;strong&gt; speedup over single-node execution. Social networking benchmarks (e.g., DeathStarBench) benefit from DRust’s ability to eliminate serialization overhead by passing references rather than data copies. For high-compute workloads like GEMM, DRust’s scheduling ensures effective scaling across nodes, minimizing communication bottlenecks.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;strong&gt;Why do we need single write? Why is that invariant important?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Having a single write is important because it prevents multiple writes to the same data from happening. If there are multiple writes to the same data, an issue appears in the form of a question. &lt;em&gt;Who’s write are we going to take?&lt;&#x2F;em&gt; Having a single write allows the writes to be in some order so the information will gradually spread, but it also prevents conflicts and addition synchronization. Though if it’s just reading, everyone can have a copy because each copy will be the same, therefore there won’t be a conflict of which one is the “correct” one.&lt;&#x2F;li&gt;
&lt;li&gt;If multiple writes were allowed, this would cause more overhead compared to single writes. With a language like Rust, programmers can rely on the language to handle synchronization, mapping, and other semantics, whereas if they’re using something like GAM, who enforces partial store order with asynchronous writes (significant overhead), it doesn’t map cleanly, leading to more overhead without single write enforcement&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Can you explain runtime? Are startup files a part of the runtime system?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The concept of runtime was explore a little bit, with an explanation where on a system with only one server, Rust will take care of coherency, but on a system with multiple servers, someone else would have to provide that coherency. DRust’s runtime is essentially normal Rust but with a distributed server library. Further explanations explained how runtime is built into the system, comparing C code and the use of the main function to clarify the role of runtime. Rust doesn’t natively support distributed systems, which is why you need a runtime component if using Rust in a distributed system.&lt;&#x2F;li&gt;
&lt;li&gt;Embedded systems have a startup file that does a very similar thing. The startup file itself is a part of the runtime system, whereas a bootloader would not be as runtime is included in any file ran in a program.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Is it adaptable to different types of hardware?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Though the paper doesn’t talk a lot about hardware, DRust can be integrated into various types of hardware to manage coherency, assuming the hardware can support distributed shared memory (DSM). RMC is an example of hardware that does support DSM.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;Can we reason where messaging will happen with a write?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Asynchronous requests involve a remote server deallocating the original object’s memory, which allows for more efficient memory management. Messaging occurs with mutable references, and optimizing those messages requires aligning the ownership and asynchronous processes. The memory ownership model is the same, meaning that fewer messages are required. Since only one owner has write access at a time, the model inherently limits the number of messages since only one mutable write can occur at a time. This allows the system to effectively scale with the coherence protocol.&lt;&#x2F;li&gt;
&lt;li&gt;Showing the network traffic to demonstrate that there is reduced messaging could have been helpful, but it was noted that deallocation requests are asynchronous and don’t contribute to the overall cost.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;strong&gt;How does the system handle ownership changes and synchronization?&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Requiring a message to check if the data is stale, or ensuring ownership is valid, as this is managed by maintaining both the global and local addresses and the switching is based on the global access patterns. The server tree knows whenever a new allocation happens because when a write occurs, a synchronous message is sent to update the ownership.&lt;&#x2F;li&gt;
&lt;li&gt;Ownership synchronization must happen first as read and write operations both depend on any ownership changes. Once ownership has updated, the read and write operations can proceed accordingly, ensuring consistency is maintained. During this process, no reads can occur, preventing accidentally reading garbage or outdated data.&lt;&#x2F;li&gt;
&lt;li&gt;Unlike traditional systems, invalidation messages aren’t needed during a single write state because a global synchronization message is sent instead whenever a mutable value is taken. This further reduces overhead and simplifies coherence enforcement. Borrowing still requires synchronous communication which introduces some slowdown, leading to some overhead, but limited to about 2% from Rust to DRust with some optimizations. Additionally, if an object is updated within a single server, synchronization is not needed.&lt;&#x2F;li&gt;
&lt;li&gt;In a typical DSM system, data is transferred at a granularity of a page, regardless of data size leading to IO amplification. IO amplification is when a storage device needs to perform a significantly larger amount of input&#x2F;output (I&#x2F;O) operations than what is requested by the user. This inefficiency appears due to sending the entire page regardless of how much actual data is on that page. DRust improves on this by allowing mutable state transitions without needing to send the entire page as it only sends the necessary object data. This reduces the network traffic as well as the storage overhead.  Additionally, if a write operation updates the entire object, an invalidation message could be sent without including the data.&lt;&#x2F;li&gt;
&lt;li&gt;Ownership transfers during garbage collection could help prevent a cache from being backed up. It could also improve security and restrict third parties from accessing cached data, but this was not confirmed.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;DRust represents a breakthrough in the design of distributed shared memory systems, integrating Rust’s ownership model to provide fine-grained memory coherence with minimum synchronization overhead. Its ability to reduce network traffic, improve scalability, and optimize memory access without requiring major changes to existing codebases makes it a powerful tool for modern distributed computing.&lt;&#x2F;p&gt;
&lt;p&gt;While there are some limitations, DRust’s performance across a wide range of benchmarks shows how much potential it has in revolutionizing DSM systems. With further improvements, it could become a fundamental piece for future distributed computing applications&lt;&#x2F;p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ul&gt;
&lt;li&gt;Paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi24-ma-haoran.pdf&quot;&gt;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;osdi24-ma-haoran.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Slides: &lt;a href=&quot;https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1mw51FuJ3MR3hRSj8RUZ_3DmiPsyvcwF0&#x2F;view?usp=sharing&quot;&gt;https:&#x2F;&#x2F;drive.google.com&#x2F;file&#x2F;d&#x2F;1mw51FuJ3MR3hRSj8RUZ_3DmiPsyvcwF0&#x2F;view?usp=sharing&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Video: &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=19JN3Kg2yYo&quot;&gt;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=19JN3Kg2yYo&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;DRust Full Coherence Protocol Details: &lt;a href=&quot;https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2406.02803v2#A2&quot;&gt;https:&#x2F;&#x2F;arxiv.org&#x2F;html&#x2F;2406.02803v2#A2&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>IVY: A Shared Virtual Memory System for Parallel Computing</title>
                <pubDate>Wed, 12 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/ivy/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/ivy/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;In this paper, we are introduced to IVY, a shared virtual memory system for parallel computing. Published in 1988, IVY was a concept designed for a single token ring topology that was implemented in Apollo workstations. The main idea was to evaluate if shared memory was a better alternative than a memory passing system, especially with loosely coupled microprocessors since there hadn’t been enough work done at the time.&lt;&#x2F;p&gt;
&lt;p&gt;Rather than relying on message passaging, this new architecture coupled with IVY allowed for simultaneous use of virtual memories on processes happening in parallel processors. This approach highlights how shared virtual memory in parallel programs can exploit capabilities that aren’t feasible in memory passing.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;shared-memory-vs-memory-passing&quot;&gt;Shared Memory vs. Memory Passing&lt;&#x2F;h2&gt;
&lt;p&gt;Memory passing is done through an interprocess communication, which does not use global data and relies on programmers to handle the memory passing through send and receive in parallel programs. This is not ideal as the communication process is slow and inefficient for process migration and handling complex data structures. These complications come from having multiple address spaces, which shared memory gets rid of. This in turn has no problem passing complex data structures as the shared memory address can be passed around using pointers. For parallel programming in a multiprocessor system, shared memory is the ideal memory system to implement.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;&#x2F;h2&gt;
&lt;p&gt;The implementation setup is established on the Apollo domain, an integrated system composed of workstations and servers connected in a ring topology. In this setup, IVY has 5 modules. These include process management, memory allocation, initialization, remote operation, and memory mapping. A key part of this implementation is the virtual memory address, a single address space that will be shared by all the processors.&lt;&#x2F;p&gt;
&lt;p&gt;The address then gets partitioned into read only pages and write pages. Read only pages can exist on any node while write pages can only exist on one node. The memory mapper handles the read and write operations with three algorithms: the centralized manager, the fixed distributed manager, and dynamic distributed manager.&lt;&#x2F;p&gt;
&lt;p&gt;When it comes to process management, IVY implements process control blocks (PCBs), which store information such as process state that are used for process migration and scheduling. This data structure implementation helps to simplify process migration and synchronization. For memory allocation, IVY uses a single level allocator. The authors do point out that a two level memory management approach would be more efficient, as this module only lets one processor allocate memory at a time.&lt;&#x2F;p&gt;
&lt;p&gt;The remote operation module enables remote communication between the other modules in IVY. It uses a remote procedural call (RPC) to broadcast requests across the network and get responses, as well as forwarding requests from one processor to another. IVY is written in Pascal, which means programmers need to account for the nature of pascal procedure calls when implementing the system. It’s essential that programmers and compilers define what’s shared memory as well as private memory.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
&lt;p&gt;IVY is written in Pascal, and so are the experiments that were done to test for side effects in shared data structures as well as its granularity in parallelism. The first two experiments involve pascal programs of jacobi algorithms that solve linear equations and 3D partial differential equations. Other programs include the traveling salesman problem, matrix multiply, dot product, and split-merge sort. The results shown in figures 5 and 6 of the paper, we find that both split-merge sort and dot product tests do not yield linear speedups. As for the other programs, we find that the linear speedup increases when multiple processors are introduced. Multiprocessors have more memory, which means it will handle less page faults and have a better yield.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;ivy&#x2F;figure5.jpg&quot; alt=&quot;Algorithm speedups&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Another interesting point is that sometimes IVY achieves super-linear results, as shown in the following figure. This can be attributed to the low memory size of the uniprocessor that the IVY system was compared to - because of this low memory size, the uniprocessor was unable to hold all of a given algorithm’s (e.g., large matrix multiplication) page tables within memory, leading to huge performance losses in swapping pages from disk to memory. With IVY, since there were multiple processors being used, these algorithms could be stored in memory without swapping to disk - this leads to superlinear performance in the case of IVY. Proof of this large amount of page swapping is shown in Table 1. Of note is that if IVY was tested today, it likely would not have achieved superlinear results because of much larger standard memory capacities.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;ivy&#x2F;figure4.jpg&quot; alt=&quot;Large matrix speedup&quot; &#x2F;&gt;
&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;ivy&#x2F;table1.jpg&quot; alt=&quot;Table 1&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;weakness&quot;&gt;Weakness&lt;&#x2F;h2&gt;
&lt;p&gt;This paper has several weaknesses, but a notable one is the lack of comparison with memory passing. The paper makes a case for shared virtual memory, but it would’ve been insightful to see a performance difference to establish further validity. Another weakness is that the memory allocation is single level, which means only one processor can allocate memory at a time. Even the paper itself notes it would be more efficient to implement a two level memory management approach. Another weakness is that the high overhead of IVY is caused by its implementation in user mode, rather than system mode . As mentioned in the results, the split-merge sort and dot product tests did not yield a linear speed-up, which could come from bottlenecks such as the single level memory allocator as well as its high overhead.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;ivy&#x2F;figure6.jpg&quot; alt=&quot;Speedup of merge-split sort&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;is-pascal-a-dead-language&quot;&gt;Is Pascal a Dead Language?&lt;&#x2F;h3&gt;
&lt;p&gt;No, Pascal is not entirely dead—it continues to influence modern programming languages, particularly Java. One of Pascal’s most significant contributions was its p-code (pseudo-code) virtual machine, which was the first implementation of bytecode execution. Although Java does not use p-code directly, its Java Virtual Machine (JVM) is conceptually similar, operating as a stack-based execution environment much like Pascal’s VM.&lt;&#x2F;p&gt;
&lt;p&gt;When Java was developed, its creators aimed for object orientation and platform portability, leading them to adopt and extend ideas from Pascal’s virtual machine. In a way, Java’s runtime environment can be seen as an evolution of concepts pioneered by Pascal’s p-code system.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;user-mode-in-ivy&quot;&gt;User Mode in Ivy&lt;&#x2F;h3&gt;
&lt;p&gt;Ivy operates in user mode, as mentioned on page 95 of the paper, where it describes its prototype implementation.&lt;&#x2F;p&gt;
&lt;p&gt;This raises the question: Was Ivy implemented as a shared library? While the paper does not explicitly state this, the fact that Ivy runs in user mode suggests that key functionalities, such as memory management and page handling, were likely provided via user-space libraries rather than kernel modifications.&lt;&#x2F;p&gt;
&lt;p&gt;One critical aspect of Ivy’s design is its page fault handling. The system provides a user-space page fault handler, meaning that when a page fault occurs, it is handled at the user level instead of being immediately processed by the operating system’s kernel. This is similar to mechanisms in modern Linux, where page fault handlers can be invoked in user space, a feature often used in networking and distributed systems to manage memory more efficiently.&lt;&#x2F;p&gt;
&lt;p&gt;The Apollo page fault handler played a crucial role in Ivy’s implementation, allowing the system to intercept memory access requests and manage them at the application level. This approach provided flexibility but also introduced performance trade-offs, as user-space page fault handling is generally slower than direct kernel-managed memory operations.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;what-is-a-centralized-manager-in-ivy&quot;&gt;What is a Centralized Manager in Ivy?&lt;&#x2F;h3&gt;
&lt;p&gt;In Ivy, a centralized manager is responsible for coordinating shared memory access and ownership changes between processors.&lt;&#x2F;p&gt;
&lt;p&gt;When a processor (Processor A) needs access to a shared memory page, it must first interact with the centralized manager. The manager performs several key functions:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Checks the virtual-to-physical mapping to determine whether the requested page is already available on the requesting processor.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Handles invalidations and ownership transfers if the page is currently owned by another processor.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Ensures consistency by enforcing memory coherence rules across multiple nodes.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;If a requested page is not locally available, the centralized manager identifies the current owner of the page, facilitates its transfer, and invalidates other copies if necessary. This ensures that only one writable copy of a page exists in the system at any time.&lt;&#x2F;p&gt;
&lt;p&gt;While this approach simplifies memory management, it introduces a potential bottleneck, as all processors must communicate with a single entity to request pages, leading to scalability concerns in larger systems.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;where-is-the-code-for-ivy&quot;&gt;Where is the Code for Ivy?&lt;&#x2F;h3&gt;
&lt;p&gt;The original Ivy implementation was developed in the late 1980s, but its source code was not publicly released, as was common practice at the time.&lt;&#x2F;p&gt;
&lt;p&gt;However, similar implementations have been developed based on Ivy’s protocol, including:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;LibDSM – A modern distributed shared memory (DSM) library that incorporates some of Ivy’s concepts.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Giant – An implementation that directly uses the Ivy protocol to provide shared virtual memory.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Although Ivy itself is not readily available, these projects serve as useful references for understanding and experimenting with distributed shared memory systems based on Ivy’s architecture.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;apollo-domain-os-vs-unix&quot;&gt;Apollo Domain OS vs. Unix&lt;&#x2F;h3&gt;
&lt;p&gt;Apollo developed its own proprietary operating system called Domain&#x2F;OS, which was designed specifically for networked workstations and featured built-in support for distributed computing. Unlike Unix, which was widely adopted and evolved into various open-source and commercial distributions, Domain&#x2F;OS was a niche system tailored to Apollo hardware.&lt;&#x2F;p&gt;
&lt;p&gt;However, Hewlett-Packard (HP) acquired Apollo in 1989 and eventually discontinued Domain&#x2F;OS, favoring Unix-based systems instead. This marked the end of Apollo’s independent operating system, as HP integrated Apollo’s technology into its own Unix-based platforms before phasing out Domain&#x2F;OS entirely.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;superlinear-scaling-and-memory-considerations-in-ivy&quot;&gt;Superlinear Scaling and Memory Considerations in Ivy&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;superlinear-scaling-observations&quot;&gt;Superlinear Scaling Observations&lt;&#x2F;h4&gt;
&lt;p&gt;The Ivy paper highlights superlinear speedup for large matrix sizes, particularly in Figure 4, to demonstrate its performance benefits. This effect is likely due to reduced page faults as more processors increase the available memory pool, improving data locality. However, smaller matrices did not exhibit the same superlinear speedup, as their working set sizes were small enough to fit within a single processor’s cache, limiting the benefits of shared memory.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-examine-system-capabilities&quot;&gt;Why Examine System Capabilities?&lt;&#x2F;h4&gt;
&lt;p&gt;The paper explores Ivy’s capabilities in order to address page thrashing and memory efficiency—critical factors in distributed shared memory (DSM) systems. This discussion ties into modern research on far memory and disaggregated memory, where memory is physically separated from compute nodes but accessed as if it were local, potentially reducing page migration overhead.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;shared-memory-and-memory-utilization&quot;&gt;Shared Memory and Memory Utilization&lt;&#x2F;h4&gt;
&lt;p&gt;One important question not fully addressed in the paper is: How much memory is saved due to sharing? Ivy’s shared memory model allows multiple processors to access the same data, but a quantitative metric for memory utilization could provide better insights. Understanding the amount of memory saved through Ivy’s approach would help assess its efficiency compared to traditional message-passing models.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;causes-of-performance-degradation&quot;&gt;Causes of Performance Degradation&lt;&#x2F;h4&gt;
&lt;p&gt;The paper notes that performance curves flatten or degrade due to process migration overhead and false sharing.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Process migration occurs when a process moves between nodes, requiring memory transfers that can cause delays.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;False sharing happens when multiple processors frequently access the same memory page, even if they are working on separate data within that page, leading to unnecessary invalidations and cache coherence traffic.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;A useful improvement would have been software counters to track these slowdowns, allowing for a more detailed analysis of bottlenecks in Ivy’s performance.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;virtual-memory-and-disaggregated-memory&quot;&gt;Virtual Memory and Disaggregated Memory&lt;&#x2F;h4&gt;
&lt;p&gt;Ivy’s use of virtual shared memory raises the question of how disaggregated memory architectures—where memory and compute resources are physically separated—might have improved its performance. Modern interconnects like PCIe Express Link could potentially reduce the latency of remote memory accesses, making Ivy’s shared memory model more practical in today’s hardware environments.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;could-hardware-modifications-improve-ivy&quot;&gt;Could Hardware Modifications Improve Ivy?&lt;&#x2F;h3&gt;
&lt;p&gt;Yes, hardware advancements could have significantly improved Ivy’s performance. Since the system relied on software-based shared virtual memory (SVM), many of its inefficiencies stemmed from high latency memory accesses, page migration overhead, and slow interconnects.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;potential-hardware-improvements&quot;&gt;Potential Hardware Improvements:&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;Faster Interconnects (e.g., PCIe Express Link)&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ivy used a 12 MB&#x2F;s Apollo ring network, which became a major bottleneck.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Modern interconnects like PCIe Express Link or CXL (Compute Express Link) could drastically reduce memory access latency and improve bandwidth, making shared virtual memory much more viable.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Hardware-managed remote memory access (RMA) could eliminate some of the overhead caused by software-driven page migrations.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;ol start=&quot;3&quot;&gt;
&lt;li&gt;Dedicated Hardware for Shared Memory Management&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ivy relied on software page fault handling, which was slow.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Hardware-based page migration and cache coherence mechanisms, like those found in NUMA (Non-Uniform Memory Access) architectures, could have significantly reduced the overhead.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Specialized memory controllers could have handled page tracking and ownership resolution at the hardware level rather than relying on a centralized software manager.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;ol start=&quot;5&quot;&gt;
&lt;li&gt;Improvements in Cache Coherence Mechanisms&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Ivy suffered from false sharing and excessive page invalidations.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Hardware-assisted coherence protocols, such as directory-based coherence or MOESI (Modified, Owner, Exclusive, Shared, Invalid) protocols, could have reduced unnecessary memory traffic.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;ahead-of-its-time&quot;&gt;Ahead of Its Time&lt;&#x2F;h4&gt;
&lt;p&gt;Ivy was a pioneering system, introducing shared virtual memory at a time when hardware was not yet mature enough to fully support it.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The concept of distributed shared memory (DSM) later re-emerged in systems like TreadMarks (1989) and modern far-memory architectures.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Had Ivy been implemented on modern hardware with high-speed interconnects, memory disaggregation, and improved cache coherence, it could have been a viable alternative to message-passing models.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Ultimately, Ivy did not receive the implementation it truly deserved at the time due to hardware limitations. However, its core ideas remain relevant in today’s discussions on distributed memory, disaggregated computing, and heterogeneous architectures.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;ivy-s-industry-adoption-and-modern-relevance&quot;&gt;Ivy’s Industry Adoption and Modern Relevance&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;not-adopted-in-industry-but-still-used-in-research&quot;&gt;Not Adopted in Industry, but Still Used in Research&lt;&#x2F;h4&gt;
&lt;p&gt;Ivy’s shared virtual memory model was never widely adopted in industry. Instead, the industry moved towards message-passing architectures, which became the dominant paradigm in high-performance computing (HPC) and distributed systems. However, researchers continue to explore shared virtual memory concepts, particularly in the context of far memory and disaggregated memory architectures.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;far-memory-and-modern-trends&quot;&gt;Far Memory and Modern Trends&lt;&#x2F;h4&gt;
&lt;p&gt;One of the key takeaways from Ivy’s design is that it used virtual memory numbers to trigger software-based page faults, a mechanism that could have been implemented in hardware for better performance. This tradeoff between software vs. hardware page fault handling remains relevant today, particularly as far memory architectures gain traction.&lt;&#x2F;p&gt;
&lt;p&gt;Far memory architectures aim to:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Reduce page thrashing by allowing memory to be shared across multiple nodes with minimal migration overhead.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Leverage fast interconnects (e.g., CXL, PCIe Express Link) to provide low-latency memory access.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;trade-offs-in-software-page-fault-handling&quot;&gt;Trade-offs in Software Page Fault Handling&lt;&#x2F;h4&gt;
&lt;p&gt;Ivy’s software-based page fault handler provided flexibility but introduced additional overhead. A key advantage of software page fault handling is that the CPU can perform other work while waiting for a page to be fetched—a feature that modern operating systems, such as Linux, continue to leverage.&lt;&#x2F;p&gt;
&lt;p&gt;However, Ivy’s implementation had limitations:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Write protection was supported, but there was no mechanism to update read protection dynamically, relying entirely on hardware mechanisms.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;There was a plan to implement read protection updates, but it was never completed in the Ivy prototype.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;lessons-for-modern-systems&quot;&gt;Lessons for Modern Systems&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Balancing software and hardware implementations is crucial. While software page fault handling provides flexibility, hardware-assisted page fault resolution (as seen in modern NUMA and memory-disaggregated systems) can drastically improve performance.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Buffering techniques can reduce page faults, but they introduce data staleness issues, which must be managed carefully in consistency models.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;The core challenges Ivy faced—efficient memory access, process migration, and minimizing false sharing—are still relevant today, particularly as researchers explore disaggregated memory architectures.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;treadmarks-a-distributed-shared-memory-system-similar-to-ivy&quot;&gt;TreadMarks: A Distributed Shared Memory System Similar to Ivy&lt;&#x2F;h3&gt;
&lt;p&gt;TreadMarks, released in 1989, is a distributed shared memory (DSM) system designed for standard workstations and operating systems, particularly Unix-based systems.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-treadmarks-compares-to-ivy&quot;&gt;How TreadMarks Compares to Ivy&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Conceptually Similar: Like Ivy, TreadMarks provides a shared virtual memory abstraction for loosely coupled multiprocessors.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Key Difference: Unlike Ivy, which was implemented on Apollo workstations with a ring network, TreadMarks was designed to work on commodity Unix-based workstations connected via standard networking.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Lack of Citation: Interestingly, TreadMarks does not cite the Ivy paper, despite their similarities in approach. This could be due to independent development or a lack of awareness about Ivy’s contributions at the time.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;significance-of-treadmarks&quot;&gt;Significance of TreadMarks&lt;&#x2F;h4&gt;
&lt;p&gt;TreadMarks became one of the more well-known DSM implementations, demonstrating that distributed shared memory could be implemented on standard hardware and operating systems rather than requiring specialized architectures. While neither Ivy nor TreadMarks saw widespread industrial adoption, the ideas explored in these systems continue to inform modern research in far memory, memory disaggregation, and distributed computing.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;memory-allocation-and-process-blocking-in-ivy&quot;&gt;Memory Allocation and Process Blocking in Ivy&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;system-level-lock-for-memory-allocation&quot;&gt;System-Level Lock for Memory Allocation&lt;&#x2F;h4&gt;
&lt;p&gt;Ivy enforces a system-wide lock for memory allocation, meaning that only one processor can allocate memory at a time. When a processor gains the lock, all other processors attempting to allocate memory must wait until the lock is released, effectively blocking their execution during this time.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;why-is-locking-necessary&quot;&gt;Why is Locking Necessary?&lt;&#x2F;h4&gt;
&lt;p&gt;Since Ivy uses a shared virtual address space, proper synchronization is required to prevent multiple processors from allocating the same memory region simultaneously. Without locking, two processors could:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Allocate overlapping memory regions, causing memory corruption.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Map to the same virtual address, leading to incorrect writes and inconsistency in the shared memory system.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Trigger race conditions, making it difficult to maintain coherence across nodes.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;By serializing memory allocation with a global lock, Ivy ensures correctness but introduces a performance bottleneck, particularly as the number of processors increases.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;impact-of-the-apollo-ring-network&quot;&gt;Impact of the Apollo Ring Network&lt;&#x2F;h4&gt;
&lt;p&gt;Ivy was implemented on an Apollo ring network with a bandwidth of 12 MB&#x2F;s, which was relatively slow even for its time. This limited bandwidth exacerbated the performance overhead caused by:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Global memory allocation locks, which forced processors to wait rather than proceeding with execution.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Memory page transfers, which had to traverse the ring network, further slowing down the allocation process.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;In modern systems, such bottlenecks are mitigated by NUMA-aware memory allocation, distributed memory management, and high-bandwidth interconnects like PCIe, CXL, and Infiniband.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;typical-use-case-for-ivy-and-apollo-workstations&quot;&gt;Typical Use Case for Ivy and Apollo Workstations&lt;&#x2F;h3&gt;
&lt;p&gt;Ivy was designed for workstations rather than large-scale supercomputers. It was implemented on Apollo workstations, which were primarily used by engineering and CAD (Computer-Aided Design) professionals. Major Apollo customers included:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Mentor Graphics&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Ford&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;General Motors&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Boeing&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;These industries required high-performance computing for design, modeling, and simulation, making Ivy’s shared virtual memory approach appealing for parallel processing workloads on distributed workstations.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;industry-context-custom-vs-commodity-hpc&quot;&gt;Industry Context: Custom vs. Commodity HPC&lt;&#x2F;h3&gt;
&lt;p&gt;During the late 1980s and early 1990s, the industry saw a divide in high-performance computing (HPC):&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Custom-built supercomputers dominated the HPC landscape.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Researchers and engineers began exploring commodity-based HPC clusters as a lower-cost alternative.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Ivy represented one approach to enabling commodity-based HPC, leveraging distributed shared memory across standard workstations instead of relying on expensive custom-built supercomputers.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;message-passing-vs-shared-virtual-memory&quot;&gt;Message Passing vs. Shared Virtual Memory&lt;&#x2F;h3&gt;
&lt;p&gt;A key question at the time was whether shared virtual memory (SVM) (as seen in Ivy) or message-passing models (such as MPI) would be more effective for commodity-based HPC.&lt;&#x2F;p&gt;
&lt;p&gt;This discussion eventually led to the Beowulf cluster model, which emerged in the 1990s.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Beowulf clusters used commodity hardware connected via message passing networks instead of shared virtual memory.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Message passing became the dominant paradigm in distributed computing, largely due to its scalability and lower overhead compared to SVM.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;While Ivy’s shared virtual memory model was ahead of its time, it ultimately did not become the standard for HPC, as message-passing techniques proved to be more efficient and scalable for large distributed systems.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;apollo-ring-network-and-alternative-interconnects&quot;&gt;Apollo Ring Network and Alternative Interconnects&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;was-the-apollo-ring-network-unidirectional&quot;&gt;Was the Apollo Ring Network Unidirectional?&lt;&#x2F;h4&gt;
&lt;p&gt;Yes, the Apollo ring network used in Ivy was a token-based ring, meaning that data traveled in one direction around the network. This is similar to a relay race, where a token (message) is passed from one node to the next in a circular fashion.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;comparison-with-other-network-topologies&quot;&gt;Comparison with Other Network Topologies&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Token Ring: Used in Apollo’s implementation. A single token circulates, and only the node holding the token can transmit.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Ethernet: Uses a shared medium where multiple nodes can attempt to transmit simultaneously, requiring collision detection and retransmission.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Bus Network: A single shared communication channel where all nodes can hear transmissions, leading to congestion at high loads.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Mesh Network: Nodes are connected in a grid-like structure, allowing direct communication between neighbors (North, South, East, West).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Torus Network: A grid network wrapped into a ring-like topology, reducing hop distance and improving communication latency.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;High Radix Networks: Networks with many connections per node, improving performance but expensive and difficult to implement due to wiring constraints.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Dragonfly Network: A hybrid of high-radix and low-diameter networks, used in modern supercomputers to minimize hop count.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;why-the-ring-network-was-a-poor-choice-for-ivy&quot;&gt;Why the Ring Network was a Poor Choice for Ivy&lt;&#x2F;h4&gt;
&lt;p&gt;The ring topology used in Apollo’s implementation was a bottleneck because:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;It did not scale well as more nodes were added.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;It forced all traffic to flow in a single direction, leading to delays.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;A single point of failure could disrupt the entire network.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;modern-networks-and-ivy-s-relevance&quot;&gt;Modern Networks and Ivy’s Relevance&lt;&#x2F;h4&gt;
&lt;p&gt;Most modern supercomputers use a combination of high-radix and mesh-based interconnects, optimizing bandwidth and reducing hop count for communication.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The PlayStation 3 (PS3) used a ring network for its interconnect but with higher bandwidth, making it more efficient than the Apollo ring network.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Ivy itself was not dependent on the ring network; it was simply implemented on one. Had it been deployed on a more scalable network topology (e.g., mesh or torus), performance could have been significantly improved.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;key-takeaways&quot;&gt;Key Takeaways&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The Apollo ring network was unidirectional, making it a poor choice for scaling Ivy.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Mesh and torus networks would have provided better performance.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Modern HPC systems use hybrid interconnects, such as Dragonfly, to minimize latency and maximize bandwidth.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Ivy’s shared virtual memory model was limited more by its network implementation than by its fundamental design.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;evaluation-of-ivy-in-the-context-of-cs-research&quot;&gt;Evaluation of Ivy in the Context of CS Research&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;cs-research-in-the-1980s-vs-today&quot;&gt;CS Research in the 1980s vs. Today&lt;&#x2F;h4&gt;
&lt;p&gt;The bar for publishing computer science research was lower in the 1980s compared to today.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;There were fewer competitors in shared memory research, making Ivy a remarkable contribution at the time, though not revolutionary by today’s standards.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;If submitted today, the Ivy paper would likely be rejected, as modern research demands rigorous comparisons with alternative approaches and more extensive performance evaluations.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;CS students today have fewer “low-hanging fruit” problems to tackle, as many foundational ideas in parallel computing, distributed systems, and memory architectures have already been explored.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h4 id=&quot;ivy-s-main-weakness-lack-of-comparison-with-message-passing&quot;&gt;Ivy’s Main Weakness: Lack of Comparison with Message Passing&lt;&#x2F;h4&gt;
&lt;p&gt;One of Ivy’s biggest shortcomings is that it does not compare its performance against message-passing techniques, which later became the dominant model for distributed computing (e.g., MPI). Without such a comparison, it’s difficult to assess whether Ivy’s shared virtual memory approach was truly superior for real-world workloads.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;how-does-rust-play-into-this&quot;&gt;How Does Rust Play Into This?&lt;&#x2F;h4&gt;
&lt;p&gt;While the Ivy paper predates Rust, modern Rust-based systems programming offers features that could mitigate some of Ivy’s weaknesses, particularly in memory safety and concurrency:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Rust’s ownership model could help eliminate race conditions and ensure safe access to shared memory without requiring a centralized manager.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Rust’s async and multi-threading features could help in designing more efficient distributed shared memory (DSM) systems with reduced overhead.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Rust’s low-level control and performance optimizations make it well-suited for high-performance distributed systems, which Ivy aimed to be.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;While Ivy’s design was ahead of its time, it suffered from high overhead and poor scalability. Modern techniques—including better interconnects, Rust-based concurrency management, and message-passing optimizations—would likely yield a more efficient implementation of a similar concept today.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;source&quot;&gt;Source&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;IVY: A Shared Virtual Memory System for Parallel Computing. https:&#x2F;&#x2F;systems.cs.columbia.edu&#x2F;ds2-class&#x2F;papers&#x2F;li-ivy.pdf&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.eecg.toronto.edu&#x2F;~amza&#x2F;ece1747h&#x2F;papers&#x2F;treadmarks94.pdf&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>Segcache: a memory-efficient and scalable in-memory key-value cache for small objects</title>
                <pubDate>Mon, 10 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/segcache/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/segcache/</guid>
                <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h1&gt;
&lt;p&gt;Modern web applications use in-memory key-value caches such as Memcached and Redis to ensure fast response times and high throughput. However, these systems have some drawbacks, including high metadata overhead, inefficient expiration management, and challenges with scalability. This paper presents Segcache, a new caching solution that addresses these issues by reimagining the way objects are stored and managed.&lt;&#x2F;p&gt;
&lt;p&gt;Segcache takes a different approach. TTL-indexed segment chains group objects that have similar creation and expiration times, making it easier to expire and evict a bunch of data at the same time. Metadata sharing reduces per-object overhead down to just 5 bytes, a 91% reduction compared to Memcached, by merging the metadata into shared segment headers. Instead of evicting objects one by one, merge-based eviction removes entire segments, keeping the more frequently accessed data while still getting memory space back.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;segcache-design-and-implementation&quot;&gt;Segcache Design and Implementation&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;segcache&#x2F;fig-3.png&quot; alt=&quot;Figure 3&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;As illustrated in the figure above, Segcache consists of three main components:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;TTL Buckets:&lt;&#x2F;strong&gt; used for reclaiming expired objects.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Object Store:&lt;&#x2F;strong&gt; used to store segments as a log.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Hash table:&lt;&#x2F;strong&gt; used for object lookup.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;Segcache is implemented as a storage module within the open-sourced Pelikan project. [5], [6]&lt;&#x2F;p&gt;
&lt;h1 id=&quot;results&quot;&gt;Results&lt;&#x2F;h1&gt;
&lt;p&gt;These optimizations lead to significant performance gains. Compared to existing caching systems, Segcache uses 22-60% less memory and compared to Memcached on a single thread, it delivers up to 40% better throughput. It also scales pretty well, achieving an 8x speedup over Memcached with 24 threads.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;segcache&#x2F;fig-10.png&quot; alt=&quot;Figure 10&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h1 id=&quot;discussion&quot;&gt;Discussion&lt;&#x2F;h1&gt;
&lt;p&gt;&lt;strong&gt;Leader’s comment&lt;&#x2F;strong&gt;: Segcache doesn’t need to do a full-cache scan in order for it to remove expired objects, which helps improve its efficiency, but also reduces the overhead seen in traditional caching systems.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;class-discussion&quot;&gt;Class discussion&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q: What is the granularity at which locks are used?&lt;&#x2F;strong&gt; &lt;strong&gt;A:&lt;&#x2F;strong&gt; Just have locks at segment granularity.&lt;&#x2F;li&gt;
&lt;li&gt;Segment can have objects that are modified by two clients at a time - if puts are run concurrently could get invalid state of segment so need to enforce mutual exclusion to ensure correctness,&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q: What do the dotted lines in the graph (Fig 8 in [1]) below represent?&lt;&#x2F;strong&gt; &lt;strong&gt;A:&lt;&#x2F;strong&gt; The dotted lines represent the metadata size in each implementations. Smaller the object metadata size, the better miss ratio. Authors of the paper should’ve explained this better in the paper.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;segcache&#x2F;fig-8.png&quot; alt=&quot;Figure 8&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Q: Everywhere in the paper authors say relative miss ratio, i.e. relative to production [2], etc, why show against production?&lt;&#x2F;strong&gt; &lt;strong&gt;A:&lt;&#x2F;strong&gt; They had to pick something, If not in production why would they choose it?&lt;&#x2F;li&gt;
&lt;li&gt;Patterns along the gray bars in graphs are better for readability.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q: What is the relation ship between objects and segments?&lt;&#x2F;strong&gt; &lt;strong&gt;A:&lt;&#x2F;strong&gt; Objects are contained in segments. We could have key just in segments, all the segments get populated with just key updates.  Have to have some way to collapse them.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q: In the author’s presentation video, they claim that S3 gets filled you just add segment to end.  But segment is fixed size, how does that work?&lt;&#x2F;strong&gt; &lt;strong&gt;A:&lt;&#x2F;strong&gt; If you run out of slots in a segment you add a new one.  There’s no limit on the number of segments but each is fixed size.  If you fill one up you add a new one. Also, the number of buckets are fixed then the length of linked list is arbitrary.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q: How do you read objects?  Do you have to walk multiple segments or just one?&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;TTL is not just a technical requirement but also a legal one (GDPR law passed in EU).&lt;&#x2F;li&gt;
&lt;li&gt;Apparently there are people already building off of segcache. Students in Australia are building hypercache building off of this [4]. Took bucket idea and expiration.&lt;&#x2F;li&gt;
&lt;li&gt;SQLite has an in-memory implementation, and does not work with multi-threaded apps, does not have proper locking and documentation says is unsafe.  Nevertheless companies still use it. Sometimes you want to do a JOIN.  Moderns DBMS have on-the-fly JOINs.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q: Does persistent storage have to be disk?&lt;&#x2F;strong&gt;  &lt;strong&gt;A:&lt;&#x2F;strong&gt; LSM trees are usually designed for SSDs.  When talking about flash storage you have to worry about wear-leveling, so if you just take a naive log structured implementation you could wear out cells if you don’t implement compaction carefully.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Q: Benefits of log-structured trees?&lt;&#x2F;strong&gt; &lt;strong&gt;A:&lt;&#x2F;strong&gt; many log structured systems batch writes so for correctness they need to be careful to get proper results.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;&#x2F;h1&gt;
&lt;p&gt;In conclusion, Segcache uses a TTL-indexed segment structured storage mechanism to achieve high throughput, high scalability and high memory efficiency when compared to the existing state-of-the-art research&#x2F;production level solutions.&lt;&#x2F;p&gt;
&lt;h1 id=&quot;references&quot;&gt;References&lt;&#x2F;h1&gt;
&lt;ol&gt;
&lt;li&gt;Paper: &lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;nsdi21-yang.pdf&quot;&gt;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;nsdi21-yang.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Slides:&lt;a href=&quot;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;nsdi21_slides_yang-juncheng.pdf&quot;&gt;https:&#x2F;&#x2F;www.usenix.org&#x2F;system&#x2F;files&#x2F;nsdi21_slides_yang-juncheng.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;Video: &lt;a href=&quot;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=s7YtO0rk9WM&quot;&gt;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=s7YtO0rk9WM&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;HyperCache: &lt;a href=&quot;https:&#x2F;&#x2F;shbakram.github.io&#x2F;assets&#x2F;papers&#x2F;honors-thesis-junming.pdf&quot;&gt;https:&#x2F;&#x2F;shbakram.github.io&#x2F;assets&#x2F;papers&#x2F;honors-thesis-junming.pdf&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;http:&#x2F;&#x2F;www.github.com&#x2F;twitter&#x2F;pelikan&quot;&gt;http:&#x2F;&#x2F;www.github.com&#x2F;twitter&#x2F;pelikan&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;a href=&quot;http:&#x2F;&#x2F;www.github.com&#x2F;thesys-lab&#x2F;segcache&quot;&gt;http:&#x2F;&#x2F;www.github.com&#x2F;thesys-lab&#x2F;segcache&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
</description>
            </item>
        
            <item>
                <title>HotStuff: BFT Consensus with Linearity and Responsiveness</title>
                <pubDate>Wed, 05 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/hotstuff/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/hotstuff/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;HotStuff is a Byzantine Fault Tolerance (BFT) consensus protocol that achieves much lower time complexity than previous BFT consensus implementations (i.e., PBFT, SBFT, etc). HotStuff is implemented for partially synchronous systems. It achieves a lower time complexity by using a 4-step design, pipelining (in chained HotStuff), and message cryptography. HotStuff also provides optimistic responsiveness.&lt;&#x2F;p&gt;
&lt;p&gt;There are several parts of any BFT protocol: first, the BFT protocol must provide a view-change, which is an algorithm that allows a new leader to collect information and send it to the follower nodes. Second, the BFT protocol must provide a way for all the nodes to come to a consensus, which is dependent on the BFT protocol itself.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;keywords&quot;&gt;Keywords&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;BFT: Byzantine Fault Tolerance; toleration of faulty&#x2F;malicious nodes.&lt;&#x2F;li&gt;
&lt;li&gt;Consensus: Multiple nodes coming to an agreement on a subject.&lt;&#x2F;li&gt;
&lt;li&gt;GST: Global Synchronization Time, or the point in time where afterwards a message has a known transmission bound.&lt;&#x2F;li&gt;
&lt;li&gt;Optimistic responsiveness: A guarantee that a non-faulty leader node will be able to drive a system to consensus after a known condition (i.e., after GST) in a time frame dependent on the actual delays of messages on the network.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;problem&quot;&gt;Problem&lt;&#x2F;h2&gt;
&lt;p&gt;Current BFT protocols do not scale well for larger-node distributed systems (PBFT, SBFT, etc.); some protocols accomplish consensus in O(N2) time, with N being the number of nodes. Hotstuff remedies this problem by having a time complexity of O(N) for consensus and O(N) for leader replacement. See the below figure (Table 1) for a breakdown of the time complexities of different BFT protocols.
&lt;img src=&quot;https:&#x2F;&#x2F;github.com&#x2F;user-attachments&#x2F;assets&#x2F;0cdeb461-3d70-4472-a770-bd37f9287e47&quot; alt=&quot;Performance&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;implementation&quot;&gt;Implementation&lt;&#x2F;h2&gt;
&lt;p&gt;HotStuff has 4 steps:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Prepare - The leader node sends a proposal to all follower nodes; the follower nodes determine if the proposal is valid and if so, they send prepare votes to the leader.&lt;&#x2F;li&gt;
&lt;li&gt;Pre-Commit - The leader waits for the followers to agree to a proposal (get pre-commit votes from follower nodes), then combines these votes into a pre-commit QC and sends it to all nodes for authentication.&lt;&#x2F;li&gt;
&lt;li&gt;Commit - Follower nodes vote on moving to the Commit stage. If n-f nodes agree to move to the Commit stage, then they are locked into that commit QC.&lt;&#x2F;li&gt;
&lt;li&gt;Decide - Hotstuff moves to the next view.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;p&gt;HotStuff uses Cryptography for messages: each node has a signature assigned to it, which is used to provide proof that a message comes from a given node. The signatures from each node are aggregated and merged into a Quorum Certificate (QC), which allows for a simple and fast way to see if a consensus has been reached or not. This decreases the number of signatures required to be authenticated to O(N) time.&lt;&#x2F;p&gt;
&lt;p&gt;The chained implementation of HotStuff allows for pipelining - a set of nodes can be in different views at the same time, with each view being in a different HotStuff stage. This allows for more parallelism and better performance.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion&lt;&#x2F;h2&gt;
&lt;p&gt;Is the paper and accompanying slides based on the basic version of HotStuff, or the pipelined version of HotStuff?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;The paper goes over both versions, with performance comparisons being drawn between the two in the slides.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Can a leader election happen between the two command stages? Does it ever happen?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;HotStuff can have different leaders between commands. The table in the slides showcases a chained HotStuff pipeline example.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;What does optimistic responsiveness mean?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Optimistic responsiveness ensures that even with a faulty leader, a system can elect a new leader and can keep going.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;For all the protocols being tested in Table 1, are all the protocols in the table partially synchronous models or partially asynchronous?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Asynchronous BFT protocols cannot exist (they cannot reach consensus); partially synchronous means the same thing as partially asynchronous, but the table uses BFT protocols that are labeled as partially synchronous.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Is HotStuff used in industry?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Hotstuff was used for quite a bit at Facebook, it’s still recent, so there is not as much adoption as one would expect.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;For a distributed system that requires consistent data, would BFT be the beneficial choice?&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;If your system and nodes are secured, you probably don’t need BFT.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;sources&quot;&gt;sources&lt;&#x2F;h2&gt;
&lt;p&gt;https:&#x2F;&#x2F;dl.acm.org&#x2F;doi&#x2F;10.1145&#x2F;3293611.3331591&lt;&#x2F;p&gt;
&lt;p&gt;https:&#x2F;&#x2F;courses.corelab.ntua.gr&#x2F;pluginfile.php&#x2F;9663&#x2F;course&#x2F;section&#x2F;1387&#x2F;22-23.atc.balla.pdf&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>In Search of an Understandable Consensus Algorithm</title>
                <pubDate>Mon, 03 Feb 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/understandable-consensus-algorithm/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/understandable-consensus-algorithm/</guid>
                <description>&lt;h2 id=&quot;background&quot;&gt;Background&lt;&#x2F;h2&gt;
&lt;p&gt;Raft is a consensus algorithm that was created by two researchers at Stanford University in order to create a more understandable algorithm than Paxos. These algorithms were created for separate servers that work together to form a coherent group. The benefit of this working model is that the group can survive the death of some of the separate servers. The two researchers attempted to learn Paxos for over a year but were ultimately unsuccessful. The researchers decided that a new consensus algorithm needed to be designed with understandability at the forefront.&lt;&#x2F;p&gt;
&lt;p&gt;In order to create a more understandable consensus algorithm the authors focused on decomposition of the solutions into three parts: leader election, log replication, and safety mechanisms. If the solutions could be broken down into smaller pieces, this would create more manageable pieces for understanding and using Raft. Another technique they used to increase the understandability was reducing the state spaces to simplify behavior and decrease nondeterminism. Unlike Paxos, which allows entries in the log to be entered in different ways, Raft ensures all logs match the leader’s log, avoiding inconsistencies and this stems from having a leader, replicating logs to all systems, and the leader enforcing a log order.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;main-contributions&quot;&gt;Main Contributions&lt;&#x2F;h2&gt;
&lt;p&gt;The researchers contributed a new consensus algorithm that is open source.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;results&quot;&gt;Results&lt;&#x2F;h2&gt;
&lt;p&gt;The results of this paper showed that students scored higher test scores for Raft when given lectures about Raft and Paxos with the order of the lectures distributed evenly across the test group. 33 students out of 43 scored higher on the Raft test than on the Paxos test. The other result that was discussed in the paper was that the performance of Raft is comparable to Paxos.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;impacts&quot;&gt;Impacts&lt;&#x2F;h2&gt;
&lt;p&gt;The new consensus algorithm, Raft, was quickly and widely adopted which is in stark contrast to Paxos. Paxos was created in the late 1970s by Leslie Lamport but even though Lamport published Paxos paper, there was no widespread adoption. So he later published a “Paxos made simple” paper to make it more adaptable because people could not understand or use Paxos. This is why Raft is seen as an important alternative to Paxos. People were using Raft within weeks of this paper being published.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;strength-and-weaknesses&quot;&gt;Strength and Weaknesses&lt;&#x2F;h2&gt;
&lt;p&gt;This paper highlighted the understandability of a consensus algorithm and the paper was written with seemingly similar goals in mind. This paper was by far the easiest to read of any we have looked at so far in class so that is a strength that not many technical papers have. The downside of this is that they seemed to have lost some technical detail that would have strengthened their claims of superiority. There was no test done to actually compare the functionality of Paxos to Raft and yet they claim that the performance is similar. This seems slightly suspicious especially after the authors self-reported that they couldn’t really use Paxos. I also would like to see a larger sample size than 43 since this is a small sample size to get statistically significant data from. A weakness of Raft itself is that using one server as a leader could create a bottleneck within high-traffic systems.&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>End to End Sequential Consistency</title>
                <pubDate>Wed, 29 Jan 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/end-to-end-sc/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/end-to-end-sc/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This blog post covers End-to-End Sequential Consistency, a research paper published on June 13, 2012, and presented in our group on January 29, 2025. The paper tackles the challenge of making Sequential Consistency (SC) efficient enough for real-world use. While SC ensures a clear, intuitive execution order for multi-threaded programs, its strict memory ordering usually comes with a steep performance hit. To address this, the authors propose a low-overhead SC implementation that combines compiler optimizations and lightweight hardware modifications. Their approach classifies memory accesses as “safe” (private or read-only) or “unsafe” (shared and writable), allowing most operations to bypass SC constraints. Benchmarks show that this hybrid approach incurs only a 6.2% overhead compared to TSO hardware, which suggest that end-to-end SC might be more practical than previously thought.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;background-and-context&quot;&gt;Background and Context&lt;&#x2F;h2&gt;
&lt;p&gt;Modern multiprocessors use weak memory consistency models like Total Store Order (TSO) and Relaxed Memory Order (RMO) to boost performance by reordering memory operations, which makes multi-threaded programming harder to reason about. Sequential Consistency (SC) offers a much simpler model by ensuring all memory operations appear in a globally ordered sequence, but enforcing this model dampens performance. Compilers and processors further complicate things by reordering instructions to optimize execution, meaning SC isn’t naturally preserved. To ensure correctness, programmers typically rely on locks, memory fences, and atomic operations, which add complexity and slow things down.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;keywords&quot;&gt;Keywords&lt;&#x2F;h2&gt;
&lt;h4 id=&quot;memory-consistency-model&quot;&gt;Memory Consistency Model&lt;&#x2F;h4&gt;
&lt;p&gt;Defines how memory operations (reads&#x2F;writes) appear across threads in a multiprocessor system, impacting parallel program behavior and performance.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;sequential-consistency-sc&quot;&gt;Sequential Consistency (SC)&lt;&#x2F;h4&gt;
&lt;p&gt;A strict memory model where operations execute in program order and all threads see a single global order. Improves predictability but hurts performance.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;total-store-order-tso&quot;&gt;Total Store Order (TSO)&lt;&#x2F;h4&gt;
&lt;p&gt;A weaker model  that allows loads to execute before earlier stores complete, improving speed while maintaining store ordering for consistency.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;relaxed-memory-order-rmo&quot;&gt;Relaxed Memory Order (RMO)&lt;&#x2F;h4&gt;
&lt;p&gt;A high-performance model that allows both loads and stores to be reordered, requiring explicit synchronization to maintain correctness.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;thread-local-memory&quot;&gt;Thread-Local Memory&lt;&#x2F;h4&gt;
&lt;p&gt;Memory only accessed by a single thread, avoiding synchronization overhead and allowing SC optimizations by skipping ordering constraints.&lt;&#x2F;p&gt;
&lt;p&gt;Shared Read-Only Memory&lt;br &#x2F;&gt;
Data readable but not writable by multiple threads. Since it never changes, it doesn’t require strict ordering, allowing SC optimizations.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;store-buffer&quot;&gt;Store Buffer&lt;&#x2F;h4&gt;
&lt;p&gt;A hardware queue that holds pending stores before committing to memory. Improves performance but can break SC if loads execute before previous stores complete.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;fifo-store-buffer&quot;&gt;FIFO Store Buffer&lt;&#x2F;h4&gt;
&lt;p&gt;A First-In-First-Out buffer that preserves store order for shared memory, enforcing SC only where necessary.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;out-of-order-execution-oooe&quot;&gt;Out-of-Order Execution (OoOE)&lt;&#x2F;h4&gt;
&lt;p&gt;A CPU optimization that executes instructions as soon as dependencies are ready, improving performance but requiring memory ordering mechanisms to enforce SC.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;static-analysis&quot;&gt;Static Analysis&lt;&#x2F;h4&gt;
&lt;p&gt;A compile-time optimization where LLVM detects thread-local and read-only accesses, marking them as safe for SC efficiency.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;dynamic-analysis&quot;&gt;Dynamic Analysis&lt;&#x2F;h4&gt;
&lt;p&gt;A runtime optimization where the OS tracks memory sharing and updates classifications, preventing unnecessary SC enforcement.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;llvm-compiler-framework&quot;&gt;LLVM Compiler Framework&lt;&#x2F;h4&gt;
&lt;p&gt;An open-source compiler infrastructure modified to classify memory accesses and ensure SC is preserved without unnecessary performance loss.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary-of-the-paper&quot;&gt;Summary of the Paper&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;1-introduction&quot;&gt;1. Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Memory consistency models define how multi-threaded programs interact with memory, balancing programmer simplicity with system performance. Sequential Consistency (SC) is one of the strongest models, enforcing a global order of memory operations, but its strict constraints limit compiler and hardware optimizations, making it costly to implement efficiently.&lt;&#x2F;p&gt;
&lt;p&gt;This paper proposes a practical SC hardware design that avoids speculation by recognizing that most memory accesses are private or shared read-only, meaning SC constraints aren’t always necessary. By classifying memory accesses at compile time and runtime, SC enforcement is applied only when needed. The design splits the store buffer into a FIFO buffer for shared writes and an unordered buffer for private writes, reducing stalls and improving efficiency. Results show only a 6.2% overhead compared to TSO, making SC enforcement viable for modern processors.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;2-background&quot;&gt;2. Background&lt;&#x2F;h2&gt;
&lt;p&gt;Most programming languages only guarantee SC for data-race-free programs, but many real-world programs contain data races, intentional or not. Modern compilers violate SC through optimizations like instruction reordering, which is why languages use weak memory models. However, research shows that an SC-preserving LLVM compiler can retain most optimizations while keeping overhead low (~3.8%). Running these SC-safe binaries on SC hardware would provide end-to-end SC enforcement.&lt;&#x2F;p&gt;
&lt;p&gt;Designing efficient SC hardware remains a challenge because naive SC forces strict ordering, severely limiting performance. Existing approaches rely on complex speculation and rollback mechanisms, which are costly and difficult to implement.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;3-relaxing-memory-model-constraints-for-safe-accesses&quot;&gt;3. Relaxing Memory Model Constraints for Safe Accesses&lt;&#x2F;h2&gt;
&lt;p&gt;Many memory operations, such as thread-local or shared read-only accesses, do not require strict ordering since they cannot be observed or altered by other threads. If the compiler or runtime system can guarantee an access is safe, the processor can freely reorder it, improving performance without violating SC.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;4-design-memory-access-type-driven-sc-hardware&quot;&gt;4. Design: Memory Access Type Driven SC Hardware&lt;&#x2F;h2&gt;
&lt;p&gt;The proposed SC hardware optimizes memory ordering by classifying accesses as safe or unsafe. This classification is determined through a hybrid approach: static compiler analysis, which flags function-local and read-only accesses, and dynamic runtime analysis, which tracks page-level access patterns via the OS and MMU. If either method deems an access safe, SC constraints are relaxed, minimizing performance overhead.&lt;&#x2F;p&gt;
&lt;p&gt;To further improve efficiency, the store buffer is split into two parts: a FIFO buffer for unsafe stores (preserving order) and an unordered buffer for safe stores (allowing faster execution). This setup lets safe loads commit without waiting for stores.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;5-static-classification-of-memory-accesses&quot;&gt;5. Static Classification of Memory Accesses&lt;&#x2F;h2&gt;
&lt;p&gt;The compiler classifies memory accesses as safe (private or read-only shared) or unsafe (shared writeable) and marks them accordingly in machine code. Safe accesses can be reordered, while unsafe ones must follow SC constraints. Function-local, non-escaping variables and literals are classified as safe, while global, heap-allocated, and escaped variables are unsafe. To maintain correctness, any instruction that accesses both safe and unsafe variables is marked as unsafe. This static analysis is conservative but avoids the runtime costs of page-level tracking.&lt;&#x2F;p&gt;
&lt;p&gt;To ensure correct store-to-load forwarding, loads and stores to the same address must have the same classification. The processor performs an extra check before committing a store to avoid conflicts when different functions reuse stack locations. This ensures safe and unsafe stores do not execute out of order. Union variables are always marked unsafe, as aliasing complicates classification. In CISC architectures, memory instructions can access multiple variables, so the ISA is extended to store separate safety bits for each memory operand, ensuring proper classification at the micro-operation level.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;6-dynamic-classification-of-memory-accesses&quot;&gt;6. Dynamic Classification of Memory Accesses&lt;&#x2F;h2&gt;
&lt;p&gt;Since static classification is conservative, a dynamic approach leverages the Memory Management Unit (MMU) and OS page protection to track memory access patterns at runtime. Pages are classified as private, shared read-only, or shared read-write, with only the last requiring strict SC enforcement. The Translation Lookaside Buffer (TLB) is extended with a safe bit to quickly determine if an access is safe. If either static or dynamic classification marks an access as safe, SC constraints are relaxed, reducing performance overhead.&lt;&#x2F;p&gt;
&lt;p&gt;Page states transition dynamically based on access patterns. A new page starts as “untouched”, and the first access sets it as private. If another thread reads it, it becomes shared read-only, and if written to by multiple threads, it transitions to shared read-write, requiring strict SC. Inter-processor interrupts (IPIs) ensure memory ordering during transitions, invalidating TLB entries and flushing store buffers. TLB entries are reset on context switches to maintain correctness, and Direct Memory Access (DMA) interactions are managed by either enforcing exclusive access or temporarily marking affected pages as unsafe.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;7-results&quot;&gt;7. Results&lt;&#x2F;h2&gt;
&lt;p&gt;The proposed SC-hybrid hardware reduces the overhead of enforcing Sequential Consistency (SC) to just 2.0% on average, a significant improvement over the 9.1% overhead of baseline SC. Worst-case overhead is 5.4% (facesim) compared to 28.8% in the baseline SC design. The hybrid classification approach, which combines static compiler analysis and dynamic page-based classification, achieves 71.5% accuracy, close to the 81.5% of an ideal byte-level scheme. End-to-end SC enforcement, including both compiler and hardware constraints, incurs a 6.2% overhead compared to a traditional TSO processor running a stock compiler, far lower than the 12.7% cost of naive SC. Store buffer optimizations ensure minimal additional hardware cost, with only a 1% increase in overhead when buffer sizes are reduced.&lt;&#x2F;p&gt;
&lt;h4 id=&quot;important-results&quot;&gt;Important Results&lt;&#x2F;h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;This paper demonstrates that Sequential Consistency (SC) can be enforced efficiently by selectively applying memory constraints only where necessary, reducing overhead without complex speculation.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Instead of enforcing strict SC for all memory accesses, the approach classifies them as safe (thread-local or shared read-only) or unsafe (shared writeable) by using a hybrid classification method, avoiding unnecessary stalls for 81% of accesses.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Results show that the SC-hybrid design incurs just 2.0% overhead over TSO, while full end-to-end SC (compiler + hardware) adds only 6.2%, far lower than naive SC’s 12.7% penalty. Benchmarks confirm near-TSO performance, with worst-case overhead at 5.4% (facesim) vs. 28.8% in naive SC.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;strengths-and-weaknesses&quot;&gt;Strengths and Weaknesses&lt;&#x2F;h2&gt;
&lt;h4 id=&quot;strengths&quot;&gt;Strengths&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Efficient SC Enforcement with Minimal Overhead&lt;br &#x2F;&gt;
Achieves end-to-end SC with just 6.2% overhead, significantly lower than traditional SC implementations (~12.7%). Avoids speculative execution, making it simpler and more power-efficient than past SC approaches.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Dual-Buffer Store Architecture&lt;br &#x2F;&gt;
Introduces a FIFO + Unordered Store Buffer system, reducing stalls and improving efficiency.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Hybrid Memory Classification (Compiler + Runtime)&lt;br &#x2F;&gt;
Combines static compiler analysis (LLVM-based) with dynamic runtime tracking (MMU &amp;amp; page tables) to accurately classify memory accesses with minimal overhead.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h4 id=&quot;weaknesses&quot;&gt;Weaknesses&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Limited Applicability Beyond x86&lt;br &#x2F;&gt;
Designed for TSO-based architectures (like x86), which already enforce relatively strong memory consistency. Adoption in weaker memory models like ARM is uncertain, as they rely more on relaxed consistency.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Reliance on OS Support for Runtime Classification&lt;br &#x2F;&gt;
Requires OS modifications for tracking memory access types, which may limit adoption on existing systems.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Evaluation Based on Simulation, Not Real Hardware&lt;br &#x2F;&gt;
All results are from a Simics-based simulator, not real hardware. Without FPGA or ASIC testing, real-world feasibility remains uncertain.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion:&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;1-what-are-some-architectures-that-prioritize-performance-over-programming-ease-of-use&quot;&gt;1.  What are some architectures that prioritize performance over programming ease of use?&lt;&#x2F;h3&gt;
&lt;p&gt;Some architectures sacrifice ease of programming for raw performance, making them powerful but difficult to work with.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;IBM’s Cell processor (used in the PS3) had manual workload distribution and memory management, making it fast but developer-unfriendly.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Intel’s Itanium relied on compiler-managed parallelism (EPIC), which hurt adoption due to compiler complexity.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Intel Xeon Phi – Optimized for massively parallel workloads, but required special programming models.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;VLIW Processors – Found in early GPUs&#x2F;DSPs, demanded precise instruction scheduling.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;RISC-V Vector Extensions – Powerful but requires manual low-level management.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;2-balance-between-sequential-consistency-and-understandability-of-architecture-code&quot;&gt;2.  Balance between sequential consistency and understandability of architecture&#x2F;code?&lt;&#x2F;h3&gt;
&lt;p&gt;There’s a trade-off between sequential consistency (SC) and system understandability. SC makes multi-threaded code easier to reason about, but it restricts optimizations, slowing down execution. On the other hand, weaker memory models allow better performance but make debugging concurrency issues much harder.&lt;&#x2F;p&gt;
&lt;p&gt;In reality, only a small percentage of code actually requires strong consistency—most programs don’t frequently share stores between processors. For these cases, weaker memory models (like TSO or RC) are a better fit, as they allow higher performance without unnecessary synchronization overhead.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;3-multiprocessors-work-in-lockstep-how-do-they-synchronize-instructions-faulty-instructions&quot;&gt;3.  Multiprocessors work in lockstep? How do they synchronize instructions&#x2F;faulty instructions?&lt;&#x2F;h3&gt;
&lt;p&gt;Multiprocessors do not operate in lockstep—each core runs independently, and there are no guarantees that instructions will execute at the same time across all cores. Some cores may run at different speeds due to frequency scaling, thermal constraints, or workload differences, but this doesn’t matter for applications that don’t share data.For high-performance computing (HPC) workloads, where synchronization is critical, frequency scaling is often disabled, ensuring all cores run at a fixed speed to avoid performance variation and keep parallel computations aligned.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;4-shared-page-can-be-unsafe&quot;&gt;4.  Shared page can be unsafe?&lt;&#x2F;h3&gt;
&lt;p&gt;A shared memory page can be unsafe because it might contain both safe (thread-local or read-only) and unsafe (shared writeable) accesses. This makes it difficult to blindly treat an entire page as safe, as a single writeable access can break consistency guarantees. To handle this, a Finite State Machine (FSM) can track and dynamically classify memory accesses at the page level. The FSM can adjust permissions and enforcement rules based on how a page is being accessed.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;5-what-platform-did-they-use-to-run-these-benchmarks&quot;&gt;5.  What platform did they use to run these benchmarks?&lt;&#x2F;h3&gt;
&lt;p&gt;The benchmarks were run on a simulated platform, not real hardware. The authors used a Simex processor simulator, which started with an x86 core and was modified to support their SC-aware optimizations. They also implemented a modified LLVM compiler to classify memory accesses as safe or unsafe, integrating their approach into the software stack. The evaluation included Apache server benchmarks alongside PARSEC and SPLASH-2, representing real-world multi-threaded workloads.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;6-instructions-xyz-in-diff-threads-can-map-to-the-same-physical-location-what-does-it-mean-when-can-it-happen&quot;&gt;6.  Instructions xyz (in diff threads) can map to the same physical location - what does it mean, when can it happen?&lt;&#x2F;h3&gt;
&lt;p&gt;In multi-threaded programs, different threads can have instructions that access the same physical memory location, even if they use different virtual addresses. This can lead to unexpected memory interactions and potential safety issues.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;7-has-this-been-implemented-anywhere-in-actual-hardware&quot;&gt;7.  Has this been implemented anywhere (in actual hardware)?&lt;&#x2F;h3&gt;
&lt;p&gt;This approach has not been implemented in actual hardware. The paper was published in 2012, and there are no known commercial processors that have adopted this exact SC enforcement method. One major reason is that industry is slow to adopt changes that fundamentally alter core architecture. Implementing end-to-end SC enforcement requires modifications to hardware, compilers, operating systems, and programming models, making it a massive undertaking with uncertain incentives. Given that weak memory models already work well for performance, most companies don’t see a strong reason to switch.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;8-implementation-in-risc-v-and-arm&quot;&gt;8.  Implementation in RISC-V and ARM?&lt;&#x2F;h3&gt;
&lt;p&gt;This approach could be interesting for RISC-V, as it supports multiple consistency models, allowing flexibility in experimenting with SC enforcement. However, ARM relies on weak consistency for performance and efficiency, making it unlikely to adopt a strict SC model, especially since its OS ecosystem is already optimized for relaxed consistency models.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;9-challenges-with-page-table-approach&quot;&gt;9.  Challenges with Page Table Approach?&lt;&#x2F;h3&gt;
&lt;p&gt;One issue with enforcing SC at the page level is that each page (typically 4KB) may contain both safe and unsafe data. If one part of the page needs strict SC while another doesn’t, the system would have to either enforce SC on the whole page (wasting performance) or split memory in a way that increases complexity. In many cases, the amount of mixed safe&#x2F;unsafe data in a page might not justify the overhead of tracking and enforcing SC at that granularity.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;10-better-implementation-would-have-done-analysis-on-the-page-table&quot;&gt;10.  Better implementation - would have done analysis on the page table?&lt;&#x2F;h3&gt;
&lt;p&gt;A more refined approach would involve deeper analysis at the page table level, ensuring that only truly shared read-write memory gets SC enforcement. Instead of enforcing SC on entire pages, the system could track memory regions at a finer granularity, reducing unnecessary constraints. A more efficient solution would involve adaptive tracking, ensuring that SC is only applied where actual shared modifications occur, rather than blindly enforcing it at the entire page level. This would help balance performance and correctness while minimizing unintended synchronization overhead.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;11-what-about-the-language-mem-addr-can-escape-thread-and-be-used-in-a-diff-thread-are-there-languages-that-prevent-this&quot;&gt;11.  What about the language? Mem addr can escape thread and be used in a diff thread; are there languages that prevent this?&lt;&#x2F;h3&gt;
&lt;p&gt;Certain programming languages prevent unsafe memory escapes between threads, making sequential consistency (SC) enforcement easier. Rust, for example, enforces strict ownership and borrowing rules, ensuring that a memory address cannot be accessed by multiple threads unless explicitly marked as safe. This paper was likely ahead of its time since escape analysis was still an evolving concept when this paper was written. Detecting whether a memory address escapes a thread is extremely difficult and requires deep compiler analysis, which is impossible to get 100% correct in many cases.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;12-what-was-this-done-in&quot;&gt;12.  What was this done in?&lt;&#x2F;h3&gt;
&lt;p&gt;The implementation was primarily written in C++ using a modified LLVM compiler to classify memory accesses as safe or unsafe. Since LLVM is widely used for compiler research, this choice allowed fine-grained control over instruction ordering and optimization passes.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;13-memory-models-how-a-programmer-works-with-it-is-important&quot;&gt;13.  Memory models - how a programmer works with it is important?&lt;&#x2F;h3&gt;
&lt;p&gt;Memory models define how programmers interact with memory in multi-threaded programs, and Sequential Consistency (SC) is particularly frustrating because it restricts compiler and hardware optimizations. At the compiler level, memory ordering isn’t fixed because optimizations like instruction reordering can change consistency guarantees. If the compiler doesn’t explicitly enforce SC, it can freely reorder operations unless memory barriers or synchronization primitives are used. This leads to issues with undefined behavior, where the compiler assumes it can optimize memory accesses however it wants unless explicitly told otherwise.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;14-what-are-the-benefits-of-sc&quot;&gt;14.  What are the benefits of SC?&lt;&#x2F;h3&gt;
&lt;p&gt;The main advantage of SC is that it makes multi-threaded programming easier to understand. Since memory operations appear in a global, ordered sequence, programmers don’t have to worry about unexpected reordering by the compiler or hardware, making debugging and reasoning about concurrency much simpler.&lt;&#x2F;p&gt;
&lt;h3 id=&quot;15-analyzing-the-graphs-and-performance-trends&quot;&gt;15.  Analyzing the Graphs and Performance Trends&lt;&#x2F;h3&gt;
&lt;p&gt;Shared vs. Unshared Data Performance&lt;&#x2F;p&gt;
&lt;p&gt;If the paper’s claims hold, workloads with more shared data should perform better under the proposed SC model compared to naive SC. The results should show a clear gap between naive SC (SC-baseline) and the optimized SC-hybrid approach, especially for programs that frequently share memory between threads.&lt;&#x2F;p&gt;
&lt;p&gt;Which Workload Should Perform Best?&lt;&#x2F;p&gt;
&lt;p&gt;From the benchmarks, Barnes has the most shared data, so it should see the biggest performance improvement. However, in Streamcluster, the SC-Ideal case performs better than SC-Hybrid, raising questions about whether SC-Hybrid is fully optimizing memory accesses as expected.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;interpreting-sc-ideal&quot;&gt;Interpreting SC-Ideal&lt;&#x2F;h2&gt;
&lt;p&gt;The SC-Ideal case likely represents a perfect scenario where SC enforcement incurs zero penalties, serving as a best-case theoretical performance bound. The inclusion of “(byte)” in SC-Ideal’s title is unclear—possibly indicating a byte-granularity analysis, but the paper should clarify its significance.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;visualization-issues&quot;&gt;Visualization Issues&lt;&#x2F;h2&gt;
&lt;p&gt;One clear flaw in the presentation is poor graph design—everything is in grayscale, making it harder to distinguish between datasets. Modern conference guidelines recommend using color and patterns to improve readability, so the authors should refine their visualization for better clarity.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;reevaluating-multiprocessing-through-the-sc-lens&quot;&gt;Reevaluating Multiprocessing Through the SC Lens&lt;&#x2F;h2&gt;
&lt;p&gt;SC assumes that shared memory is the common case, enforcing strict ordering to maintain consistency. However, real-world workloads often show the opposite—the majority of memory accesses are thread-local or read-only, with relatively few requiring strict read&#x2F;write (R&#x2F;W) synchronization. This suggests that optimizing for global SC might not be the best approach for modern multiprocessors.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;does-an-instagram-like-system-need-sc&quot;&gt;Does an Instagram-Like System Need SC?&lt;&#x2F;h2&gt;
&lt;p&gt;Implementing Instagram (or any large-scale social media platform) on a multi-core processor raises the question: is Sequential Consistency (SC) necessary? The answer depends on which parts of the system require strict memory ordering.&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;For basic content delivery (loading posts, images, likes), SC is unnecessary because these operations don’t require strict synchronization—eventual consistency is fine.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;For counters (like counts, view counts), SC isn’t required either since approximate values are acceptable, and techniques like relaxed consistency with periodic synchronization work well.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;For critical sections like comments and replies, SC becomes more important. If two users reply to a comment at the same time, ensuring correct order matters, or discussions could become garbled.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;finding-the-right-balance-sc-vs-scalability&quot;&gt;Finding the Right Balance: SC vs. Scalability&lt;&#x2F;h2&gt;
&lt;p&gt;A single point of consistency in the system (such as a single-threaded bottleneck) is a bad idea because it creates a failure-prone and slow system. Instead, modern architectures rely on distributed databases and eventual consistency, selectively enforcing strong consistency where it truly matters (transactional updates, authentication). For programmers, this is a trade-off between consistency and scalability—strict SC ensures correctness but hurts performance, while weaker models improve capacity at the cost of occasional inconsistencies.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;are-they-being-selective-with-their-benchmarks&quot;&gt;Are They Being Selective With Their Benchmarks?&lt;&#x2F;h2&gt;
&lt;p&gt;It’s fair to question whether the benchmarks used in the paper accurately represent real-world workloads. The authors focus on PARSEC, SPLASH-2, and Apache benchmarks, which are widely used in academic research, but they don’t explicitly show how much data sharing actually occurs in these workloads. A moderation graph showing the percentage of shared vs. unshared data accesses would help clarify whether SC optimizations are truly impactful in common cases.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;are-these-workloads-reflective-of-modern-applications&quot;&gt;Are These Workloads Reflective of Modern Applications?&lt;&#x2F;h2&gt;
&lt;p&gt;While these benchmarks were relevant at the time, they don’t necessarily represent today’s software landscape. One glaring omission is AI&#x2F;ML workloads, which heavily rely on parallelism and could benefit from or break under strict SC enforcement. Given that this paper was published before the AI&#x2F;ML boom, it’s understandable that such benchmarks weren’t included, but it still highlights a broader issue with academic evaluations—they often miss fast-moving trends in real-world computing.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;benchmark-selection-matters-in-paper-acceptance&quot;&gt;Benchmark Selection Matters in Paper Acceptance&lt;&#x2F;h2&gt;
&lt;p&gt;A key issue with research papers is how benchmarks influence acceptance. If reviewers believe the authors chose benchmarks that favor their results, they might reject the paper for not testing “software that actually matters.” Adding more diverse workloads, especially those involving data-heavy and modern parallel computing applications, would make the findings more generalizable and harder to dismiss.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources:&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;End-to-end sequential consistency: http:&#x2F;&#x2F;web.cs.ucla.edu&#x2F;~todd&#x2F;research&#x2F;isca12.pdf&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;OSTEP Textbook Remzi Arpaci-Dusseau&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Computer Organization and Design RISC-V Edition The Hardware Software Interface 2nd Edition - December 11, 2020 Authors: David A. Patterson, John L. Hennessy&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Computer Architecture, Sixth Edition: A Quantitative Approach December 2017 Authors: John L. Hennessy, David A. Patterson&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h2 id=&quot;generative-ai&quot;&gt;Generative AI&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Link to specific Tools: &lt;a href=&quot;https:&#x2F;&#x2F;chatgpt.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;chatgpt.com&#x2F;&lt;&#x2F;a&gt; &lt;a href=&quot;https:&#x2F;&#x2F;www.deepseek.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.deepseek.com&#x2F;&lt;&#x2F;a&gt;  &lt;a href=&quot;https:&#x2F;&#x2F;claude.ai&quot;&gt;https:&#x2F;&#x2F;claude.ai&lt;&#x2F;a&gt;&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;This tool was used to help generate ideas for the outline, provide explanations of keywords, and offer feedback on prose.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Claude appears to have a tiny context window that is unusable for long conversations. However, it apparently has a much larger context window than chatgpt which means that chatgpt has been truncating content without informing users. (&lt;a href=&quot;https:&#x2F;&#x2F;prompt.16x.engineer&#x2F;blog&#x2F;claude-sonnet-gpt4-context-window-token-limit&quot;&gt;https:&#x2F;&#x2F;prompt.16x.engineer&#x2F;blog&#x2F;claude-sonnet-gpt4-context-window-token-limit&lt;&#x2F;a&gt;)&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Deepseek is frequently down due to poor cloud service and is also biased&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;ChatGPT has by far the best value proposition because of its user interface.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Generative AI tools are excellent tools for brainstorming, offering feedback, and providing explanations. These tools should not be trusted for accuracy. Any specific detail mentioned must be externally validated.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Concrete example: changed “Has this been implemented anywhere (in actual hardware)&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Published in 2012, could not find anything about it&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Industry balks at research that fundamentally changes core architecture; might not be enough incentive to update architecture&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Another guess: touches every part of the stack: special compiler, hardware, language, etc.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Takes a long time to adopt; either all or nothing with adoption&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;“ to “This approach has not been implemented in actual hardware. The paper was published in 2012, and there are no known commercial processors that have adopted this exact SC enforcement method. One major reason is that industry is slow to adopt changes that fundamentally alter core architecture. Implementing end-to-end SC enforcement requires modifications to hardware, compilers, operating systems, and programming models, making it a massive undertaking with uncertain incentives. Given that weak memory models already work well for performance, most companies don’t see a strong reason to switch.”&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Information given could be misleading. For example, the LLM claimed and the strength of the paper was “Strong Experimental Validation - Uses widely recognized benchmarks to demonstrate real-world applicability.” However, in the discussion, it was pointed out that the authors seemed to cherry pick their benchmarks.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>The Multikernel: Ambitious OS Architecture Ahead of Its Time</title>
                <pubDate>Mon, 27 Jan 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/barrelfish/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/barrelfish/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This paper describes a different paradigm for constructing an OS relying on message-passing instead of shared memory for organizing OS data structures across multiple cores.  Mainstream operating system kernels predominantly make use of shared memory data structures - global information used for the core OS, device drivers or major subsystems that use some form of mutual exclusion to protect access.  This paper presents an alternative approach: data structures are not shared but instead cross-core message-based communication was used instead.&lt;&#x2F;p&gt;
&lt;p&gt;The paper makes a case for the diversity of hardware architectures making it increasingly difficult for an OS to adapt to these differences.  Some of these claims have not aged particularly well from 2009 as seen from today as great improvements have been made in describing to OSes hardware differences so it can adapt - but the core concepts in the paper are still relevant today.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;the-shared-memory-scaling-problem&quot;&gt;The Shared Memory Scaling Problem&lt;&#x2F;h2&gt;
&lt;p&gt;The architecture of the rack&#x2F;board&#x2F;socket&#x2F;die&#x2F;core hierarchy and the corresponding interconnect+cache topology can have a profound impact on the performance of the OS and applications.  The shared kernel data structure approach is exposed to these performance effects as we see from the shared memory RPC benchmarks which show latency increasing as number of cores increase but also as the size of the messages increase (from 1-8 cachelines):&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;barrelfish&#x2F;rpc_shm.PNG&quot; alt=&quot;RPC Shared Memory Results&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;message-passing-to-the-rescue&quot;&gt;Message-Passing to the Rescue?&lt;&#x2F;h2&gt;
&lt;p&gt;The solution used in the Multikernel paper and implemented in the Barrelfish OS passes messages instead, so rather than the kernel data structures being inherently shared, instead messages are sent to request that one responsible entity carry out the requested action:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;barrelfish&#x2F;message_passing.PNG&quot; alt=&quot;Message Passing Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;and an interesting result occurs, showing improved scaling for core count and message sizes versus shared memory:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;barrelfish&#x2F;rpc_message_shm.PNG&quot; alt=&quot;RPC Messaging-Passing and Shared Memory Results&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;The graph above is from a 4-socket AMD server platform circa 2009.  These X86 systems have cache coherency protocols active but the effects are only observed in the RPC implementation itself as the Barrelfish OS does not share other data.&lt;&#x2F;p&gt;
&lt;p&gt;The authors present a comparison of a TLB shootdown (Unmap) flow across Windows, Linux and Barrelfish, the first two using shared memory kernel data structures:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;barrelfish&#x2F;unmap_latency.PNG&quot; alt=&quot;Unmap Latencies Across OSes&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;rpc-shared-memory-optimization&quot;&gt;RPC Shared Memory Optimization&lt;&#x2F;h2&gt;
&lt;p&gt;Interestingly, even though the paper set out to define a “no shared data” OS architecture to enable it to be immune to hardware interconnect and cache coherency implementation differences, the practical effects of the RPC system on the X86 server hardware showed that RPC performance was directly impacted by the hardware implementation.  Specifically, the shared data (RPC message buffer) data flows from the sender to the receiver had a performance impact requiring the RPC layer to be adapted:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;barrelfish&#x2F;multicast.PNG&quot; alt=&quot;Multicast RPC Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;We see that this optimization has a profound impact as the number of cores increases:&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;barrelfish&#x2F;rpc_optimization.PNG&quot; alt=&quot;RPC Optimziation Results&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;Which perhaps goes to show that as long as any shared memory is involved that NUMA is ready to bite you.&lt;&#x2F;p&gt;
&lt;p&gt;Given that the experimental setup still used shared memory for RPC itself and we know the hardware implementation has a measurable impact on performance, it would be interesting to consider using modern (2025) hardware to repeat some of these experiments including RPC mechanisms that did not rely on shared memory at all.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;&#x2F;h2&gt;
&lt;p&gt;Think about the kind of experiments we could set up today to explore the tradeoffs between shared memory and message-passing approaches.  What effects would we see running across more diverse topologies including multiple dies in a package ranging to multiple servers across racks?  Could adjusting interconnect and cache coherency policies fundamentally change the performance characteristics of operating system data sharing?  Is there some hardware assist that would be optimal for OS message passing?  Would an adaptive algorithm that chooses the best of both shared memory and message passing be feasible?&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.sigops.org&#x2F;s&#x2F;conferences&#x2F;sosp&#x2F;2009&#x2F;papers&#x2F;baumann-sosp09.pdf&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.sigops.org&#x2F;s&#x2F;conferences&#x2F;sosp&#x2F;2009&#x2F;slides&#x2F;baumann-slides-sosp09.pdf&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;www.youtube.com&#x2F;watch?v=fZt1LILFyXY&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;barrelfish.org&#x2F;&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;BarrelfishOS&#x2F;barrelfish&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>FMI: Fast and Cheap Message Passing for Serverless Functions</title>
                <pubDate>Wed, 22 Jan 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/fmi/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/fmi/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;This blog post covers &lt;strong&gt;FMI: Fast and Cheap Message Passing for Serverless Functions&lt;&#x2F;strong&gt;, a research paper submitted on &lt;strong&gt;May 15, 2023&lt;&#x2F;strong&gt;, and presented on &lt;strong&gt;January 22, 2025&lt;&#x2F;strong&gt;. The paper introduces the &lt;strong&gt;FaaS Message Interface (FMI)&lt;&#x2F;strong&gt;, a high-performance communication framework for &lt;strong&gt;serverless computing&lt;&#x2F;strong&gt;. Traditional serverless architectures rely on &lt;strong&gt;storage-based communication&lt;&#x2F;strong&gt; (&lt;strong&gt;AWS S3, Redis, DynamoDB&lt;&#x2F;strong&gt;), introducing significant &lt;strong&gt;latency and cost overhead&lt;&#x2F;strong&gt;. FMI overcomes these challenges using &lt;strong&gt;direct TCP communication&lt;&#x2F;strong&gt; enabled through &lt;strong&gt;TCP NAT hole punching (TCPunch)&lt;&#x2F;strong&gt;, reducing &lt;strong&gt;latency, cost, and complexity&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;background-and-context&quot;&gt;Background and Context&lt;&#x2F;h2&gt;
&lt;p&gt;Function-as-a-Service (&lt;strong&gt;FaaS&lt;&#x2F;strong&gt;) is a widely used &lt;strong&gt;serverless&lt;&#x2F;strong&gt; cloud computing model, offering &lt;strong&gt;elastic scaling&lt;&#x2F;strong&gt; and &lt;strong&gt;fine-grained billing&lt;&#x2F;strong&gt;, making it ideal for &lt;strong&gt;machine learning, data analytics, and distributed applications&lt;&#x2F;strong&gt;. However, serverless functions &lt;strong&gt;lack efficient, low-latency communication mechanisms&lt;&#x2F;strong&gt; and often depend on &lt;strong&gt;cloud storage-based solutions&lt;&#x2F;strong&gt; such as &lt;strong&gt;AWS S3, Redis, and DynamoDB&lt;&#x2F;strong&gt;. These solutions &lt;strong&gt;increase latency and cost&lt;&#x2F;strong&gt;, making &lt;strong&gt;frequent inter-function communication&lt;&#x2F;strong&gt; inefficient.&lt;&#x2F;p&gt;
&lt;p&gt;Additionally, serverless functions operate &lt;strong&gt;behind Network Address Translation (NAT) gateways&lt;&#x2F;strong&gt;, preventing &lt;strong&gt;direct&lt;&#x2F;strong&gt; connections between functions. This introduces &lt;strong&gt;overhead and complexity&lt;&#x2F;strong&gt;, requiring functions to &lt;strong&gt;relay messages&lt;&#x2F;strong&gt; through cloud-based storage which further increases &lt;strong&gt;latency&lt;&#x2F;strong&gt;.&lt;&#x2F;p&gt;
&lt;p&gt;To solve these issues, &lt;strong&gt;FMI&lt;&#x2F;strong&gt; introduces &lt;strong&gt;TCP NAT hole punching (TCPunch)&lt;&#x2F;strong&gt; to establish &lt;strong&gt;direct, low-latency&lt;&#x2F;strong&gt; connections between functions.&lt;&#x2F;p&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;keywords&quot;&gt;KEYWORDS&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;High-Performance Computing (HPC)&lt;&#x2F;strong&gt; – Systems designed for maximum processing speed and efficiency, often used for computationally intensive tasks. Utilizes optimized communication protocols like &lt;strong&gt;Message Passing Interface (MPI)&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;I&#x2F;O (Input&#x2F;Output)&lt;&#x2F;strong&gt; – Data transfer mechanisms affecting performance in serverless platforms. High-latency I&#x2F;O limits performance and scalability.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Serverless&lt;&#x2F;strong&gt; – A cloud model where functions execute &lt;strong&gt;without server management&lt;&#x2F;strong&gt;. Cost-effective, scalable, and eliminates infrastructure maintenance.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Function-as-a-Service (FaaS)&lt;&#x2F;strong&gt; – A subset of serverless computing focused on &lt;strong&gt;stateless function execution&lt;&#x2F;strong&gt; with elastic scaling and fine-grained billing.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Remote Memory Access (RMA)&lt;&#x2F;strong&gt; – A mechanism allowing a process to &lt;strong&gt;directly access the memory&lt;&#x2F;strong&gt; of a remote process, reducing access latency.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Stateless Functions&lt;&#x2F;strong&gt; – Functions &lt;strong&gt;without memory of prior invocations&lt;&#x2F;strong&gt;, simplifying scaling and deployment.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Elastic Scaling&lt;&#x2F;strong&gt; – &lt;strong&gt;Dynamic resource allocation&lt;&#x2F;strong&gt; based on workload demand.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Communication Bottleneck&lt;&#x2F;strong&gt; – &lt;strong&gt;Slow and costly&lt;&#x2F;strong&gt; inter-function messaging in serverless platforms.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Cloud Storage&lt;&#x2F;strong&gt; – Persistent storage solutions (AWS S3, Redis, DynamoDB) with &lt;strong&gt;high latency&lt;&#x2F;strong&gt; and additional cost.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Transmission Control Protocol (TCP)&lt;&#x2F;strong&gt; – A &lt;strong&gt;reliable&lt;&#x2F;strong&gt; communication protocol. FMI leverages TCP for message passing.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Network Address Translation (NAT)&lt;&#x2F;strong&gt; – A process that maps private IP addresses to public IPs, enabling internet communication but restricting direct function-to-function networking.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;NAT Hole Punching&lt;&#x2F;strong&gt; – A technique to &lt;strong&gt;bypass NAT restrictions&lt;&#x2F;strong&gt; by using a relay server to share external ports, enabling direct connections.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;TCP NAT Hole Punching (TCPunch)&lt;&#x2F;strong&gt; – The paper’s novel technique for &lt;strong&gt;direct TCP communication&lt;&#x2F;strong&gt; in serverless environments.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Message Passing Interface (MPI)&lt;&#x2F;strong&gt; – A standardized API for &lt;strong&gt;parallel computing&lt;&#x2F;strong&gt; that FMI is inspired by.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;AWS Lambda&lt;&#x2F;strong&gt; – A widely used &lt;strong&gt;serverless computing platform&lt;&#x2F;strong&gt;.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Redis&lt;&#x2F;strong&gt; – An in-memory data store and fast message broker. Suitable for low-latency but requires user-managed scalability.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;DynamoDB&lt;&#x2F;strong&gt; – A fully managed NoSQL database optimized for low-latency data retrieval.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;AWS S3&lt;&#x2F;strong&gt; – Scalable object storage with &lt;strong&gt;high durability&lt;&#x2F;strong&gt; and availability. Has higher latency and cost for frequent communication.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;detailed-summary-of-the-paper&quot;&gt;Detailed Summary of the Paper&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;summary-of-1-introduction&quot;&gt;Summary of 1. Introduction&lt;&#x2F;h3&gt;
&lt;p&gt;The paper introduces the &lt;strong&gt;FaaS Message Interface (FMI)&lt;&#x2F;strong&gt;, a modular and high-performance communication framework designed to address the inefficiencies of serverless communication. Inspired by the Message Passing Interface (MPI), FMI brings standardized abstractions for point-to-point and collective communication to Function-as-a-Service (FaaS) platforms.&lt;&#x2F;p&gt;
&lt;p&gt;Key contributions include:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;A library for message passing that provides common, standardized abstractions for serverless point-to-point and group communication.&lt;&#x2F;li&gt;
&lt;li&gt;Analytical models for communication channels in FaaS and a discussion of the performance-price trade-offs of serverless communication.&lt;&#x2F;li&gt;
&lt;li&gt;A demonstration of the application of FMI to serverless machine learning, showing reduced communication overhead by up to &lt;strong&gt;162x&lt;&#x2F;strong&gt; and cost savings up to &lt;strong&gt;397x&lt;&#x2F;strong&gt; compared to existing solutions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;summary-of-2-design-of-fmi&quot;&gt;Summary of 2. Design of FMI&lt;&#x2F;h3&gt;
&lt;p&gt;FMI’s design includes multiple &lt;strong&gt;communication channels&lt;&#x2F;strong&gt;, each tailored for specific use cases and trade-offs:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Direct Channels&lt;&#x2F;strong&gt;: Utilize TCP connections with NAT hole punching for low-latency, efficient communication.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Mediated Channels&lt;&#x2F;strong&gt;: Leverage cloud storage systems like AWS S3, Redis, and DynamoDB for scenarios requiring persistent data exchange.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;Key features of FMI’s design:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cloud-Agnostic Portability&lt;&#x2F;strong&gt;: Operates across various serverless platforms (AWS Lambda, Kubernetes) while integrating with MPI for hybrid HPC-serverless workflows.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Customizability&lt;&#x2F;strong&gt;: Developers can add new communication protocols (QUIC) and optimize collective algorithms for workload-specific needs.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Collective Operations&lt;&#x2F;strong&gt;: Implements MPI-inspired collective operations (broadcast, reduce, allreduce) optimized for serverless contexts using efficient algorithms.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;summary-of-3-implementation-of-fmi&quot;&gt;Summary of 3. Implementation of FMI&lt;&#x2F;h3&gt;
&lt;p&gt;The FMI framework is lightweight (~1,900 lines of C++ code) and provides:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;TCPunch Library&lt;&#x2F;strong&gt;: A custom NAT hole-punching solution storing and sharing address translations to enable direct connections between serverless functions.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Multi-language Support&lt;&#x2F;strong&gt;: Primarily C++, but with Python bindings for accessibility.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Communicators&lt;&#x2F;strong&gt;: Organize serverless functions into groups, enabling scalable and independent communication patterns. Functions within a communicator synchronize with timers for consistency and fault isolation.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;summary-of-4-evaluation&quot;&gt;Summary of 4. Evaluation&lt;&#x2F;h3&gt;
&lt;p&gt;FMI is evaluated for &lt;strong&gt;latency&lt;&#x2F;strong&gt;, &lt;strong&gt;bandwidth&lt;&#x2F;strong&gt;, &lt;strong&gt;cost&lt;&#x2F;strong&gt;, and &lt;strong&gt;scalability&lt;&#x2F;strong&gt;:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Latency&lt;&#x2F;strong&gt;: Direct TCP communication reaches microsecond-level latency, ~162x faster than AWS S3 or DynamoDB.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Cost Efficiency&lt;&#x2F;strong&gt;: FMI reduces costs by up to &lt;strong&gt;397x&lt;&#x2F;strong&gt;, with as little as $0.02 per 1,000 epochs for distributed machine learning workloads.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Scalability&lt;&#x2F;strong&gt;: Scales efficiently to 256 functions, outperforming Redis and DynamoDB, which suffer bottlenecks and timeouts at scale.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Bandwidth&lt;&#x2F;strong&gt;: Delivers superior bandwidth across various message sizes, maintaining stability under high concurrency.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;important-results&quot;&gt;Important Results&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reduction in Communication Latency&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Direct TCP achieves microsecond-level latency, up to &lt;strong&gt;162x faster&lt;&#x2F;strong&gt; than storage-based methods.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Cost Savings&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Up to &lt;strong&gt;397x&lt;&#x2F;strong&gt; reduction in communication costs. Some ML workloads see costs under &lt;strong&gt;$0.02&lt;&#x2F;strong&gt; per 1,000 epochs, versus &lt;strong&gt;$7.52&lt;&#x2F;strong&gt; with DynamoDB.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Improved Scalability&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Efficient scaling to 256 serverless functions while maintaining low latency and high bandwidth.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Bandwidth Performance&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Superior bandwidth performance across message sizes, stable under high concurrency.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Optimized Collective Operations&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Implements broadcast, reduce, and allreduce with the lowest latency across all evaluated solutions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Case Study in Distributed Machine Learning&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Replacing DynamoDB with FMI yields a &lt;strong&gt;1224x&lt;&#x2F;strong&gt; improvement in communication speed, with no significant integration overhead.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Minimal Integration Overhead&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Only &lt;strong&gt;four lines of code&lt;&#x2F;strong&gt; were changed to replace DynamoDB with FMI in the machine learning example.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;strengths-and-weaknesses-of-the-paper&quot;&gt;Strengths and Weaknesses of the Paper&lt;&#x2F;h3&gt;
&lt;h4 id=&quot;strengths&quot;&gt;Strengths&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Innovative Solution&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Introduces a novel approach (TCP NAT hole punching) to solve communication bottlenecks in serverless computing.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Comprehensive Evaluation&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Benchmarks compare FMI against AWS S3, DynamoDB, and Redis for latency, cost, bandwidth, and scalability.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Scalability&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Maintains performance up to 256 serverless functions without significant degradation.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Low Cost and High Performance&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Achieves up to 397x cost savings and 162x faster communication.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Portability and Modularity&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Cloud-agnostic design compatible with AWS Lambda, Kubernetes, and MPI.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ease of Integration&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Minimal code changes required, facilitating adoption in existing systems.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h4 id=&quot;weaknesses&quot;&gt;Weaknesses&lt;&#x2F;h4&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Reliance on Assumptions&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Assumes all functions in a communication group are co-located and run concurrently, which may not always hold in practice.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Limited Fault Tolerance&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Lacks built-in mechanisms for handling individual function failures mid-communication.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Dependency on External Infrastructure&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Requires a &lt;strong&gt;hole-punching server&lt;&#x2F;strong&gt; for address translation.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Limited Real-World Testing&lt;&#x2F;strong&gt;&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Evaluated mainly in controlled benchmarks and case studies with broader real-world validations needed.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;clarification-on-table-1&quot;&gt;Clarification on Table 1&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;Table 1 presents a &lt;strong&gt;general performance comparison&lt;&#x2F;strong&gt; of object storage, NoSQL databases, in-memory caches, and direct TCP.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Key takeaway&lt;&#x2F;strong&gt;: Direct TCP has zero cloud provider cost, but the computation burden may shift to the user, slightly reducing the impact of “no cost.”&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;udp-vs-tcp&quot;&gt;UDP vs TCP?&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Why UDP wasn’t tested&lt;&#x2F;strong&gt;:
&lt;ul&gt;
&lt;li&gt;Cloud providers (like AWS) control UDP usage and do not currently allow it in serverless environments.&lt;&#x2F;li&gt;
&lt;li&gt;Existing serverless frameworks already provide direct TCP communication.&lt;&#x2F;li&gt;
&lt;li&gt;If AWS supported UDP for serverless computing, the authors might have tested it instead of TCP.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;open-source-cloud&quot;&gt;Open Source Cloud?&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Are cloud providers open source?&lt;&#x2F;strong&gt;
&lt;ul&gt;
&lt;li&gt;Typically, no. Cloud providers like AWS are highly opaque, preventing visibility into the underlying infrastructure.&lt;&#x2F;li&gt;
&lt;li&gt;They can move serverless functions among different physical servers without user knowledge, complicating assumptions about network state.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;clarification-on-figures&quot;&gt;Clarification on Figures&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Violin plots and box plots&lt;&#x2F;strong&gt;: The paper uses them to display &lt;strong&gt;data distribution&lt;&#x2F;strong&gt; across multiple trials.
&lt;ul&gt;
&lt;li&gt;Violin plots show data spread (with a median dot).&lt;&#x2F;li&gt;
&lt;li&gt;Box plots depict the median, quartiles, and outliers more succinctly.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;The authors use these to demonstrate performance variance and the stability of FMI versus other solutions.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;openai-and-other-cloud-providers-use-of-fmi&quot;&gt;OpenAI (and other cloud providers’) Use of FMI?&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Could OpenAI use FMI?&lt;&#x2F;strong&gt;
&lt;ul&gt;
&lt;li&gt;Potentially yes. OpenAI’s services could benefit from direct TCP for certain workloads.&lt;&#x2F;li&gt;
&lt;li&gt;TCP tunables (like window size) impact performance, as shown in the paper’s figures.&lt;&#x2F;li&gt;
&lt;li&gt;If an open-source serverless framework offered full control, UDP or RMA-based approaches might be tested as well.&lt;&#x2F;li&gt;
&lt;li&gt;AWS’s closed nature limits certain optimizations.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;rma-discussion&quot;&gt;RMA Discussion?&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;The paper references &lt;strong&gt;RMA&lt;&#x2F;strong&gt; (Remote Memory Access) as another model of communication.
&lt;ul&gt;
&lt;li&gt;It’s mentioned to highlight potential benefits and trade-offs of different data-exchange paradigms in heterogeneous environments (including FPGAs).&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FMI: Fast and Cheap Message Passing for Serverless Functions&lt;&#x2F;strong&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;OSTEP Textbook&lt;&#x2F;strong&gt; – Remzi Arpaci-Dusseau&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Computer Organization and Design RISC-V Edition: The Hardware&#x2F;Software Interface, 2nd Edition&lt;&#x2F;strong&gt; (2020)
&lt;ul&gt;
&lt;li&gt;Authors: David A. Patterson, John L. Hennessy&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Computer Architecture, Sixth Edition: A Quantitative Approach&lt;&#x2F;strong&gt; (2017)
&lt;ul&gt;
&lt;li&gt;Authors: John L. Hennessy, David A. Patterson&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;h2 id=&quot;generative-ai&quot;&gt;Generative AI&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;ai-tools-used&quot;&gt;AI Tools Used&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;ChatGPT&lt;&#x2F;strong&gt;: &lt;a href=&quot;https:&#x2F;&#x2F;chatgpt.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;chatgpt.com&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;DeepSeek&lt;&#x2F;strong&gt;: &lt;a href=&quot;https:&#x2F;&#x2F;www.deepseek.com&#x2F;&quot;&gt;https:&#x2F;&#x2F;www.deepseek.com&#x2F;&lt;&#x2F;a&gt;&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;p&gt;These tools aided in:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Generating ideas for the outline.&lt;&#x2F;li&gt;
&lt;li&gt;Explaining complex keywords.&lt;&#x2F;li&gt;
&lt;li&gt;Providing prose feedback.&lt;&#x2F;li&gt;
&lt;li&gt;Converting the document to Markdown&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;example-contribution&quot;&gt;Example Contribution&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Improving Clarity&lt;&#x2F;strong&gt;: Simplifying sentence structure and refining word choices for better readability.&lt;&#x2F;li&gt;
&lt;li&gt;Changed “The paper references RMA, which does shared memory over distributed memory - another way of doing send&#x2F;receives. What was this reference for? The reference to RMA was to talk about heterogeneous environments in comparison with others such as FPGAs.” to “The paper references &lt;strong&gt;RMA&lt;&#x2F;strong&gt; (Remote Memory Access) as another model of communication.  It’s mentioned to highlight potential benefits and trade-offs of different data-exchange paradigms in heterogeneous environments (including FPGAs).”&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;limitations&quot;&gt;Limitations&lt;&#x2F;h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Potential Inaccuracy&lt;&#x2F;strong&gt;: Generative AI content should be externally validated.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Brainstorming vs. Factual Source&lt;&#x2F;strong&gt;: Great for brainstorming and suggestions, but final facts must be verified.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Unavailable&lt;&#x2F;strong&gt;: Deepseek provides a free alternative to ChatGPT, but the server is frequently Unavailable.&lt;&#x2F;li&gt;
&lt;li&gt;&lt;strong&gt;Low Quality&lt;&#x2F;strong&gt;: Writing is often repetitive and uninspired with important details missing. Changed “Message Passing Interface (MPI) – A standardized API for parallel programming in distributed memory systems. MPI enables communication between processes on different nodes. FMI is modeled after MPI to provide similar abstractions for serverless platforms.” to “&lt;strong&gt;Message Passing Interface (MPI)&lt;&#x2F;strong&gt; – A standardized API for &lt;strong&gt;parallel computing&lt;&#x2F;strong&gt;, which FMI models for FaaS.”&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;hr &#x2F;&gt;
&lt;p&gt;&lt;em&gt;Did you find this post insightful? Share your thoughts below!&lt;&#x2F;em&gt;&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>Questions and Thoughts on nuKSM</title>
                <pubDate>Wed, 15 Jan 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/nuksm/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/nuksm/</guid>
                <description>&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;&#x2F;h2&gt;
&lt;p&gt;Recently, our group reviewed a very interesting article titled “nuKSM:
NUMA-aware Memory De-duplication on Multi-socket Servers” by Akash Panda,
Ashish Panwar, and Arkaprava Basu.  This blog post focuses on nuKSM, a proposed
solution to Linux’s bad interaction with NUMA systems that deal with a lot of
data, such as servers. This past week we as a group collectively looked over
this paper and discussed it among ourselves, through this blog post I hope to
record some of our findings and questions.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;&#x2F;h2&gt;
&lt;p&gt;A quick table of notable terms:
NUMA-non uniform memory access
KSM-Kernel Same page Merging
TPS-Transparent page sharing
VM- Virtual machine&lt;&#x2F;p&gt;
&lt;p&gt;NUMA systems have special configurations of hardware called NUMA nodes, these are small collections of processors, memory, and I&#x2F;O buses that are linked together in a NUMA architecture.&lt;&#x2F;p&gt;
&lt;p&gt;&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;nuksm&#x2F;numa.PNG&quot; alt=&quot;NUMA Diagram&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;h2 id=&quot;nuksm-s-purpose&quot;&gt;nuKSM’s purpose&lt;&#x2F;h2&gt;
&lt;p&gt;nuKSM is meant to fix many of the problems KSM has been known to have over the
years, it accounts for priority when deciding which pages to delete, it changes
the way pages are structured to increase de-duplication speeds, and it makes
KSM work with NUMA in the first place.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;discussion&quot;&gt;Discussion&lt;&#x2F;h2&gt;
&lt;p&gt;Me and my classmates began the discussion listening to our presenter Eugene reading off a powerpoint included in the sources of this blog. After we finished reading the presentation together we began an open discussion dissecting the paper together. I would like to highlight a few interesting questions we had after reading, if any of these questions speaks to you please feel free to look more into it or reach out for further discussion.
Could nuKSM have positive effects on GPU memory storage techniques?
I asked this question to Euguene due to reading the acronym CUP as GPU, even though it was a misunderstanding it led to an interesting tangent where we discussed if an implementation like nuKSM could benefit a piece of hardware like a graphics card. Both use memory and move large amounts of data around but they do it for different purposes and in different ways. We ended the discussion on that topic by noting that it might be beneficial for GPU developers to look at the techniques of nuKSM but it won’t be directly applicable to their work
How fast is good enough for deduplication?
One interesting thing noted in the paper is the redesign of the page reclamation algorithm. This algorithm is used to assign a since value for the priority of the specific process using a page.
&lt;img src=&quot;https:&#x2F;&#x2F;khale.github.io&#x2F;mem-systems-w25&#x2F;blog&#x2F;nuksm&#x2F;formula.PNG&quot; alt=&quot;nuShare Formula&quot; &#x2F;&gt;&lt;&#x2F;p&gt;
&lt;p&gt;This new algorithm is stated to increase fairness of priority, in addition the
page structure which used to follow a tree design was changed to a forest
design. This forest makes for faster page duplication so that more pages can be
deduplicated before new data comes in to overwhelm the system. The question
remaining is how big of a burst of data is required to cause a system failure.
nuKSM github mystery?&lt;&#x2F;p&gt;
&lt;p&gt;According to our presenter Eugene when he began researching this paper he found the nuKSM github page for the open source version of the project. The page is still intake and folder names are present however upon trying to download anything from the page you will find a lot of empty files and links. Since the project is only 4 years old and from the Indian Institute of Science you would think the resources would still be available. The kernel patch set is currently lost to time and unavailable, they do have their own benchmarks for the kernel but it is untestable. This is even more strange when you consider that most universities require papers and their respective materials to be preserved in databases like ACM.&lt;&#x2F;p&gt;
&lt;p&gt;Are duplicate pages acceptable?&lt;&#x2F;p&gt;
&lt;p&gt;A small discussion took place involving the point that the original graphs
presented in the paper did not take into account the possibility of duplicate
pages, i.e. the practice of not deleting either page and leaving a copy with
both VM’s. This is a scenario that would only occur in rare cases where the
priority rate is split evenly between some number of VM’s by nearly equal
amounts. The argument was that it would be more computationally cost effective
to just sacrifice some memory to save a lot of time accessing a different place
in memory, this is especially true if we consider nuKSM’s implementation of
forested pages where the data we have to fetch is deep inside the page.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;final-thoughts&quot;&gt;Final Thoughts&lt;&#x2F;h2&gt;
&lt;p&gt;nuKSM makes two important changes to vanilla KSM, it makes NUMA aware, and it
changes the way it creates and manages pages. While this implementation does
increase productivity and fix issues for NUMA systems using linux there are
still many more chances for growth, there is a recently published paper
discussing adaptive memory deduplication linked below that might be the next
step in this line of thought. This paper in fact references nuKSM trying to
harmonize cache access with page access counters in a way similar to the page
relocation algorithm.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources&lt;&#x2F;h2&gt;
&lt;ul&gt;
&lt;li&gt;https:&#x2F;&#x2F;apanwariisc.github.io&#x2F;publications&#x2F;pact-2021-nuksm&#x2F;nuksm-pact21-slides.pdf&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;github.com&#x2F;csl-iisc&#x2F;nuKSM-pact21-artifact&lt;&#x2F;li&gt;
&lt;li&gt;https:&#x2F;&#x2F;ieeexplore.ieee.org&#x2F;document&#x2F;10466365&lt;&#x2F;li&gt;
&lt;li&gt;nuKSM: NUMA-aware Memory De-duplication on Multi-socket Servers&lt;&#x2F;li&gt;
&lt;li&gt;Akash Panda*, Ashish Panwar, Arkaprava Basu&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
</description>
            </item>
        
            <item>
                <title>Mosiac Pages: Big TLB Reach with Small Pages</title>
                <pubDate>Mon, 13 Jan 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/mosaic-pages/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/mosaic-pages/</guid>
                <description>&lt;h2 id=&quot;background-and-context&quot;&gt;Background and Context&lt;&#x2F;h2&gt;
&lt;p&gt;The Bottleneck Problem - Modern workloads like machine learning have massive working sets of memory that often exceed the TLB’s capacity which leads to frequent misses and performance degradation. Previous approaches rely on physical memory contiguity which increases overhead from memory defragmentation.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;keywords&quot;&gt;KEYWORDS&lt;&#x2F;h2&gt;
&lt;p&gt;virtual memory - an abstraction of memory which provides the illusion of contiguous physical memory.&lt;&#x2F;p&gt;
&lt;p&gt;address translation - the process of converting virtual addresses into physical addresses by using page tables and TLBs&lt;&#x2F;p&gt;
&lt;p&gt;Translation Look-aside Buffer (TLB) - special memory cache that holds translations between virtual and physical memory, relatively small compared to caches because it is designed to be faster&lt;&#x2F;p&gt;
&lt;p&gt;paging - physical memory is divided into frames and virtual memory is divided into pages and the operating system maps pages to frames in the page table&lt;&#x2F;p&gt;
&lt;p&gt;Hashing - A computational technique that maps input data to a fixed range of output values using a hash function.&lt;&#x2F;p&gt;
&lt;p&gt;Iceberg hashing - An advanced hashing technique used in Mosaic Pages to optimize memory allocation. It features low associativity, meaning each virtual page has limited potential physical frame mappings, and stability, ensuring consistent mappings.&lt;&#x2F;p&gt;
&lt;p&gt;Radix tree - A tree-like data structure used to store key-value pairs compactly, where each node represents a prefix of the keys.&lt;&#x2F;p&gt;
&lt;p&gt;Arity - The number of arguments or inputs a function takes.
Address Space Identifier (ASID) - A unique identifier assigned to each process’s TLB mappings. ASIDs prevent TLB entries from one process being accessed by another during a context switch.&lt;&#x2F;p&gt;
&lt;p&gt;Detailed summary of the main contributions of the paper&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary-of-1-introduction&quot;&gt;Summary of 1. Introduction:&lt;&#x2F;h2&gt;
&lt;p&gt;The paper introduces Mosaic Pages, which is a technique to increase the reach of Translation Lookaside Buffers (TLBs) without relying on physical contiguity, thus eliminating the need for memory defragmentation. Mosaic Pages leverages virtual contiguity and Iceberg hashing to compress multiple address translations into a single TLB entry. Some highlights from the paper include:
Mosaic Pages compresses translations by mapping virtual pages to a limited number of physical page frames using hashing which reduces the size of each translation so that multiple mappings can fit into one TLB entry.
A full-system prototype implemented in gem5 demonstrated up to 98% reduction in TLB misses compared to a baseline model.
A Verilog implementation of the technique indicates minimal impact on clock speed or area.
Mosaic Pages swaps fewer pages than Linux while maintaining similar performance when under memory pressure.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary-of-2-mosaic-pages&quot;&gt;Summary of 2. Mosaic Pages:&lt;&#x2F;h2&gt;
&lt;p&gt;The section discusses the design of Mosaic Pages. A Mosaic Page comprises a virtually consecutive 4 KiB base pages where a is between 4 and up to 64. Translations for all a base pages are compressed into one TLB entry using Compressed Physical Frame Numbers (CPFNs). These CPFNs mean that virtual pages are mapped to a small set of physical frames (h = 104 in experiments) using hashing, reducing the number of bits required for translation. Physical memory is structured as a hash table with buckets containing slots. Each virtual page is hashed to specific buckets, and its placement is recorded as a CPFN. The Mosaic Pages technique uses Iceberg Hashing, which ensures low associativity, stability, and high utilization of memory. Physical memory is divided into buckets with “front yard” and “back yard” sections for load balancing. Pages are allocated to the least-filled bucket, minimizing conflicts while maintaining near-full memory utilization. Mosaic Pages use a new Horizon LRU algorithm for managing swapping when no slots are available.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary-of-3-implementation-of-mosaic-pages&quot;&gt;Summary of 3. Implementation of Mosaic Pages:&lt;&#x2F;h2&gt;
&lt;p&gt;This section describes the implementation of Mosaic Pages using the gem5 simulator and a Linux prototype. The components include a Gem5 Simulation of the Mosaic TLB and a Linux Prototype that uses the reserved memory and LRU replacement policy.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary-of-4-evaluation&quot;&gt;Summary of 4. Evaluation:&lt;&#x2F;h2&gt;
&lt;p&gt;This section discusses the performance and feasibility of Mosaic Pages. Using workloads like Graph500 and XSBench, it was found that Mosaic Pages reduces TLB misses across all workloads and achieves up to 98% reduction with a Mosaic-64 configuration. Mosaic experiences associativity conflicts only after 98% memory utilization. Swapping was found to be comparable to base Linux.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;summary-of-5-related-work&quot;&gt;Summary of 5. Related Work:&lt;&#x2F;h2&gt;
&lt;p&gt;This section reviews prior research addressing TLB performance challenges. Previously, “huge pages” were used to improve TLB reach but this would lead to overhead due to defragmentation. Mosaic avoids physical contiguity by using low-associative hashing for flexibility and reduced hardware costs. A redesign of TLB entries that combines adjacent virtually and physically contiguous pages into one entry reduces TLB misses. Mosaic uses Iceberg hashing to increase TLB reach and hit rate, while hashed page tables focus on reducing miss costs.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;important-results-and-what-they-mean&quot;&gt;Important results and what they mean&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;Reduction in TLB Misses Mosaic Pages reduced TLB misses by 6–81% across various workloads compared to a traditional TLB. Using Graph500, Mosaic reduced misses with an improvement of up to 98%.&lt;&#x2F;li&gt;
&lt;li&gt;Memory Utilization and Swapping Behavior Mosaic Pages achieved 98% memory utilization before experiencing associativity conflicts, compared to 99% for Linux.&lt;&#x2F;li&gt;
&lt;li&gt;Hardware Feasibility Mosaic hardware synthesized on a 28nm CMOS process demonstrated 220 ps latency at 4 GHz.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;strengths-and-weaknesses-of-the-paper&quot;&gt;Strengths and Weaknesses of the Paper&lt;&#x2F;h2&gt;
&lt;p&gt;Strengths of the Paper&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Innovative Concept: Mosaic Pages introduces a novel approach to increasing the reach of Translation Lookaside Buffers (TLBs) without relying on physical memory contiguity. By leveraging virtual contiguity and Iceberg Hashing, it compresses multiple address translations into a single TLB entry.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Comprehensive Evaluation: The paper provides extensive performance evaluations using a variety of workloads, such as Graph500 and XSBench, and simulations implemented in gem5.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Scalability: Mosaic Pages performs well under high memory utilization, maintaining efficiency up to 98% memory usage without significant degradation.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Low Hardware Overhead: The hardware implementation of Mosaic Pages adds minimal area and latency overhead. The 28nm CMOS implementation demonstrates only 220 ps of latency at 4 GHz, making it feasible for integration into existing systems.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;weaknesses-of-the-paper&quot;&gt;Weaknesses of the Paper&lt;&#x2F;h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Dependency on Iceberg Hashing Scheme: Mosaic Pages relies heavily on the Iceberg Hashing algorithm to achieve its goals. This dependency limits flexibility and introduces a single point of failure if the hashing algorithm does not perform well under specific conditions or workloads.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Worse memory pressure performance compared to Linux: Under high memory pressure, Mosaic Pages performs slightly worse than Linux. While Mosaic Pages avoids memory overhead, its inability to match Linux’s performance under these conditions limits its appeal for memory-constrained environments.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Low number of citations might indicate that paper is not sufficiently novel: The paper has a limited number of citations (21 at the time of writing), which could indicate that the concept is either not widely known or lacks sufficient novelty to generate significant interest in the academic community.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Page table sharing is not available: Mosaic Pages does not support sharing of page tables across processes. For example, shared libraries, which are common in multi-process applications, cannot be mapped efficiently, leading to higher memory usage.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Did not discuss NUMA or GPU implementations: The paper does not address Non-Uniform Memory Access (NUMA) architectures or GPU memory systems, both of which have unique challenges and requirements. NUMA systems require careful handling of memory locality, while GPUs rely on massively parallel processing with their own memory hierarchies.&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;class-discussion&quot;&gt;Class Discussion:&lt;&#x2F;h2&gt;
&lt;h2 id=&quot;gaming-workloads&quot;&gt;Gaming workloads?&lt;&#x2F;h2&gt;
&lt;p&gt;The lack of improvement on the GUPS benchmark, which measures random memory access performance, suggests that Mosaic Pages may face challenges with workloads heavily reliant on randomness. While gaming workloads share some aspects of random access, their diverse memory access patterns, including sequential access for assets and textures, might still allow Mosaic Pages to provide performance benefits in certain scenarios.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;gpus&quot;&gt;GPUs?&lt;&#x2F;h2&gt;
&lt;p&gt;While the paper focuses on CPU workloads, it raises questions about the applicability of Mosaic Pages to GPUs. GPUs, which typically have TLBs as part of their memory management units (MMUs), rely heavily on high-bandwidth and parallel processing architectures. The integration of Mosaic Pages into GPU systems could reduce TLB misses, particularly for workloads with predictable memory access patterns. However, unique architectural challenges in GPUs such as managing parallel threads and high throughput might require significant adaptations of Mosaic Pages to align with GPU memory subsystems.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;context-switching-in-tlb&quot;&gt;Context Switching in TLB?&lt;&#x2F;h2&gt;
&lt;p&gt;What happens to TLB when a context switch occurs? Context switches, an essential part of multitasking in modern operating systems, pose a significant challenge to TLB efficiency and security. During a context switch, the TLB must be managed to prevent unauthorized access to memory pages from the previous process. Typically, this is achieved by entirely invalidating the TLB, which ensures security but introduces performance overhead as new mappings must be repopulated. A more optimal solution involves the use of Address Space Identifiers (ASIDs), which assign a unique identifier to each process. ASIDs allow the TLB to selectively clear only the mappings associated with the previous process, leaving shared mappings intact. This reduces the performance cost of context switches while maintaining strict memory isolation. While the paper does not explicitly discuss context switch management, integrating Mosaic Pages with an ASID-based approach could further enhance TLB efficiency and security.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;how-does-iceberg-hashing-work&quot;&gt;How does Iceberg Hashing Work?&lt;&#x2F;h2&gt;
&lt;p&gt;Iceberg Hashing is an essential part of Mosaic Pages ability to compress virtual-to-physical address mappings into Compressed Physical Frame Numbers (CPFNs). This advanced hashing technique reduces the number of bits required per mapping, enabling the TLB to store multiple mappings in a single entry. To achieve this, Iceberg Hashing must meet three critical properties:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Range: The hash function should evenly distribute virtual addresses across the available physical frames to ensure efficient memory utilization.&lt;&#x2F;li&gt;
&lt;li&gt;Stability: The function must produce consistent, deterministic mappings for the same inputs to maintain system reliability.&lt;&#x2F;li&gt;
&lt;li&gt;Reliability: It should minimize collisions and conflicts, even under high memory pressure.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;mapping-conflict&quot;&gt;Mapping Conflict?&lt;&#x2F;h2&gt;
&lt;p&gt;Mapping conflicts can occur in systems using hashed memory allocation like Mosaic Pages. When a new virtual-to-physical mapping conflicts with an existing one, Mosaic Pages resolves the issue by evicting the previous mapping. This eviction follows a heuristic akin to the Least Recently Used (LRU) policy, ensuring that the mapping least likely to be accessed is replaced. Mosaic Pages operates with a fixed associativity. This level of associativity ensures that mappings are evenly distributed across memory, reducing the frequency of conflicts.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;front-yard-vs-back-yard&quot;&gt;Front Yard vs Back Yard?&lt;&#x2F;h2&gt;
&lt;p&gt;Mosaic Pages introduces a memory organization strategy by dividing physical memory buckets into two distinct regions called the front yard and the backyard. The front yard, which constitutes the larger portion of the bucket, is reserved for frequently accessed data. This prioritization ensures low-latency access to high-demand pages, enhancing performance for workloads that depend on fast memory access. On the other hand, the backyard serves as a storage area for less frequently accessed, ‘ghost’ pages.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;dealing-with-conflict&quot;&gt;Dealing with Conflict?&lt;&#x2F;h2&gt;
&lt;p&gt;Mosaic Pages deals with memory allocation conflicts with a fixed associativity of 104, derived from the properties of Iceberg Hashing. This level of associativity ensures a well-distributed mapping of virtual pages to physical frames, reducing the likelihood of conflicts even as memory utilization approaches 98%. In comparison, the standard Linux allocator begins swapping at similar utilization levels but incurs higher memory overhead. Mosaic Pages avoids this overhead by managing conflicts efficiently, relying on the hashing algorithm to spread entries evenly across the available physical frames. Even under high memory pressure, Mosaic Pages maintains performance comparable to Linux.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;numa&quot;&gt;NUMA?&lt;&#x2F;h2&gt;
&lt;p&gt;Mosaic Pages applicability to NUMA (Non-Uniform Memory Access) architectures remains unexplored. NUMA introduces additional dimensions to the physical address space, as memory is divided across multiple nodes with varying access latencies. Integrating Mosaic Pages into NUMA systems would require enhancements to the Iceberg Hashing algorithm to handle 2D load balancing mappings not only across buckets but also across NUMA nodes. This adaptation would involve node-aware hashing and dynamic monitoring to ensure even distribution of memory accesses and minimize the performance penalties associated with remote access. Exploring this area could unlock further scalability for Mosaic Pages in high-performance and distributed computing environments.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;isa-adaptation&quot;&gt;ISA Adaptation?&lt;&#x2F;h2&gt;
&lt;p&gt;Adapting Mosaic TLB to different Instruction Set Architectures (ISAs) would involve primarily hardware-level modifications to the Memory Management Unit (MMU), rather than changes to the ISA itself. For example, implementing Mosaic TLB on x86 would require integrating Iceberg Hashing and support for Compressed Physical Frame Numbers (CPFNs) into the MMU, alongside minor adjustments to the page allocator. This adaptation is feasible, as demonstrated by the Gem5-based prototype. In contrast, RISC-V offers greater flexibility due to its open-source nature, allowing deeper customization of page tables and memory management. Researchers could design a RISC-V processor with a custom MMU tailored for Mosaic Pages, ensuring seamless integration with the ISA.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;cost&quot;&gt;Cost?&lt;&#x2F;h2&gt;
&lt;p&gt;The cost of implementing Mosaic Pages is described as ‘not prohibitively expensive,’ primarily due to its efficient use of resources. By compressing multiple mappings into a single TLB entry through Compressed Physical Frame Numbers (CPFNs), Mosaic Pages significantly reduce the memory footprint of TLB entries. This enables increased TLB capacity without requiring additional memory resources. The Verilog implementation further highlights the practicality of Mosaic Pages and suggest that Mosaic Pages can be integrated into existing hardware with modest design changes to the Memory Management Unit (MMU).&lt;&#x2F;p&gt;
&lt;h2 id=&quot;origin-of-iceberg-hashing&quot;&gt;Origin of Iceberg Hashing?&lt;&#x2F;h2&gt;
&lt;p&gt;The authors of Mosaic Pages did not invent Iceberg Hashing but recognized its potential for their TLB optimization technique. Iceberg Hashing excels in three critical areas:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Low Associativity: By minimizing collisions, it ensures efficient distribution of virtual-to-physical mappings across available memory frames.&lt;&#x2F;li&gt;
&lt;li&gt;Stability: Its deterministic behavior guarantees consistent mappings, an essential feature for reliable memory management.&lt;&#x2F;li&gt;
&lt;li&gt;High Utilization: Iceberg Hashing optimizes memory usage, achieving near-full capacity without significant performance penalties.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h2 id=&quot;shared-memory-support&quot;&gt;Shared Memory Support?&lt;&#x2F;h2&gt;
&lt;p&gt;One notable limitation of Mosaic Pages is its lack of support for shared memory. Shared memory allows multiple processes to reference the same data without duplication, reducing the memory footprint and improving efficiency. Without this capability, Mosaic Pages requires each process to maintain its own copy of shared data, such as libraries or frameworks, leading to increased memory usage. The primary reason for this limitation lies in the Iceberg Hashing algorithm, which maps virtual pages to physical frames in a one-to-one manner for each process. Modifying the algorithm to handle shared mappings would require significant changes to both the hashing mechanism and the TLB structure. While the paper acknowledges this shortcoming, it stops short of offering a solution. Addressing this limitation would make Mosaic Pages far more versatile.&lt;&#x2F;p&gt;
&lt;p&gt;Arbitrary mapping between PA and VA?
Mosaic Pages imposes restrictions on arbitrary physical address (PA) to virtual address (VA) mappings due to the constraints of the Iceberg Hashing algorithm. This deterministic hashing mechanism is designed to optimize memory utilization and reduce TLB misses by compressing multiple virtual-to-physical mappings into a single TLB entry. This could pose challenges in specialized systems that require fine-grained control over address mappings. This trade-off reflects a common theme in academic research of focusing on solving specific challenges which often leads to compromises in other areas.&lt;&#x2F;p&gt;
&lt;h2 id=&quot;sources&quot;&gt;Sources:&lt;&#x2F;h2&gt;
&lt;p&gt;One-Level Storage System Kilburn et al. IRE Transactions on Electronic Computers, 1962.
OSTEP Textbook Chapters 18-20 Remzi Arpaci-Dusseau
Mosaic Pages: Big TLB Reach with Small Pages
Computer Organization and Design RISC-V Edition The Hardware Software Interface 2nd Edition - December 11, 2020 Authors: David A. Patterson, John L. Hennessy
Computer Architecture, Sixth Edition: A Quantitative Approach December 2017 Authors: John L. Hennessy, David A. Patterson&lt;&#x2F;p&gt;
&lt;h2 id=&quot;generative-ai&quot;&gt;Generative AI&lt;&#x2F;h2&gt;
&lt;p&gt;Link to specific Tool: https:&#x2F;&#x2F;chatgpt.com&#x2F;
This tool was used to help generate ideas for the outline, provide explanations of keywords, and offer feedback on specific prose.
Concrete example: changed “Radix tree - a data structure used in page tables” to “Radix tree - A tree-like data structure used to store key-value pairs compactly, where each node represents a prefix of the keys.”
Prose assistance could be overly verbose and ostentatious: “Unlike traditional systems that allow arbitrary mappings of physical addresses (PAs) to virtual addresses (VAs), Mosaic Pages imposes restrictions on these mappings due to the constraints of the Iceberg Hashing algorithm. This deterministic hashing mechanism is designed to optimize memory utilization and reduce TLB misses by compressing multiple virtual-to-physical mappings into a single TLB entry. However, the trade-off is a loss of flexibility. Addresses can only map to a limited set of physical frames determined by the hash function.”
Generative AI tools are excellent tools for brainstorming, offering feedback, and providing explanations. These tools should not be trusted for accuracy. Any specific detail mentioned must be externally validated.&lt;&#x2F;p&gt;
</description>
            </item>
        
            <item>
                <title>Welcome to ECE 4&#x2F;599!</title>
                <pubDate>Mon, 06 Jan 2025 00:00:00 +0000</pubDate>
                <link>https%3A//khale.github.io/mem-systems-w25/blog/welcome/</link>
                <guid>https%3A//khale.github.io/mem-systems-w25/blog/welcome/</guid>
                <description>&lt;p&gt;I’m excited to teach this new research course on memory systems at OSU! We’ll be covering a lot of ground,
focusing on systems software research, but touching areas in memory technologies, fault tolerance, distributed systems, storage, hardware accelerators, and much more.&lt;&#x2F;p&gt;
&lt;p&gt;We’ll use this course blog for paper discussions and project reports throughout the quarter.&lt;&#x2F;p&gt;
</description>
            </item>
        
    </channel>
</rss>
